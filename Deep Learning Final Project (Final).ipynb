{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "from torch.nn.modules import MSELoss, L1Loss, BCELoss\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "from numpy import array, asarray, ndarray, swapaxes\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training controls\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "training_size = 0.8\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 268, 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data holders\n",
    "x_test = []\n",
    "x_train = []\n",
    "y_test= []\n",
    "y_train= []\n",
    "tempY = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'index', 'imdbId', 'Imdb Link', 'Title', 'IMDB Score', 'Genre', 'Poster']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opening the dataset\n",
    "dataset = csv.reader(open(\"MovieGenre417.csv\",encoding=\"utf8\",errors='replace'), delimiter=\",\")\n",
    "\n",
    "# skipping the header line\n",
    "next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract images from zip folder\n",
    "\n",
    "import zipfile as zf\n",
    "\n",
    "files = zf.ZipFile(\"FinalProjectUp.zip\", 'r')\n",
    "files.extractall()\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of image files in SampleMoviePosters folder\n",
    "flist=glob.glob('FinalProject/*.jpg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5384"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = []\n",
    "\n",
    "for path in flist:\n",
    "    start = path.rfind(\"/\")+1\n",
    "    end = len(path)-4\n",
    "    image_ids.append(path[start:end])\n",
    "    \n",
    "#image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.imdb.com/title/tt114709</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>Animation|Adventure|Comedy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>113228</td>\n",
       "      <td>http://www.imdb.com/title/tt113228</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>6.6</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>112896</td>\n",
       "      <td>http://www.imdb.com/title/tt112896</td>\n",
       "      <td>Dracula: Dead and Loving It (1995)</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Comedy|Fantasy|Horror</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>114168</td>\n",
       "      <td>http://www.imdb.com/title/tt114168</td>\n",
       "      <td>Powder (1995)</td>\n",
       "      <td>6.5</td>\n",
       "      <td>Drama|Fantasy|Mystery</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>114011</td>\n",
       "      <td>http://www.imdb.com/title/tt114011</td>\n",
       "      <td>Now and Then (1995)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5379</th>\n",
       "      <td>39322</td>\n",
       "      <td>40060</td>\n",
       "      <td>4508542</td>\n",
       "      <td>http://www.imdb.com/title/tt4508542</td>\n",
       "      <td>Nacida para ganar (2016)</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>39333</td>\n",
       "      <td>40071</td>\n",
       "      <td>1018706</td>\n",
       "      <td>http://www.imdb.com/title/tt1018706</td>\n",
       "      <td>CÌ£o Sem Dono (2007)</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Drama</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5381</th>\n",
       "      <td>39347</td>\n",
       "      <td>40085</td>\n",
       "      <td>4189294</td>\n",
       "      <td>http://www.imdb.com/title/tt4189294</td>\n",
       "      <td>Lego DC Comics: Batman Be-Leaguered (2014)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Animation|Short|Action</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>39362</td>\n",
       "      <td>40100</td>\n",
       "      <td>98216</td>\n",
       "      <td>http://www.imdb.com/title/tt98216</td>\n",
       "      <td>Roller Blade Warriors: Taken by Force (1989)</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>39364</td>\n",
       "      <td>40102</td>\n",
       "      <td>83291</td>\n",
       "      <td>http://www.imdb.com/title/tt83291</td>\n",
       "      <td>Los violadores (1981)</td>\n",
       "      <td>5.5</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5384 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  index   imdbId                            Imdb Link  \\\n",
       "0              0      0   114709   http://www.imdb.com/title/tt114709   \n",
       "1              2      2   113228   http://www.imdb.com/title/tt113228   \n",
       "2             11     11   112896   http://www.imdb.com/title/tt112896   \n",
       "3             23     23   114168   http://www.imdb.com/title/tt114168   \n",
       "4             26     26   114011   http://www.imdb.com/title/tt114011   \n",
       "...          ...    ...      ...                                  ...   \n",
       "5379       39322  40060  4508542  http://www.imdb.com/title/tt4508542   \n",
       "5380       39333  40071  1018706  http://www.imdb.com/title/tt1018706   \n",
       "5381       39347  40085  4189294  http://www.imdb.com/title/tt4189294   \n",
       "5382       39362  40100    98216    http://www.imdb.com/title/tt98216   \n",
       "5383       39364  40102    83291    http://www.imdb.com/title/tt83291   \n",
       "\n",
       "                                             Title  IMDB Score  \\\n",
       "0                                 Toy Story (1995)         8.3   \n",
       "1                          Grumpier Old Men (1995)         6.6   \n",
       "2               Dracula: Dead and Loving It (1995)         5.8   \n",
       "3                                    Powder (1995)         6.5   \n",
       "4                              Now and Then (1995)         6.8   \n",
       "...                                            ...         ...   \n",
       "5379                      Nacida para ganar (2016)         4.8   \n",
       "5380                          CÌ£o Sem Dono (2007)         6.9   \n",
       "5381    Lego DC Comics: Batman Be-Leaguered (2014)         6.8   \n",
       "5382  Roller Blade Warriors: Taken by Force (1989)         3.9   \n",
       "5383                         Los violadores (1981)         5.5   \n",
       "\n",
       "                           Genre  \\\n",
       "0     Animation|Adventure|Comedy   \n",
       "1                 Comedy|Romance   \n",
       "2          Comedy|Fantasy|Horror   \n",
       "3          Drama|Fantasy|Mystery   \n",
       "4           Comedy|Drama|Romance   \n",
       "...                          ...   \n",
       "5379                      Comedy   \n",
       "5380                       Drama   \n",
       "5381      Animation|Short|Action   \n",
       "5382     Action|Adventure|Sci-Fi   \n",
       "5383             Action|Thriller   \n",
       "\n",
       "                                                 Poster  \n",
       "0     https://images-na.ssl-images-amazon.com/images...  \n",
       "1     https://images-na.ssl-images-amazon.com/images...  \n",
       "2     https://images-na.ssl-images-amazon.com/images...  \n",
       "3     https://images-na.ssl-images-amazon.com/images...  \n",
       "4     https://images-na.ssl-images-amazon.com/images...  \n",
       "...                                                 ...  \n",
       "5379  https://images-na.ssl-images-amazon.com/images...  \n",
       "5380  https://images-na.ssl-images-amazon.com/images...  \n",
       "5381  https://images-na.ssl-images-amazon.com/images...  \n",
       "5382  https://images-na.ssl-images-amazon.com/images...  \n",
       "5383  https://images-na.ssl-images-amazon.com/images...  \n",
       "\n",
       "[5384 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset2 = pd.read_csv(\"MovieGenre417.csv\")\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "indexlist = []\n",
    "classes = tuple()\n",
    "ids = dataset2.imdbId.values.tolist()\n",
    "for image_id in image_ids:\n",
    "    #print(dataset2[\"imdbId\"])\n",
    "    genres = tuple((dataset2[dataset2[\"imdbId\"] == int(image_id)][\"Genre\"].values[0]).split(\"|\"))\n",
    "    if int(image_id) in ids:\n",
    "        indexlist.append(image_id)\n",
    "    y.append(genres)\n",
    "    classes = classes + genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y)\n",
    "y = mlb.transform(y)\n",
    "classes = set(classes)\n",
    "classes = list(classes)\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drama</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Crime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2371315</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504404</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363473</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389778</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42693</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409904</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106685</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375370</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331846</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519245</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5384 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Drama  Comedy  Romance  Action  Crime\n",
       "2371315      1       0        0       0      1\n",
       "2504404      1       0        0       0      1\n",
       "363473       1       0        0       0      0\n",
       "389778       1       0        0       0      0\n",
       "42693        0       1        0       1      0\n",
       "...        ...     ...      ...     ...    ...\n",
       "409904       0       0        0       1      0\n",
       "106685       1       1        0       0      0\n",
       "3375370      0       1        0       0      0\n",
       "3331846      1       1        0       0      0\n",
       "1519245      1       0        0       0      0\n",
       "\n",
       "[5384 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame(y, columns = classes, index = indexlist)\n",
    "y_df = y_df[['Drama', 'Comedy', 'Romance', 'Action', 'Crime']]\n",
    "classes = y_df.columns.tolist()\n",
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "      <th>genrelst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.imdb.com/title/tt114709</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>Animation|Adventure|Comedy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>113228</td>\n",
       "      <td>http://www.imdb.com/title/tt113228</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>6.6</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>112896</td>\n",
       "      <td>http://www.imdb.com/title/tt112896</td>\n",
       "      <td>Dracula: Dead and Loving It (1995)</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Comedy|Fantasy|Horror</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>114168</td>\n",
       "      <td>http://www.imdb.com/title/tt114168</td>\n",
       "      <td>Powder (1995)</td>\n",
       "      <td>6.5</td>\n",
       "      <td>Drama|Fantasy|Mystery</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>114011</td>\n",
       "      <td>http://www.imdb.com/title/tt114011</td>\n",
       "      <td>Now and Then (1995)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[1, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5379</th>\n",
       "      <td>39322</td>\n",
       "      <td>40060</td>\n",
       "      <td>4508542</td>\n",
       "      <td>http://www.imdb.com/title/tt4508542</td>\n",
       "      <td>Nacida para ganar (2016)</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>39333</td>\n",
       "      <td>40071</td>\n",
       "      <td>1018706</td>\n",
       "      <td>http://www.imdb.com/title/tt1018706</td>\n",
       "      <td>CÌ£o Sem Dono (2007)</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Drama</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5381</th>\n",
       "      <td>39347</td>\n",
       "      <td>40085</td>\n",
       "      <td>4189294</td>\n",
       "      <td>http://www.imdb.com/title/tt4189294</td>\n",
       "      <td>Lego DC Comics: Batman Be-Leaguered (2014)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Animation|Short|Action</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>39362</td>\n",
       "      <td>40100</td>\n",
       "      <td>98216</td>\n",
       "      <td>http://www.imdb.com/title/tt98216</td>\n",
       "      <td>Roller Blade Warriors: Taken by Force (1989)</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>39364</td>\n",
       "      <td>40102</td>\n",
       "      <td>83291</td>\n",
       "      <td>http://www.imdb.com/title/tt83291</td>\n",
       "      <td>Los violadores (1981)</td>\n",
       "      <td>5.5</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5384 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  index   imdbId                            Imdb Link  \\\n",
       "0              0      0   114709   http://www.imdb.com/title/tt114709   \n",
       "1              2      2   113228   http://www.imdb.com/title/tt113228   \n",
       "2             11     11   112896   http://www.imdb.com/title/tt112896   \n",
       "3             23     23   114168   http://www.imdb.com/title/tt114168   \n",
       "4             26     26   114011   http://www.imdb.com/title/tt114011   \n",
       "...          ...    ...      ...                                  ...   \n",
       "5379       39322  40060  4508542  http://www.imdb.com/title/tt4508542   \n",
       "5380       39333  40071  1018706  http://www.imdb.com/title/tt1018706   \n",
       "5381       39347  40085  4189294  http://www.imdb.com/title/tt4189294   \n",
       "5382       39362  40100    98216    http://www.imdb.com/title/tt98216   \n",
       "5383       39364  40102    83291    http://www.imdb.com/title/tt83291   \n",
       "\n",
       "                                             Title  IMDB Score  \\\n",
       "0                                 Toy Story (1995)         8.3   \n",
       "1                          Grumpier Old Men (1995)         6.6   \n",
       "2               Dracula: Dead and Loving It (1995)         5.8   \n",
       "3                                    Powder (1995)         6.5   \n",
       "4                              Now and Then (1995)         6.8   \n",
       "...                                            ...         ...   \n",
       "5379                      Nacida para ganar (2016)         4.8   \n",
       "5380                          CÌ£o Sem Dono (2007)         6.9   \n",
       "5381    Lego DC Comics: Batman Be-Leaguered (2014)         6.8   \n",
       "5382  Roller Blade Warriors: Taken by Force (1989)         3.9   \n",
       "5383                         Los violadores (1981)         5.5   \n",
       "\n",
       "                           Genre  \\\n",
       "0     Animation|Adventure|Comedy   \n",
       "1                 Comedy|Romance   \n",
       "2          Comedy|Fantasy|Horror   \n",
       "3          Drama|Fantasy|Mystery   \n",
       "4           Comedy|Drama|Romance   \n",
       "...                          ...   \n",
       "5379                      Comedy   \n",
       "5380                       Drama   \n",
       "5381      Animation|Short|Action   \n",
       "5382     Action|Adventure|Sci-Fi   \n",
       "5383             Action|Thriller   \n",
       "\n",
       "                                                 Poster         genrelst  \n",
       "0     https://images-na.ssl-images-amazon.com/images...  [0, 1, 0, 0, 0]  \n",
       "1     https://images-na.ssl-images-amazon.com/images...  [0, 1, 1, 0, 0]  \n",
       "2     https://images-na.ssl-images-amazon.com/images...  [0, 1, 0, 0, 0]  \n",
       "3     https://images-na.ssl-images-amazon.com/images...  [1, 0, 0, 0, 0]  \n",
       "4     https://images-na.ssl-images-amazon.com/images...  [1, 1, 1, 0, 0]  \n",
       "...                                                 ...              ...  \n",
       "5379  https://images-na.ssl-images-amazon.com/images...  [0, 1, 0, 0, 0]  \n",
       "5380  https://images-na.ssl-images-amazon.com/images...  [1, 0, 0, 0, 0]  \n",
       "5381  https://images-na.ssl-images-amazon.com/images...  [0, 0, 0, 1, 0]  \n",
       "5382  https://images-na.ssl-images-amazon.com/images...  [0, 0, 0, 1, 0]  \n",
       "5383  https://images-na.ssl-images-amazon.com/images...  [0, 0, 0, 1, 0]  \n",
       "\n",
       "[5384 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df_reset = y_df.reset_index()\n",
    "\n",
    "shape = y_df_reset.shape[1]\n",
    "\n",
    "index_value = []\n",
    "genre_lst = []\n",
    "\n",
    "for i in range(len(y_df_reset)):\n",
    "    index_value.append(int(y_df_reset.loc[i,\"index\"]))\n",
    "    temp_list = []\n",
    "    for j in y_df_reset.columns[1:]:\n",
    "        temp_list.append(y_df_reset.loc[i,j])\n",
    "    genre_lst.append(temp_list)\n",
    "\n",
    "df = pd.DataFrame(list(zip(index_value, genre_lst)),\n",
    "               columns =['imdbId', 'genrelst'])\n",
    "\n",
    "result = dataset2.merge(df, on=\"imdbId\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(result)):\n",
    "    tempY.append((int(result['imdbId'].iloc[x]),result['genrelst'].iloc[x]))\n",
    "\n",
    "#tempY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4307"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting the length of training data\n",
    "length=int(len(flist)*training_size)\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the data about the images that are available\n",
    "i=0\n",
    "for filename in flist:\n",
    "    name=int(filename.split('/')[-1][:-4])\n",
    "    for z in tempY:\n",
    "        if(z[0]==name):\n",
    "            \n",
    "            img = array(cv2.imread(filename))\n",
    "            img = swapaxes(img, 2,0)\n",
    "            img = swapaxes(img, 2,1)\n",
    "\n",
    "            if(i<length):\n",
    "                x_train.append(img)\n",
    "                y_train.append(z[1])\n",
    "                i+=1\n",
    "            else:\n",
    "                x_test.append(img)\n",
    "                y_test.append(z[1])\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data from lists to numpy arrays\n",
    "x_train=asarray(x_train,dtype=float)\n",
    "x_test=asarray(x_test,dtype=float)\n",
    "y_train=asarray(y_train,dtype=float)\n",
    "y_test=asarray(y_test,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling down the RGB data\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4307, 3, 268, 182)\n",
      "4307 train samples\n",
      "1077 test samples\n"
     ]
    }
   ],
   "source": [
    "#printing stats about the features\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = x_train.shape[0]\n",
    "\n",
    "x_train=torch.from_numpy(x_train)\n",
    "x_test=torch.from_numpy(x_test)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_test=torch.from_numpy(y_test)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train, y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(x_test, y_test)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric calculation\n",
    "\n",
    "def metric(scores, targets):\n",
    "    \"\"\"\n",
    "    :param scores: the output the model predict\n",
    "    :param targets: the gt label\n",
    "    :return: OP, OR, OF1, CP, CR, CF1\n",
    "    calculate the Precision of every class by: TP/TP+FP i.e. TP/total predict\n",
    "    calculate the Recall by: TP/total GT\n",
    "    \"\"\"\n",
    "    num, num_class = scores.shape\n",
    "    gt_num = np.zeros(num_class)\n",
    "    tp_num = np.zeros(num_class)\n",
    "    predict_num = np.zeros(num_class)\n",
    "\n",
    "\n",
    "    for index in range(num_class):\n",
    "        score = scores[:, index]\n",
    "        target = targets[:, index]\n",
    "\n",
    "        gt_num[index] = np.sum(target == 1)\n",
    "        predict_num[index] = np.sum(score >= 0.5)\n",
    "        tp_num[index] = np.sum(target * (score >= 0.5))\n",
    "\n",
    "    predict_num[predict_num == 0] = 1  # avoid dividing 0\n",
    "    OP = np.sum(tp_num) / np.sum(predict_num) #OP (Overall Precision) is the ratio of the number of correctly predicted positive samples to the total number of positive predictions made by the model\n",
    "    OR = np.sum(tp_num) / np.sum(gt_num) #OR (Overall Recall) is the ratio of the number of correctly predicted positive samples to the total number of positive samples in the ground truth.\n",
    "    OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n",
    "\n",
    "    return OP, OR, OF1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 model\n",
    "from torchvision import models\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(2048, len(classes))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNet()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4307 (0%)]\tLoss: 0.663779 \tOP: 0.464286\tOR: 0.128713\tOF1: 0.201550\n",
      "Train Epoch: 0 [64/4307 (1%)]\tLoss: 0.611307 \tOP: 0.727273\tOR: 0.155340\tOF1: 0.256000\n",
      "Train Epoch: 0 [128/4307 (3%)]\tLoss: 0.629323 \tOP: 0.521739\tOR: 0.123711\tOF1: 0.200000\n",
      "Train Epoch: 0 [192/4307 (4%)]\tLoss: 0.604056 \tOP: 0.388889\tOR: 0.081395\tOF1: 0.134615\n",
      "Train Epoch: 0 [256/4307 (6%)]\tLoss: 0.611843 \tOP: 0.760000\tOR: 0.193878\tOF1: 0.308943\n",
      "Train Epoch: 0 [320/4307 (7%)]\tLoss: 0.574073 \tOP: 0.684211\tOR: 0.142857\tOF1: 0.236364\n",
      "Train Epoch: 0 [384/4307 (9%)]\tLoss: 0.590977 \tOP: 0.680000\tOR: 0.161905\tOF1: 0.261538\n",
      "Train Epoch: 0 [448/4307 (10%)]\tLoss: 0.634496 \tOP: 0.666667\tOR: 0.117647\tOF1: 0.200000\n",
      "Train Epoch: 0 [512/4307 (12%)]\tLoss: 0.625372 \tOP: 0.571429\tOR: 0.116505\tOF1: 0.193548\n",
      "Train Epoch: 0 [576/4307 (13%)]\tLoss: 0.645597 \tOP: 0.700000\tOR: 0.126126\tOF1: 0.213740\n",
      "Train Epoch: 0 [640/4307 (15%)]\tLoss: 0.578532 \tOP: 0.772727\tOR: 0.170000\tOF1: 0.278689\n",
      "Train Epoch: 0 [704/4307 (16%)]\tLoss: 0.588233 \tOP: 0.541667\tOR: 0.136842\tOF1: 0.218487\n",
      "Train Epoch: 0 [768/4307 (18%)]\tLoss: 0.564480 \tOP: 0.807692\tOR: 0.216495\tOF1: 0.341463\n",
      "Train Epoch: 0 [832/4307 (19%)]\tLoss: 0.559991 \tOP: 0.684211\tOR: 0.138298\tOF1: 0.230088\n",
      "Train Epoch: 0 [896/4307 (21%)]\tLoss: 0.593533 \tOP: 0.608696\tOR: 0.145833\tOF1: 0.235294\n",
      "Train Epoch: 0 [960/4307 (22%)]\tLoss: 0.602232 \tOP: 0.800000\tOR: 0.188679\tOF1: 0.305344\n",
      "Train Epoch: 0 [1024/4307 (24%)]\tLoss: 0.635381 \tOP: 0.566667\tOR: 0.165049\tOF1: 0.255639\n",
      "Train Epoch: 0 [1088/4307 (25%)]\tLoss: 0.645771 \tOP: 0.692308\tOR: 0.165138\tOF1: 0.266667\n",
      "Train Epoch: 0 [1152/4307 (26%)]\tLoss: 0.632655 \tOP: 0.560000\tOR: 0.127273\tOF1: 0.207407\n",
      "Train Epoch: 0 [1216/4307 (28%)]\tLoss: 0.608604 \tOP: 0.666667\tOR: 0.196078\tOF1: 0.303030\n",
      "Train Epoch: 0 [1280/4307 (29%)]\tLoss: 0.646318 \tOP: 0.620690\tOR: 0.193548\tOF1: 0.295082\n",
      "Train Epoch: 0 [1344/4307 (31%)]\tLoss: 0.555835 \tOP: 0.714286\tOR: 0.271739\tOF1: 0.393701\n",
      "Train Epoch: 0 [1408/4307 (32%)]\tLoss: 0.584668 \tOP: 0.774194\tOR: 0.226415\tOF1: 0.350365\n",
      "Train Epoch: 0 [1472/4307 (34%)]\tLoss: 0.600305 \tOP: 0.703704\tOR: 0.197917\tOF1: 0.308943\n",
      "Train Epoch: 0 [1536/4307 (35%)]\tLoss: 0.601419 \tOP: 0.612903\tOR: 0.190000\tOF1: 0.290076\n",
      "Train Epoch: 0 [1600/4307 (37%)]\tLoss: 0.575106 \tOP: 0.705882\tOR: 0.226415\tOF1: 0.342857\n",
      "Train Epoch: 0 [1664/4307 (38%)]\tLoss: 0.571461 \tOP: 0.700000\tOR: 0.233333\tOF1: 0.350000\n",
      "Train Epoch: 0 [1728/4307 (40%)]\tLoss: 0.561117 \tOP: 0.676471\tOR: 0.242105\tOF1: 0.356589\n",
      "Train Epoch: 0 [1792/4307 (41%)]\tLoss: 0.598458 \tOP: 0.709677\tOR: 0.224490\tOF1: 0.341085\n",
      "Train Epoch: 0 [1856/4307 (43%)]\tLoss: 0.617738 \tOP: 0.588235\tOR: 0.204082\tOF1: 0.303030\n",
      "Train Epoch: 0 [1920/4307 (44%)]\tLoss: 0.610063 \tOP: 0.600000\tOR: 0.159574\tOF1: 0.252101\n",
      "Train Epoch: 0 [1984/4307 (46%)]\tLoss: 0.579869 \tOP: 0.727273\tOR: 0.252632\tOF1: 0.375000\n",
      "Train Epoch: 0 [2048/4307 (47%)]\tLoss: 0.571063 \tOP: 0.642857\tOR: 0.197802\tOF1: 0.302521\n",
      "Train Epoch: 0 [2112/4307 (49%)]\tLoss: 0.587528 \tOP: 0.592593\tOR: 0.160000\tOF1: 0.251969\n",
      "Train Epoch: 0 [2176/4307 (50%)]\tLoss: 0.589781 \tOP: 0.741935\tOR: 0.214953\tOF1: 0.333333\n",
      "Train Epoch: 0 [2240/4307 (51%)]\tLoss: 0.602269 \tOP: 0.680000\tOR: 0.180851\tOF1: 0.285714\n",
      "Train Epoch: 0 [2304/4307 (53%)]\tLoss: 0.597416 \tOP: 0.600000\tOR: 0.150000\tOF1: 0.240000\n",
      "Train Epoch: 0 [2368/4307 (54%)]\tLoss: 0.632265 \tOP: 0.619048\tOR: 0.120370\tOF1: 0.201550\n",
      "Train Epoch: 0 [2432/4307 (56%)]\tLoss: 0.608154 \tOP: 0.629630\tOR: 0.178947\tOF1: 0.278689\n",
      "Train Epoch: 0 [2496/4307 (57%)]\tLoss: 0.584407 \tOP: 0.640000\tOR: 0.175824\tOF1: 0.275862\n",
      "Train Epoch: 0 [2560/4307 (59%)]\tLoss: 0.599997 \tOP: 0.588235\tOR: 0.091743\tOF1: 0.158730\n",
      "Train Epoch: 0 [2624/4307 (60%)]\tLoss: 0.622808 \tOP: 0.521739\tOR: 0.114286\tOF1: 0.187500\n",
      "Train Epoch: 0 [2688/4307 (62%)]\tLoss: 0.619532 \tOP: 0.655172\tOR: 0.188119\tOF1: 0.292308\n",
      "Train Epoch: 0 [2752/4307 (63%)]\tLoss: 0.591377 \tOP: 0.764706\tOR: 0.126214\tOF1: 0.216667\n",
      "Train Epoch: 0 [2816/4307 (65%)]\tLoss: 0.552650 \tOP: 0.740741\tOR: 0.202020\tOF1: 0.317460\n",
      "Train Epoch: 0 [2880/4307 (66%)]\tLoss: 0.605625 \tOP: 0.772727\tOR: 0.173469\tOF1: 0.283333\n",
      "Train Epoch: 0 [2944/4307 (68%)]\tLoss: 0.622205 \tOP: 0.607143\tOR: 0.173469\tOF1: 0.269841\n",
      "Train Epoch: 0 [3008/4307 (69%)]\tLoss: 0.591825 \tOP: 0.709677\tOR: 0.211538\tOF1: 0.325926\n",
      "Train Epoch: 0 [3072/4307 (71%)]\tLoss: 0.600396 \tOP: 0.518519\tOR: 0.142857\tOF1: 0.224000\n",
      "Train Epoch: 0 [3136/4307 (72%)]\tLoss: 0.594820 \tOP: 0.681818\tOR: 0.147059\tOF1: 0.241935\n",
      "Train Epoch: 0 [3200/4307 (74%)]\tLoss: 0.589893 \tOP: 0.781250\tOR: 0.265957\tOF1: 0.396825\n",
      "Train Epoch: 0 [3264/4307 (75%)]\tLoss: 0.580899 \tOP: 0.730769\tOR: 0.195876\tOF1: 0.308943\n",
      "Train Epoch: 0 [3328/4307 (76%)]\tLoss: 0.610474 \tOP: 0.766667\tOR: 0.230000\tOF1: 0.353846\n",
      "Train Epoch: 0 [3392/4307 (78%)]\tLoss: 0.539196 \tOP: 0.777778\tOR: 0.277228\tOF1: 0.408759\n",
      "Train Epoch: 0 [3456/4307 (79%)]\tLoss: 0.600746 \tOP: 0.653846\tOR: 0.160377\tOF1: 0.257576\n",
      "Train Epoch: 0 [3520/4307 (81%)]\tLoss: 0.603858 \tOP: 0.655172\tOR: 0.188119\tOF1: 0.292308\n",
      "Train Epoch: 0 [3584/4307 (82%)]\tLoss: 0.647703 \tOP: 0.703704\tOR: 0.182692\tOF1: 0.290076\n",
      "Train Epoch: 0 [3648/4307 (84%)]\tLoss: 0.594966 \tOP: 0.625000\tOR: 0.141509\tOF1: 0.230769\n",
      "Train Epoch: 0 [3712/4307 (85%)]\tLoss: 0.588799 \tOP: 0.760000\tOR: 0.186275\tOF1: 0.299213\n",
      "Train Epoch: 0 [3776/4307 (87%)]\tLoss: 0.619175 \tOP: 0.607143\tOR: 0.157407\tOF1: 0.250000\n",
      "Train Epoch: 0 [3840/4307 (88%)]\tLoss: 0.570956 \tOP: 0.645161\tOR: 0.206186\tOF1: 0.312500\n",
      "Train Epoch: 0 [3904/4307 (90%)]\tLoss: 0.623481 \tOP: 0.642857\tOR: 0.189474\tOF1: 0.292683\n",
      "Train Epoch: 0 [3968/4307 (91%)]\tLoss: 0.611253 \tOP: 0.694444\tOR: 0.265957\tOF1: 0.384615\n",
      "Train Epoch: 0 [4032/4307 (93%)]\tLoss: 0.590059 \tOP: 0.740741\tOR: 0.194175\tOF1: 0.307692\n",
      "Train Epoch: 0 [4096/4307 (94%)]\tLoss: 0.593989 \tOP: 0.708333\tOR: 0.173469\tOF1: 0.278689\n",
      "Train Epoch: 0 [4160/4307 (96%)]\tLoss: 0.583059 \tOP: 0.742857\tOR: 0.250000\tOF1: 0.374101\n",
      "Train Epoch: 0 [4224/4307 (97%)]\tLoss: 0.640751 \tOP: 0.560000\tOR: 0.130841\tOF1: 0.212121\n",
      "Train Epoch: 0 [1273/4307 (99%)]\tLoss: 0.572207 \tOP: 0.555556\tOR: 0.192308\tOF1: 0.285714\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-9fe02c90224d>:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5336 \n",
      "OP: 0.689040\n",
      "OR: 0.196484\n",
      "OF1: 0.304910\n",
      "\n",
      "Train Epoch: 1 [0/4307 (0%)]\tLoss: 0.523886 \tOP: 0.833333\tOR: 0.309278\tOF1: 0.451128\n",
      "Train Epoch: 1 [64/4307 (1%)]\tLoss: 0.558500 \tOP: 0.703704\tOR: 0.188119\tOF1: 0.296875\n",
      "Train Epoch: 1 [128/4307 (3%)]\tLoss: 0.553790 \tOP: 0.720000\tOR: 0.178218\tOF1: 0.285714\n",
      "Train Epoch: 1 [192/4307 (4%)]\tLoss: 0.528563 \tOP: 0.928571\tOR: 0.254902\tOF1: 0.400000\n",
      "Train Epoch: 1 [256/4307 (6%)]\tLoss: 0.545514 \tOP: 0.705882\tOR: 0.255319\tOF1: 0.375000\n",
      "Train Epoch: 1 [320/4307 (7%)]\tLoss: 0.516800 \tOP: 0.862069\tOR: 0.240385\tOF1: 0.375940\n",
      "Train Epoch: 1 [384/4307 (9%)]\tLoss: 0.502045 \tOP: 0.846154\tOR: 0.358696\tOF1: 0.503817\n",
      "Train Epoch: 1 [448/4307 (10%)]\tLoss: 0.546841 \tOP: 0.758621\tOR: 0.229167\tOF1: 0.352000\n",
      "Train Epoch: 1 [512/4307 (12%)]\tLoss: 0.523004 \tOP: 0.777778\tOR: 0.223404\tOF1: 0.347107\n",
      "Train Epoch: 1 [576/4307 (13%)]\tLoss: 0.540657 \tOP: 0.814815\tOR: 0.213592\tOF1: 0.338462\n",
      "Train Epoch: 1 [640/4307 (15%)]\tLoss: 0.571775 \tOP: 0.933333\tOR: 0.256881\tOF1: 0.402878\n",
      "Train Epoch: 1 [704/4307 (16%)]\tLoss: 0.535731 \tOP: 0.800000\tOR: 0.242424\tOF1: 0.372093\n",
      "Train Epoch: 1 [768/4307 (18%)]\tLoss: 0.508827 \tOP: 0.866667\tOR: 0.254902\tOF1: 0.393939\n",
      "Train Epoch: 1 [832/4307 (19%)]\tLoss: 0.538242 \tOP: 0.833333\tOR: 0.235849\tOF1: 0.367647\n",
      "Train Epoch: 1 [896/4307 (21%)]\tLoss: 0.569435 \tOP: 0.783784\tOR: 0.278846\tOF1: 0.411348\n",
      "Train Epoch: 1 [960/4307 (22%)]\tLoss: 0.533552 \tOP: 0.878788\tOR: 0.295918\tOF1: 0.442748\n",
      "Train Epoch: 1 [1024/4307 (24%)]\tLoss: 0.573903 \tOP: 0.848485\tOR: 0.264151\tOF1: 0.402878\n",
      "Train Epoch: 1 [1088/4307 (25%)]\tLoss: 0.534631 \tOP: 0.758621\tOR: 0.231579\tOF1: 0.354839\n",
      "Train Epoch: 1 [1152/4307 (26%)]\tLoss: 0.534977 \tOP: 0.931034\tOR: 0.247706\tOF1: 0.391304\n",
      "Train Epoch: 1 [1216/4307 (28%)]\tLoss: 0.564058 \tOP: 0.818182\tOR: 0.252336\tOF1: 0.385714\n",
      "Train Epoch: 1 [1280/4307 (29%)]\tLoss: 0.543802 \tOP: 0.785714\tOR: 0.220000\tOF1: 0.343750\n",
      "Train Epoch: 1 [1344/4307 (31%)]\tLoss: 0.565430 \tOP: 0.880000\tOR: 0.203704\tOF1: 0.330827\n",
      "Train Epoch: 1 [1408/4307 (32%)]\tLoss: 0.563389 \tOP: 0.805556\tOR: 0.295918\tOF1: 0.432836\n",
      "Train Epoch: 1 [1472/4307 (34%)]\tLoss: 0.498113 \tOP: 0.891892\tOR: 0.370787\tOF1: 0.523810\n",
      "Train Epoch: 1 [1536/4307 (35%)]\tLoss: 0.559453 \tOP: 0.707317\tOR: 0.329545\tOF1: 0.449612\n",
      "Train Epoch: 1 [1600/4307 (37%)]\tLoss: 0.518414 \tOP: 0.743590\tOR: 0.290000\tOF1: 0.417266\n",
      "Train Epoch: 1 [1664/4307 (38%)]\tLoss: 0.540315 \tOP: 0.756757\tOR: 0.297872\tOF1: 0.427481\n",
      "Train Epoch: 1 [1728/4307 (40%)]\tLoss: 0.586176 \tOP: 0.593750\tOR: 0.202128\tOF1: 0.301587\n",
      "Train Epoch: 1 [1792/4307 (41%)]\tLoss: 0.556733 \tOP: 0.794118\tOR: 0.275510\tOF1: 0.409091\n",
      "Train Epoch: 1 [1856/4307 (43%)]\tLoss: 0.493778 \tOP: 0.913043\tOR: 0.214286\tOF1: 0.347107\n",
      "Train Epoch: 1 [1920/4307 (44%)]\tLoss: 0.526099 \tOP: 0.846154\tOR: 0.305556\tOF1: 0.448980\n",
      "Train Epoch: 1 [1984/4307 (46%)]\tLoss: 0.535469 \tOP: 0.775000\tOR: 0.322917\tOF1: 0.455882\n",
      "Train Epoch: 1 [2048/4307 (47%)]\tLoss: 0.536758 \tOP: 0.781250\tOR: 0.233645\tOF1: 0.359712\n",
      "Train Epoch: 1 [2112/4307 (49%)]\tLoss: 0.546178 \tOP: 0.705882\tOR: 0.220183\tOF1: 0.335664\n",
      "Train Epoch: 1 [2176/4307 (50%)]\tLoss: 0.552680 \tOP: 0.800000\tOR: 0.235294\tOF1: 0.363636\n",
      "Train Epoch: 1 [2240/4307 (51%)]\tLoss: 0.498095 \tOP: 0.864865\tOR: 0.336842\tOF1: 0.484848\n",
      "Train Epoch: 1 [2304/4307 (53%)]\tLoss: 0.566649 \tOP: 0.764706\tOR: 0.254902\tOF1: 0.382353\n",
      "Train Epoch: 1 [2368/4307 (54%)]\tLoss: 0.553339 \tOP: 0.833333\tOR: 0.265487\tOF1: 0.402685\n",
      "Train Epoch: 1 [2432/4307 (56%)]\tLoss: 0.558778 \tOP: 0.793103\tOR: 0.219048\tOF1: 0.343284\n",
      "Train Epoch: 1 [2496/4307 (57%)]\tLoss: 0.613408 \tOP: 0.645161\tOR: 0.202020\tOF1: 0.307692\n",
      "Train Epoch: 1 [2560/4307 (59%)]\tLoss: 0.521424 \tOP: 0.906250\tOR: 0.292929\tOF1: 0.442748\n",
      "Train Epoch: 1 [2624/4307 (60%)]\tLoss: 0.553308 \tOP: 0.750000\tOR: 0.260870\tOF1: 0.387097\n",
      "Train Epoch: 1 [2688/4307 (62%)]\tLoss: 0.551490 \tOP: 0.923077\tOR: 0.240000\tOF1: 0.380952\n",
      "Train Epoch: 1 [2752/4307 (63%)]\tLoss: 0.509070 \tOP: 0.818182\tOR: 0.254717\tOF1: 0.388489\n",
      "Train Epoch: 1 [2816/4307 (65%)]\tLoss: 0.512845 \tOP: 0.827586\tOR: 0.263736\tOF1: 0.400000\n",
      "Train Epoch: 1 [2880/4307 (66%)]\tLoss: 0.508500 \tOP: 0.864865\tOR: 0.310680\tOF1: 0.457143\n",
      "Train Epoch: 1 [2944/4307 (68%)]\tLoss: 0.550567 \tOP: 0.850000\tOR: 0.346939\tOF1: 0.492754\n",
      "Train Epoch: 1 [3008/4307 (69%)]\tLoss: 0.518753 \tOP: 0.894737\tOR: 0.320755\tOF1: 0.472222\n",
      "Train Epoch: 1 [3072/4307 (71%)]\tLoss: 0.534708 \tOP: 0.800000\tOR: 0.269231\tOF1: 0.402878\n",
      "Train Epoch: 1 [3136/4307 (72%)]\tLoss: 0.547727 \tOP: 0.787879\tOR: 0.262626\tOF1: 0.393939\n",
      "Train Epoch: 1 [3200/4307 (74%)]\tLoss: 0.566228 \tOP: 0.666667\tOR: 0.255319\tOF1: 0.369231\n",
      "Train Epoch: 1 [3264/4307 (75%)]\tLoss: 0.574420 \tOP: 0.800000\tOR: 0.282828\tOF1: 0.417910\n",
      "Train Epoch: 1 [3328/4307 (76%)]\tLoss: 0.517887 \tOP: 0.806452\tOR: 0.235849\tOF1: 0.364964\n",
      "Train Epoch: 1 [3392/4307 (78%)]\tLoss: 0.563654 \tOP: 0.750000\tOR: 0.244898\tOF1: 0.369231\n",
      "Train Epoch: 1 [3456/4307 (79%)]\tLoss: 0.549063 \tOP: 0.843750\tOR: 0.272727\tOF1: 0.412214\n",
      "Train Epoch: 1 [3520/4307 (81%)]\tLoss: 0.540561 \tOP: 0.878049\tOR: 0.330275\tOF1: 0.480000\n",
      "Train Epoch: 1 [3584/4307 (82%)]\tLoss: 0.576618 \tOP: 0.757576\tOR: 0.252525\tOF1: 0.378788\n",
      "Train Epoch: 1 [3648/4307 (84%)]\tLoss: 0.561220 \tOP: 0.815789\tOR: 0.322917\tOF1: 0.462687\n",
      "Train Epoch: 1 [3712/4307 (85%)]\tLoss: 0.532877 \tOP: 0.783784\tOR: 0.308511\tOF1: 0.442748\n",
      "Train Epoch: 1 [3776/4307 (87%)]\tLoss: 0.548649 \tOP: 0.615385\tOR: 0.260870\tOF1: 0.366412\n",
      "Train Epoch: 1 [3840/4307 (88%)]\tLoss: 0.531901 \tOP: 0.763158\tOR: 0.290000\tOF1: 0.420290\n",
      "Train Epoch: 1 [3904/4307 (90%)]\tLoss: 0.532297 \tOP: 0.774194\tOR: 0.247423\tOF1: 0.375000\n",
      "Train Epoch: 1 [3968/4307 (91%)]\tLoss: 0.549362 \tOP: 0.780488\tOR: 0.347826\tOF1: 0.481203\n",
      "Train Epoch: 1 [4032/4307 (93%)]\tLoss: 0.540669 \tOP: 0.857143\tOR: 0.252632\tOF1: 0.390244\n",
      "Train Epoch: 1 [4096/4307 (94%)]\tLoss: 0.518259 \tOP: 0.875000\tOR: 0.277228\tOF1: 0.421053\n",
      "Train Epoch: 1 [4160/4307 (96%)]\tLoss: 0.525991 \tOP: 0.828571\tOR: 0.284314\tOF1: 0.423358\n",
      "Train Epoch: 1 [4224/4307 (97%)]\tLoss: 0.519712 \tOP: 0.821429\tOR: 0.244681\tOF1: 0.377049\n",
      "Train Epoch: 1 [1273/4307 (99%)]\tLoss: 0.579122 \tOP: 0.636364\tOR: 0.225806\tOF1: 0.333333\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5039 \n",
      "OP: 0.715385\n",
      "OR: 0.369511\n",
      "OF1: 0.486932\n",
      "\n",
      "Train Epoch: 2 [0/4307 (0%)]\tLoss: 0.494758 \tOP: 0.897959\tOR: 0.427184\tOF1: 0.578947\n",
      "Train Epoch: 2 [64/4307 (1%)]\tLoss: 0.505029 \tOP: 0.875000\tOR: 0.271845\tOF1: 0.414815\n",
      "Train Epoch: 2 [128/4307 (3%)]\tLoss: 0.496157 \tOP: 0.933333\tOR: 0.407767\tOF1: 0.567568\n",
      "Train Epoch: 2 [192/4307 (4%)]\tLoss: 0.448417 \tOP: 1.000000\tOR: 0.351064\tOF1: 0.519685\n",
      "Train Epoch: 2 [256/4307 (6%)]\tLoss: 0.489705 \tOP: 0.914286\tOR: 0.344086\tOF1: 0.500000\n",
      "Train Epoch: 2 [320/4307 (7%)]\tLoss: 0.512886 \tOP: 0.970588\tOR: 0.323529\tOF1: 0.485294\n",
      "Train Epoch: 2 [384/4307 (9%)]\tLoss: 0.540873 \tOP: 0.870968\tOR: 0.272727\tOF1: 0.415385\n",
      "Train Epoch: 2 [448/4307 (10%)]\tLoss: 0.483168 \tOP: 0.906250\tOR: 0.311828\tOF1: 0.464000\n",
      "Train Epoch: 2 [512/4307 (12%)]\tLoss: 0.540160 \tOP: 0.906250\tOR: 0.268519\tOF1: 0.414286\n",
      "Train Epoch: 2 [576/4307 (13%)]\tLoss: 0.460996 \tOP: 0.970588\tOR: 0.347368\tOF1: 0.511628\n",
      "Train Epoch: 2 [640/4307 (15%)]\tLoss: 0.468561 \tOP: 0.903226\tOR: 0.301075\tOF1: 0.451613\n",
      "Train Epoch: 2 [704/4307 (16%)]\tLoss: 0.458621 \tOP: 0.936170\tOR: 0.440000\tOF1: 0.598639\n",
      "Train Epoch: 2 [768/4307 (18%)]\tLoss: 0.499734 \tOP: 0.921053\tOR: 0.336538\tOF1: 0.492958\n",
      "Train Epoch: 2 [832/4307 (19%)]\tLoss: 0.505027 \tOP: 0.894737\tOR: 0.350515\tOF1: 0.503704\n",
      "Train Epoch: 2 [896/4307 (21%)]\tLoss: 0.502847 \tOP: 0.804878\tOR: 0.343750\tOF1: 0.481752\n",
      "Train Epoch: 2 [960/4307 (22%)]\tLoss: 0.519477 \tOP: 0.833333\tOR: 0.257732\tOF1: 0.393701\n",
      "Train Epoch: 2 [1024/4307 (24%)]\tLoss: 0.503853 \tOP: 0.883721\tOR: 0.368932\tOF1: 0.520548\n",
      "Train Epoch: 2 [1088/4307 (25%)]\tLoss: 0.504806 \tOP: 0.800000\tOR: 0.336842\tOF1: 0.474074\n",
      "Train Epoch: 2 [1152/4307 (26%)]\tLoss: 0.512656 \tOP: 0.805556\tOR: 0.287129\tOF1: 0.423358\n",
      "Train Epoch: 2 [1216/4307 (28%)]\tLoss: 0.487497 \tOP: 0.852941\tOR: 0.298969\tOF1: 0.442748\n",
      "Train Epoch: 2 [1280/4307 (29%)]\tLoss: 0.510307 \tOP: 0.818182\tOR: 0.272727\tOF1: 0.409091\n",
      "Train Epoch: 2 [1344/4307 (31%)]\tLoss: 0.496962 \tOP: 0.818182\tOR: 0.290323\tOF1: 0.428571\n",
      "Train Epoch: 2 [1408/4307 (32%)]\tLoss: 0.432241 \tOP: 0.952381\tOR: 0.404040\tOF1: 0.567376\n",
      "Train Epoch: 2 [1472/4307 (34%)]\tLoss: 0.498462 \tOP: 0.945946\tOR: 0.327103\tOF1: 0.486111\n",
      "Train Epoch: 2 [1536/4307 (35%)]\tLoss: 0.478449 \tOP: 0.950000\tOR: 0.365385\tOF1: 0.527778\n",
      "Train Epoch: 2 [1600/4307 (37%)]\tLoss: 0.473810 \tOP: 0.900000\tOR: 0.356436\tOF1: 0.510638\n",
      "Train Epoch: 2 [1664/4307 (38%)]\tLoss: 0.477501 \tOP: 1.000000\tOR: 0.340000\tOF1: 0.507463\n",
      "Train Epoch: 2 [1728/4307 (40%)]\tLoss: 0.518580 \tOP: 0.878788\tOR: 0.302083\tOF1: 0.449612\n",
      "Train Epoch: 2 [1792/4307 (41%)]\tLoss: 0.498208 \tOP: 0.884615\tOR: 0.252747\tOF1: 0.393162\n",
      "Train Epoch: 2 [1856/4307 (43%)]\tLoss: 0.480811 \tOP: 0.931034\tOR: 0.287234\tOF1: 0.439024\n",
      "Train Epoch: 2 [1920/4307 (44%)]\tLoss: 0.455825 \tOP: 0.942857\tOR: 0.351064\tOF1: 0.511628\n",
      "Train Epoch: 2 [1984/4307 (46%)]\tLoss: 0.520722 \tOP: 0.941176\tOR: 0.310680\tOF1: 0.467153\n",
      "Train Epoch: 2 [2048/4307 (47%)]\tLoss: 0.486732 \tOP: 0.883721\tOR: 0.376238\tOF1: 0.527778\n",
      "Train Epoch: 2 [2112/4307 (49%)]\tLoss: 0.497349 \tOP: 0.794872\tOR: 0.298077\tOF1: 0.433566\n",
      "Train Epoch: 2 [2176/4307 (50%)]\tLoss: 0.495526 \tOP: 0.868421\tOR: 0.330000\tOF1: 0.478261\n",
      "Train Epoch: 2 [2240/4307 (51%)]\tLoss: 0.507633 \tOP: 0.925000\tOR: 0.336364\tOF1: 0.493333\n",
      "Train Epoch: 2 [2304/4307 (53%)]\tLoss: 0.506999 \tOP: 0.931818\tOR: 0.383178\tOF1: 0.543046\n",
      "Train Epoch: 2 [2368/4307 (54%)]\tLoss: 0.512985 \tOP: 0.900000\tOR: 0.270000\tOF1: 0.415385\n",
      "Train Epoch: 2 [2432/4307 (56%)]\tLoss: 0.461735 \tOP: 0.914894\tOR: 0.438776\tOF1: 0.593103\n",
      "Train Epoch: 2 [2496/4307 (57%)]\tLoss: 0.468332 \tOP: 0.894737\tOR: 0.373626\tOF1: 0.527132\n",
      "Train Epoch: 2 [2560/4307 (59%)]\tLoss: 0.484724 \tOP: 0.837209\tOR: 0.327273\tOF1: 0.470588\n",
      "Train Epoch: 2 [2624/4307 (60%)]\tLoss: 0.531555 \tOP: 0.878788\tOR: 0.276190\tOF1: 0.420290\n",
      "Train Epoch: 2 [2688/4307 (62%)]\tLoss: 0.508401 \tOP: 0.829268\tOR: 0.336634\tOF1: 0.478873\n",
      "Train Epoch: 2 [2752/4307 (63%)]\tLoss: 0.487768 \tOP: 0.750000\tOR: 0.358696\tOF1: 0.485294\n",
      "Train Epoch: 2 [2816/4307 (65%)]\tLoss: 0.481974 \tOP: 0.888889\tOR: 0.326531\tOF1: 0.477612\n",
      "Train Epoch: 2 [2880/4307 (66%)]\tLoss: 0.492334 \tOP: 0.918919\tOR: 0.354167\tOF1: 0.511278\n",
      "Train Epoch: 2 [2944/4307 (68%)]\tLoss: 0.461975 \tOP: 0.904762\tOR: 0.387755\tOF1: 0.542857\n",
      "Train Epoch: 2 [3008/4307 (69%)]\tLoss: 0.492350 \tOP: 0.857143\tOR: 0.309278\tOF1: 0.454545\n",
      "Train Epoch: 2 [3072/4307 (71%)]\tLoss: 0.500826 \tOP: 0.852941\tOR: 0.292929\tOF1: 0.436090\n",
      "Train Epoch: 2 [3136/4307 (72%)]\tLoss: 0.480500 \tOP: 0.933333\tOR: 0.288660\tOF1: 0.440945\n",
      "Train Epoch: 2 [3200/4307 (74%)]\tLoss: 0.499801 \tOP: 0.853659\tOR: 0.364583\tOF1: 0.510949\n",
      "Train Epoch: 2 [3264/4307 (75%)]\tLoss: 0.521960 \tOP: 0.800000\tOR: 0.291667\tOF1: 0.427481\n",
      "Train Epoch: 2 [3328/4307 (76%)]\tLoss: 0.532737 \tOP: 0.903226\tOR: 0.252252\tOF1: 0.394366\n",
      "Train Epoch: 2 [3392/4307 (78%)]\tLoss: 0.530215 \tOP: 0.861111\tOR: 0.319588\tOF1: 0.466165\n",
      "Train Epoch: 2 [3456/4307 (79%)]\tLoss: 0.484572 \tOP: 0.868421\tOR: 0.326733\tOF1: 0.474820\n",
      "Train Epoch: 2 [3520/4307 (81%)]\tLoss: 0.493433 \tOP: 0.825000\tOR: 0.320388\tOF1: 0.461538\n",
      "Train Epoch: 2 [3584/4307 (82%)]\tLoss: 0.477654 \tOP: 0.926829\tOR: 0.365385\tOF1: 0.524138\n",
      "Train Epoch: 2 [3648/4307 (84%)]\tLoss: 0.511410 \tOP: 0.882353\tOR: 0.303030\tOF1: 0.451128\n",
      "Train Epoch: 2 [3712/4307 (85%)]\tLoss: 0.482491 \tOP: 0.916667\tOR: 0.207547\tOF1: 0.338462\n",
      "Train Epoch: 2 [3776/4307 (87%)]\tLoss: 0.497896 \tOP: 0.809524\tOR: 0.340000\tOF1: 0.478873\n",
      "Train Epoch: 2 [3840/4307 (88%)]\tLoss: 0.559469 \tOP: 0.942857\tOR: 0.292035\tOF1: 0.445946\n",
      "Train Epoch: 2 [3904/4307 (90%)]\tLoss: 0.516408 \tOP: 0.916667\tOR: 0.323529\tOF1: 0.478261\n",
      "Train Epoch: 2 [3968/4307 (91%)]\tLoss: 0.486040 \tOP: 0.882353\tOR: 0.306122\tOF1: 0.454545\n",
      "Train Epoch: 2 [4032/4307 (93%)]\tLoss: 0.533251 \tOP: 0.829268\tOR: 0.354167\tOF1: 0.496350\n",
      "Train Epoch: 2 [4096/4307 (94%)]\tLoss: 0.530601 \tOP: 0.848485\tOR: 0.288660\tOF1: 0.430769\n",
      "Train Epoch: 2 [4160/4307 (96%)]\tLoss: 0.514886 \tOP: 0.785714\tOR: 0.308411\tOF1: 0.442953\n",
      "Train Epoch: 2 [4224/4307 (97%)]\tLoss: 0.505813 \tOP: 0.846154\tOR: 0.317308\tOF1: 0.461538\n",
      "Train Epoch: 2 [1273/4307 (99%)]\tLoss: 0.420256 \tOP: 0.916667\tOR: 0.407407\tOF1: 0.564103\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5166 \n",
      "OP: 0.677723\n",
      "OR: 0.457798\n",
      "OF1: 0.545979\n",
      "\n",
      "Train Epoch: 3 [0/4307 (0%)]\tLoss: 0.473418 \tOP: 0.944444\tOR: 0.323810\tOF1: 0.482270\n",
      "Train Epoch: 3 [64/4307 (1%)]\tLoss: 0.455993 \tOP: 0.976744\tOR: 0.388889\tOF1: 0.556291\n",
      "Train Epoch: 3 [128/4307 (3%)]\tLoss: 0.469105 \tOP: 0.921053\tOR: 0.327103\tOF1: 0.482759\n",
      "Train Epoch: 3 [192/4307 (4%)]\tLoss: 0.478859 \tOP: 0.850000\tOR: 0.346939\tOF1: 0.492754\n",
      "Train Epoch: 3 [256/4307 (6%)]\tLoss: 0.464528 \tOP: 0.937500\tOR: 0.441176\tOF1: 0.600000\n",
      "Train Epoch: 3 [320/4307 (7%)]\tLoss: 0.452051 \tOP: 0.842105\tOR: 0.304762\tOF1: 0.447552\n",
      "Train Epoch: 3 [384/4307 (9%)]\tLoss: 0.506032 \tOP: 0.888889\tOR: 0.307692\tOF1: 0.457143\n",
      "Train Epoch: 3 [448/4307 (10%)]\tLoss: 0.461522 \tOP: 0.933333\tOR: 0.285714\tOF1: 0.437500\n",
      "Train Epoch: 3 [512/4307 (12%)]\tLoss: 0.425275 \tOP: 0.934783\tOR: 0.447917\tOF1: 0.605634\n",
      "Train Epoch: 3 [576/4307 (13%)]\tLoss: 0.456923 \tOP: 0.909091\tOR: 0.412371\tOF1: 0.567376\n",
      "Train Epoch: 3 [640/4307 (15%)]\tLoss: 0.474430 \tOP: 0.970588\tOR: 0.330000\tOF1: 0.492537\n",
      "Train Epoch: 3 [704/4307 (16%)]\tLoss: 0.405019 \tOP: 0.921053\tOR: 0.380435\tOF1: 0.538462\n",
      "Train Epoch: 3 [768/4307 (18%)]\tLoss: 0.484874 \tOP: 0.875000\tOR: 0.346535\tOF1: 0.496454\n",
      "Train Epoch: 3 [832/4307 (19%)]\tLoss: 0.453983 \tOP: 0.934783\tOR: 0.430000\tOF1: 0.589041\n",
      "Train Epoch: 3 [896/4307 (21%)]\tLoss: 0.433877 \tOP: 0.956522\tOR: 0.458333\tOF1: 0.619718\n",
      "Train Epoch: 3 [960/4307 (22%)]\tLoss: 0.443892 \tOP: 0.930233\tOR: 0.425532\tOF1: 0.583942\n",
      "Train Epoch: 3 [1024/4307 (24%)]\tLoss: 0.495319 \tOP: 0.975000\tOR: 0.378641\tOF1: 0.545455\n",
      "Train Epoch: 3 [1088/4307 (25%)]\tLoss: 0.465429 \tOP: 0.926829\tOR: 0.400000\tOF1: 0.558824\n",
      "Train Epoch: 3 [1152/4307 (26%)]\tLoss: 0.548964 \tOP: 0.766667\tOR: 0.252747\tOF1: 0.380165\n",
      "Train Epoch: 3 [1216/4307 (28%)]\tLoss: 0.427413 \tOP: 0.959184\tOR: 0.451923\tOF1: 0.614379\n",
      "Train Epoch: 3 [1280/4307 (29%)]\tLoss: 0.420392 \tOP: 1.000000\tOR: 0.437500\tOF1: 0.608696\n",
      "Train Epoch: 3 [1344/4307 (31%)]\tLoss: 0.469143 \tOP: 0.911765\tOR: 0.329787\tOF1: 0.484375\n",
      "Train Epoch: 3 [1408/4307 (32%)]\tLoss: 0.451089 \tOP: 0.975610\tOR: 0.430108\tOF1: 0.597015\n",
      "Train Epoch: 3 [1472/4307 (34%)]\tLoss: 0.476772 \tOP: 0.942857\tOR: 0.362637\tOF1: 0.523810\n",
      "Train Epoch: 3 [1536/4307 (35%)]\tLoss: 0.482181 \tOP: 0.923077\tOR: 0.349515\tOF1: 0.507042\n",
      "Train Epoch: 3 [1600/4307 (37%)]\tLoss: 0.465792 \tOP: 0.955556\tOR: 0.438776\tOF1: 0.601399\n",
      "Train Epoch: 3 [1664/4307 (38%)]\tLoss: 0.465644 \tOP: 0.888889\tOR: 0.320000\tOF1: 0.470588\n",
      "Train Epoch: 3 [1728/4307 (40%)]\tLoss: 0.475409 \tOP: 0.906977\tOR: 0.410526\tOF1: 0.565217\n",
      "Train Epoch: 3 [1792/4307 (41%)]\tLoss: 0.483753 \tOP: 0.975000\tOR: 0.378641\tOF1: 0.545455\n",
      "Train Epoch: 3 [1856/4307 (43%)]\tLoss: 0.445873 \tOP: 1.000000\tOR: 0.370000\tOF1: 0.540146\n",
      "Train Epoch: 3 [1920/4307 (44%)]\tLoss: 0.457006 \tOP: 0.972973\tOR: 0.371134\tOF1: 0.537313\n",
      "Train Epoch: 3 [1984/4307 (46%)]\tLoss: 0.448283 \tOP: 0.921053\tOR: 0.368421\tOF1: 0.526316\n",
      "Train Epoch: 3 [2048/4307 (47%)]\tLoss: 0.409841 \tOP: 0.975610\tOR: 0.416667\tOF1: 0.583942\n",
      "Train Epoch: 3 [2112/4307 (49%)]\tLoss: 0.512063 \tOP: 0.911765\tOR: 0.316327\tOF1: 0.469697\n",
      "Train Epoch: 3 [2176/4307 (50%)]\tLoss: 0.440344 \tOP: 1.000000\tOR: 0.340000\tOF1: 0.507463\n",
      "Train Epoch: 3 [2240/4307 (51%)]\tLoss: 0.451451 \tOP: 0.952381\tOR: 0.363636\tOF1: 0.526316\n",
      "Train Epoch: 3 [2304/4307 (53%)]\tLoss: 0.473535 \tOP: 0.918367\tOR: 0.468750\tOF1: 0.620690\n",
      "Train Epoch: 3 [2368/4307 (54%)]\tLoss: 0.478534 \tOP: 0.843137\tOR: 0.434343\tOF1: 0.573333\n",
      "Train Epoch: 3 [2432/4307 (56%)]\tLoss: 0.432603 \tOP: 0.928571\tOR: 0.378641\tOF1: 0.537931\n",
      "Train Epoch: 3 [2496/4307 (57%)]\tLoss: 0.477175 \tOP: 0.829268\tOR: 0.346939\tOF1: 0.489209\n",
      "Train Epoch: 3 [2560/4307 (59%)]\tLoss: 0.490064 \tOP: 0.888889\tOR: 0.347826\tOF1: 0.500000\n",
      "Train Epoch: 3 [2624/4307 (60%)]\tLoss: 0.444264 \tOP: 0.936170\tOR: 0.427184\tOF1: 0.586667\n",
      "Train Epoch: 3 [2688/4307 (62%)]\tLoss: 0.485200 \tOP: 0.955556\tOR: 0.425743\tOF1: 0.589041\n",
      "Train Epoch: 3 [2752/4307 (63%)]\tLoss: 0.456299 \tOP: 0.916667\tOR: 0.330000\tOF1: 0.485294\n",
      "Train Epoch: 3 [2816/4307 (65%)]\tLoss: 0.428621 \tOP: 0.955556\tOR: 0.387387\tOF1: 0.551282\n",
      "Train Epoch: 3 [2880/4307 (66%)]\tLoss: 0.463205 \tOP: 0.939394\tOR: 0.300971\tOF1: 0.455882\n",
      "Train Epoch: 3 [2944/4307 (68%)]\tLoss: 0.452866 \tOP: 0.925000\tOR: 0.377551\tOF1: 0.536232\n",
      "Train Epoch: 3 [3008/4307 (69%)]\tLoss: 0.498681 \tOP: 0.895833\tOR: 0.417476\tOF1: 0.569536\n",
      "Train Epoch: 3 [3072/4307 (71%)]\tLoss: 0.466344 \tOP: 0.939394\tOR: 0.298077\tOF1: 0.452555\n",
      "Train Epoch: 3 [3136/4307 (72%)]\tLoss: 0.442397 \tOP: 0.897436\tOR: 0.346535\tOF1: 0.500000\n",
      "Train Epoch: 3 [3200/4307 (74%)]\tLoss: 0.431563 \tOP: 0.978723\tOR: 0.460000\tOF1: 0.625850\n",
      "Train Epoch: 3 [3264/4307 (75%)]\tLoss: 0.450645 \tOP: 0.900000\tOR: 0.367347\tOF1: 0.521739\n",
      "Train Epoch: 3 [3328/4307 (76%)]\tLoss: 0.467844 \tOP: 0.977778\tOR: 0.400000\tOF1: 0.567742\n",
      "Train Epoch: 3 [3392/4307 (78%)]\tLoss: 0.438464 \tOP: 0.953488\tOR: 0.390476\tOF1: 0.554054\n",
      "Train Epoch: 3 [3456/4307 (79%)]\tLoss: 0.516696 \tOP: 0.916667\tOR: 0.415094\tOF1: 0.571429\n",
      "Train Epoch: 3 [3520/4307 (81%)]\tLoss: 0.441626 \tOP: 0.891304\tOR: 0.455556\tOF1: 0.602941\n",
      "Train Epoch: 3 [3584/4307 (82%)]\tLoss: 0.471215 \tOP: 0.960000\tOR: 0.461538\tOF1: 0.623377\n",
      "Train Epoch: 3 [3648/4307 (84%)]\tLoss: 0.459209 \tOP: 0.906977\tOR: 0.397959\tOF1: 0.553191\n",
      "Train Epoch: 3 [3712/4307 (85%)]\tLoss: 0.461892 \tOP: 0.916667\tOR: 0.305556\tOF1: 0.458333\n",
      "Train Epoch: 3 [3776/4307 (87%)]\tLoss: 0.458775 \tOP: 0.969697\tOR: 0.320000\tOF1: 0.481203\n",
      "Train Epoch: 3 [3840/4307 (88%)]\tLoss: 0.443680 \tOP: 0.960000\tOR: 0.457143\tOF1: 0.619355\n",
      "Train Epoch: 3 [3904/4307 (90%)]\tLoss: 0.443216 \tOP: 0.976190\tOR: 0.414141\tOF1: 0.581560\n",
      "Train Epoch: 3 [3968/4307 (91%)]\tLoss: 0.483290 \tOP: 0.883721\tOR: 0.387755\tOF1: 0.539007\n",
      "Train Epoch: 3 [4032/4307 (93%)]\tLoss: 0.446304 \tOP: 0.913043\tOR: 0.392523\tOF1: 0.549020\n",
      "Train Epoch: 3 [4096/4307 (94%)]\tLoss: 0.504095 \tOP: 0.860465\tOR: 0.377551\tOF1: 0.524823\n",
      "Train Epoch: 3 [4160/4307 (96%)]\tLoss: 0.468842 \tOP: 0.818182\tOR: 0.375000\tOF1: 0.514286\n",
      "Train Epoch: 3 [4224/4307 (97%)]\tLoss: 0.455686 \tOP: 0.888889\tOR: 0.444444\tOF1: 0.592593\n",
      "Train Epoch: 3 [1273/4307 (99%)]\tLoss: 0.440008 \tOP: 0.857143\tOR: 0.413793\tOF1: 0.558140\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5071 \n",
      "OP: 0.697515\n",
      "OR: 0.454111\n",
      "OF1: 0.549491\n",
      "\n",
      "Train Epoch: 4 [0/4307 (0%)]\tLoss: 0.417064 \tOP: 0.959184\tOR: 0.456311\tOF1: 0.618421\n",
      "Train Epoch: 4 [64/4307 (1%)]\tLoss: 0.405307 \tOP: 0.930233\tOR: 0.416667\tOF1: 0.575540\n",
      "Train Epoch: 4 [128/4307 (3%)]\tLoss: 0.443819 \tOP: 0.958333\tOR: 0.450980\tOF1: 0.613333\n",
      "Train Epoch: 4 [192/4307 (4%)]\tLoss: 0.423933 \tOP: 0.937500\tOR: 0.473684\tOF1: 0.629371\n",
      "Train Epoch: 4 [256/4307 (6%)]\tLoss: 0.442678 \tOP: 0.975610\tOR: 0.434783\tOF1: 0.601504\n",
      "Train Epoch: 4 [320/4307 (7%)]\tLoss: 0.430297 \tOP: 0.951220\tOR: 0.375000\tOF1: 0.537931\n",
      "Train Epoch: 4 [384/4307 (9%)]\tLoss: 0.447316 \tOP: 0.958333\tOR: 0.484211\tOF1: 0.643357\n",
      "Train Epoch: 4 [448/4307 (10%)]\tLoss: 0.398876 \tOP: 0.962963\tOR: 0.514851\tOF1: 0.670968\n",
      "Train Epoch: 4 [512/4307 (12%)]\tLoss: 0.440855 \tOP: 0.976190\tOR: 0.386792\tOF1: 0.554054\n",
      "Train Epoch: 4 [576/4307 (13%)]\tLoss: 0.464364 \tOP: 0.948718\tOR: 0.349057\tOF1: 0.510345\n",
      "Train Epoch: 4 [640/4307 (15%)]\tLoss: 0.374817 \tOP: 0.974359\tOR: 0.391753\tOF1: 0.558824\n",
      "Train Epoch: 4 [704/4307 (16%)]\tLoss: 0.400058 \tOP: 0.957447\tOR: 0.401786\tOF1: 0.566038\n",
      "Train Epoch: 4 [768/4307 (18%)]\tLoss: 0.425285 \tOP: 0.923077\tOR: 0.387097\tOF1: 0.545455\n",
      "Train Epoch: 4 [832/4307 (19%)]\tLoss: 0.414614 \tOP: 0.976744\tOR: 0.400000\tOF1: 0.567568\n",
      "Train Epoch: 4 [896/4307 (21%)]\tLoss: 0.421630 \tOP: 0.976190\tOR: 0.431579\tOF1: 0.598540\n",
      "Train Epoch: 4 [960/4307 (22%)]\tLoss: 0.443793 \tOP: 0.931818\tOR: 0.460674\tOF1: 0.616541\n",
      "Train Epoch: 4 [1024/4307 (24%)]\tLoss: 0.443159 \tOP: 0.973684\tOR: 0.362745\tOF1: 0.528571\n",
      "Train Epoch: 4 [1088/4307 (25%)]\tLoss: 0.401970 \tOP: 0.957447\tOR: 0.489130\tOF1: 0.647482\n",
      "Train Epoch: 4 [1152/4307 (26%)]\tLoss: 0.395958 \tOP: 0.979167\tOR: 0.451923\tOF1: 0.618421\n",
      "Train Epoch: 4 [1216/4307 (28%)]\tLoss: 0.414420 \tOP: 1.000000\tOR: 0.359223\tOF1: 0.528571\n",
      "Train Epoch: 4 [1280/4307 (29%)]\tLoss: 0.442627 \tOP: 0.976744\tOR: 0.446809\tOF1: 0.613139\n",
      "Train Epoch: 4 [1344/4307 (31%)]\tLoss: 0.421909 \tOP: 0.937500\tOR: 0.450000\tOF1: 0.608108\n",
      "Train Epoch: 4 [1408/4307 (32%)]\tLoss: 0.404805 \tOP: 0.923077\tOR: 0.363636\tOF1: 0.521739\n",
      "Train Epoch: 4 [1472/4307 (34%)]\tLoss: 0.401395 \tOP: 0.962963\tOR: 0.520000\tOF1: 0.675325\n",
      "Train Epoch: 4 [1536/4307 (35%)]\tLoss: 0.404284 \tOP: 0.934783\tOR: 0.500000\tOF1: 0.651515\n",
      "Train Epoch: 4 [1600/4307 (37%)]\tLoss: 0.417399 \tOP: 0.975000\tOR: 0.367925\tOF1: 0.534247\n",
      "Train Epoch: 4 [1664/4307 (38%)]\tLoss: 0.428674 \tOP: 0.955556\tOR: 0.417476\tOF1: 0.581081\n",
      "Train Epoch: 4 [1728/4307 (40%)]\tLoss: 0.433647 \tOP: 0.885714\tOR: 0.397436\tOF1: 0.548673\n",
      "Train Epoch: 4 [1792/4307 (41%)]\tLoss: 0.440922 \tOP: 0.944444\tOR: 0.346939\tOF1: 0.507463\n",
      "Train Epoch: 4 [1856/4307 (43%)]\tLoss: 0.437758 \tOP: 0.942308\tOR: 0.471154\tOF1: 0.628205\n",
      "Train Epoch: 4 [1920/4307 (44%)]\tLoss: 0.423747 \tOP: 1.000000\tOR: 0.432990\tOF1: 0.604317\n",
      "Train Epoch: 4 [1984/4307 (46%)]\tLoss: 0.398319 \tOP: 0.978723\tOR: 0.479167\tOF1: 0.643357\n",
      "Train Epoch: 4 [2048/4307 (47%)]\tLoss: 0.458274 \tOP: 0.979167\tOR: 0.443396\tOF1: 0.610390\n",
      "Train Epoch: 4 [2112/4307 (49%)]\tLoss: 0.411433 \tOP: 0.936170\tOR: 0.427184\tOF1: 0.586667\n",
      "Train Epoch: 4 [2176/4307 (50%)]\tLoss: 0.408282 \tOP: 0.914894\tOR: 0.443299\tOF1: 0.597222\n",
      "Train Epoch: 4 [2240/4307 (51%)]\tLoss: 0.439297 \tOP: 0.909091\tOR: 0.425532\tOF1: 0.579710\n",
      "Train Epoch: 4 [2304/4307 (53%)]\tLoss: 0.430047 \tOP: 0.910714\tOR: 0.490385\tOF1: 0.637500\n",
      "Train Epoch: 4 [2368/4307 (54%)]\tLoss: 0.396325 \tOP: 0.977778\tOR: 0.435644\tOF1: 0.602740\n",
      "Train Epoch: 4 [2432/4307 (56%)]\tLoss: 0.421845 \tOP: 0.900000\tOR: 0.478723\tOF1: 0.625000\n",
      "Train Epoch: 4 [2496/4307 (57%)]\tLoss: 0.428737 \tOP: 0.937500\tOR: 0.424528\tOF1: 0.584416\n",
      "Train Epoch: 4 [2560/4307 (59%)]\tLoss: 0.422889 \tOP: 1.000000\tOR: 0.432692\tOF1: 0.604027\n",
      "Train Epoch: 4 [2624/4307 (60%)]\tLoss: 0.407123 \tOP: 0.960000\tOR: 0.500000\tOF1: 0.657534\n",
      "Train Epoch: 4 [2688/4307 (62%)]\tLoss: 0.410320 \tOP: 0.955556\tOR: 0.483146\tOF1: 0.641791\n",
      "Train Epoch: 4 [2752/4307 (63%)]\tLoss: 0.454579 \tOP: 0.959184\tOR: 0.423423\tOF1: 0.587500\n",
      "Train Epoch: 4 [2816/4307 (65%)]\tLoss: 0.389247 \tOP: 0.953488\tOR: 0.445652\tOF1: 0.607407\n",
      "Train Epoch: 4 [2880/4307 (66%)]\tLoss: 0.410783 \tOP: 0.976744\tOR: 0.407767\tOF1: 0.575342\n",
      "Train Epoch: 4 [2944/4307 (68%)]\tLoss: 0.442403 \tOP: 0.847826\tOR: 0.393939\tOF1: 0.537931\n",
      "Train Epoch: 4 [3008/4307 (69%)]\tLoss: 0.414448 \tOP: 0.957447\tOR: 0.416667\tOF1: 0.580645\n",
      "Train Epoch: 4 [3072/4307 (71%)]\tLoss: 0.418482 \tOP: 0.975610\tOR: 0.404040\tOF1: 0.571429\n",
      "Train Epoch: 4 [3136/4307 (72%)]\tLoss: 0.417540 \tOP: 0.952381\tOR: 0.421053\tOF1: 0.583942\n",
      "Train Epoch: 4 [3200/4307 (74%)]\tLoss: 0.418443 \tOP: 0.948718\tOR: 0.345794\tOF1: 0.506849\n",
      "Train Epoch: 4 [3264/4307 (75%)]\tLoss: 0.398907 \tOP: 0.957447\tOR: 0.428571\tOF1: 0.592105\n",
      "Train Epoch: 4 [3328/4307 (76%)]\tLoss: 0.447414 \tOP: 0.976190\tOR: 0.390476\tOF1: 0.557823\n",
      "Train Epoch: 4 [3392/4307 (78%)]\tLoss: 0.448455 \tOP: 0.961538\tOR: 0.505051\tOF1: 0.662252\n",
      "Train Epoch: 4 [3456/4307 (79%)]\tLoss: 0.402448 \tOP: 0.956522\tOR: 0.419048\tOF1: 0.582781\n",
      "Train Epoch: 4 [3520/4307 (81%)]\tLoss: 0.443733 \tOP: 0.888889\tOR: 0.384615\tOF1: 0.536913\n",
      "Train Epoch: 4 [3584/4307 (82%)]\tLoss: 0.438838 \tOP: 0.951220\tOR: 0.371429\tOF1: 0.534247\n",
      "Train Epoch: 4 [3648/4307 (84%)]\tLoss: 0.410051 \tOP: 0.975610\tOR: 0.434783\tOF1: 0.601504\n",
      "Train Epoch: 4 [3712/4307 (85%)]\tLoss: 0.421942 \tOP: 0.900000\tOR: 0.445545\tOF1: 0.596026\n",
      "Train Epoch: 4 [3776/4307 (87%)]\tLoss: 0.431791 \tOP: 0.911111\tOR: 0.440860\tOF1: 0.594203\n",
      "Train Epoch: 4 [3840/4307 (88%)]\tLoss: 0.456558 \tOP: 0.956522\tOR: 0.431373\tOF1: 0.594595\n",
      "Train Epoch: 4 [3904/4307 (90%)]\tLoss: 0.459122 \tOP: 0.918367\tOR: 0.445545\tOF1: 0.600000\n",
      "Train Epoch: 4 [3968/4307 (91%)]\tLoss: 0.428866 \tOP: 0.981132\tOR: 0.514851\tOF1: 0.675325\n",
      "Train Epoch: 4 [4032/4307 (93%)]\tLoss: 0.410860 \tOP: 0.982759\tOR: 0.542857\tOF1: 0.699387\n",
      "Train Epoch: 4 [4096/4307 (94%)]\tLoss: 0.382096 \tOP: 0.966667\tOR: 0.574257\tOF1: 0.720497\n",
      "Train Epoch: 4 [4160/4307 (96%)]\tLoss: 0.442127 \tOP: 0.923077\tOR: 0.452830\tOF1: 0.607595\n",
      "Train Epoch: 4 [4224/4307 (97%)]\tLoss: 0.420514 \tOP: 0.900000\tOR: 0.445545\tOF1: 0.596026\n",
      "Train Epoch: 4 [1273/4307 (99%)]\tLoss: 0.431520 \tOP: 0.923077\tOR: 0.387097\tOF1: 0.545455\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5047 \n",
      "OP: 0.689149\n",
      "OR: 0.467862\n",
      "OF1: 0.556120\n",
      "\n",
      "Train Epoch: 5 [0/4307 (0%)]\tLoss: 0.413932 \tOP: 0.914894\tOR: 0.443299\tOF1: 0.597222\n",
      "Train Epoch: 5 [64/4307 (1%)]\tLoss: 0.371577 \tOP: 0.944444\tOR: 0.520408\tOF1: 0.671053\n",
      "Train Epoch: 5 [128/4307 (3%)]\tLoss: 0.398470 \tOP: 1.000000\tOR: 0.454545\tOF1: 0.625000\n",
      "Train Epoch: 5 [192/4307 (4%)]\tLoss: 0.430535 \tOP: 0.974359\tOR: 0.387755\tOF1: 0.554745\n",
      "Train Epoch: 5 [256/4307 (6%)]\tLoss: 0.411579 \tOP: 0.948718\tOR: 0.402174\tOF1: 0.564885\n",
      "Train Epoch: 5 [320/4307 (7%)]\tLoss: 0.381912 \tOP: 0.981481\tOR: 0.509615\tOF1: 0.670886\n",
      "Train Epoch: 5 [384/4307 (9%)]\tLoss: 0.443255 \tOP: 0.952381\tOR: 0.384615\tOF1: 0.547945\n",
      "Train Epoch: 5 [448/4307 (10%)]\tLoss: 0.435313 \tOP: 0.979592\tOR: 0.461538\tOF1: 0.627451\n",
      "Train Epoch: 5 [512/4307 (12%)]\tLoss: 0.425637 \tOP: 0.883721\tOR: 0.383838\tOF1: 0.535211\n",
      "Train Epoch: 5 [576/4307 (13%)]\tLoss: 0.421167 \tOP: 0.913043\tOR: 0.456522\tOF1: 0.608696\n",
      "Train Epoch: 5 [640/4307 (15%)]\tLoss: 0.376630 \tOP: 0.978261\tOR: 0.463918\tOF1: 0.629371\n",
      "Train Epoch: 5 [704/4307 (16%)]\tLoss: 0.453024 \tOP: 1.000000\tOR: 0.386792\tOF1: 0.557823\n",
      "Train Epoch: 5 [768/4307 (18%)]\tLoss: 0.378278 \tOP: 0.980769\tOR: 0.520408\tOF1: 0.680000\n",
      "Train Epoch: 5 [832/4307 (19%)]\tLoss: 0.441587 \tOP: 1.000000\tOR: 0.450000\tOF1: 0.620690\n",
      "Train Epoch: 5 [896/4307 (21%)]\tLoss: 0.411149 \tOP: 0.976190\tOR: 0.436170\tOF1: 0.602941\n",
      "Train Epoch: 5 [960/4307 (22%)]\tLoss: 0.455830 \tOP: 1.000000\tOR: 0.345794\tOF1: 0.513889\n",
      "Train Epoch: 5 [1024/4307 (24%)]\tLoss: 0.421460 \tOP: 0.945455\tOR: 0.509804\tOF1: 0.662420\n",
      "Train Epoch: 5 [1088/4307 (25%)]\tLoss: 0.407878 \tOP: 0.975610\tOR: 0.412371\tOF1: 0.579710\n",
      "Train Epoch: 5 [1152/4307 (26%)]\tLoss: 0.384046 \tOP: 0.957447\tOR: 0.441176\tOF1: 0.604027\n",
      "Train Epoch: 5 [1216/4307 (28%)]\tLoss: 0.384188 \tOP: 1.000000\tOR: 0.530612\tOF1: 0.693333\n",
      "Train Epoch: 5 [1280/4307 (29%)]\tLoss: 0.391830 \tOP: 0.956522\tOR: 0.440000\tOF1: 0.602740\n",
      "Train Epoch: 5 [1344/4307 (31%)]\tLoss: 0.427907 \tOP: 0.975000\tOR: 0.378641\tOF1: 0.545455\n",
      "Train Epoch: 5 [1408/4307 (32%)]\tLoss: 0.389596 \tOP: 0.978723\tOR: 0.455446\tOF1: 0.621622\n",
      "Train Epoch: 5 [1472/4307 (34%)]\tLoss: 0.387862 \tOP: 1.000000\tOR: 0.504950\tOF1: 0.671053\n",
      "Train Epoch: 5 [1536/4307 (35%)]\tLoss: 0.390320 \tOP: 1.000000\tOR: 0.373737\tOF1: 0.544118\n",
      "Train Epoch: 5 [1600/4307 (37%)]\tLoss: 0.428612 \tOP: 0.903846\tOR: 0.470000\tOF1: 0.618421\n",
      "Train Epoch: 5 [1664/4307 (38%)]\tLoss: 0.430710 \tOP: 0.930233\tOR: 0.373832\tOF1: 0.533333\n",
      "Train Epoch: 5 [1728/4307 (40%)]\tLoss: 0.386463 \tOP: 0.961538\tOR: 0.515464\tOF1: 0.671141\n",
      "Train Epoch: 5 [1792/4307 (41%)]\tLoss: 0.393014 \tOP: 1.000000\tOR: 0.443396\tOF1: 0.614379\n",
      "Train Epoch: 5 [1856/4307 (43%)]\tLoss: 0.406180 \tOP: 0.975610\tOR: 0.388350\tOF1: 0.555556\n",
      "Train Epoch: 5 [1920/4307 (44%)]\tLoss: 0.411333 \tOP: 0.955556\tOR: 0.405660\tOF1: 0.569536\n",
      "Train Epoch: 5 [1984/4307 (46%)]\tLoss: 0.406860 \tOP: 0.964286\tOR: 0.580645\tOF1: 0.724832\n",
      "Train Epoch: 5 [2048/4307 (47%)]\tLoss: 0.430355 \tOP: 0.911111\tOR: 0.422680\tOF1: 0.577465\n",
      "Train Epoch: 5 [2112/4307 (49%)]\tLoss: 0.391039 \tOP: 0.976744\tOR: 0.411765\tOF1: 0.579310\n",
      "Train Epoch: 5 [2176/4307 (50%)]\tLoss: 0.412058 \tOP: 0.972973\tOR: 0.404494\tOF1: 0.571429\n",
      "Train Epoch: 5 [2240/4307 (51%)]\tLoss: 0.453398 \tOP: 0.914894\tOR: 0.417476\tOF1: 0.573333\n",
      "Train Epoch: 5 [2304/4307 (53%)]\tLoss: 0.421710 \tOP: 0.956522\tOR: 0.468085\tOF1: 0.628571\n",
      "Train Epoch: 5 [2368/4307 (54%)]\tLoss: 0.388737 \tOP: 0.961538\tOR: 0.485437\tOF1: 0.645161\n",
      "Train Epoch: 5 [2432/4307 (56%)]\tLoss: 0.409303 \tOP: 0.930233\tOR: 0.396040\tOF1: 0.555556\n",
      "Train Epoch: 5 [2496/4307 (57%)]\tLoss: 0.407889 \tOP: 0.952381\tOR: 0.430108\tOF1: 0.592593\n",
      "Train Epoch: 5 [2560/4307 (59%)]\tLoss: 0.388750 \tOP: 0.934783\tOR: 0.483146\tOF1: 0.637037\n",
      "Train Epoch: 5 [2624/4307 (60%)]\tLoss: 0.404162 \tOP: 0.978261\tOR: 0.463918\tOF1: 0.629371\n",
      "Train Epoch: 5 [2688/4307 (62%)]\tLoss: 0.431006 \tOP: 0.959184\tOR: 0.439252\tOF1: 0.602564\n",
      "Train Epoch: 5 [2752/4307 (63%)]\tLoss: 0.402761 \tOP: 0.979592\tOR: 0.484848\tOF1: 0.648649\n",
      "Train Epoch: 5 [2816/4307 (65%)]\tLoss: 0.379653 \tOP: 0.953488\tOR: 0.445652\tOF1: 0.607407\n",
      "Train Epoch: 5 [2880/4307 (66%)]\tLoss: 0.424318 \tOP: 1.000000\tOR: 0.394231\tOF1: 0.565517\n",
      "Train Epoch: 5 [2944/4307 (68%)]\tLoss: 0.396656 \tOP: 0.976190\tOR: 0.401961\tOF1: 0.569444\n",
      "Train Epoch: 5 [3008/4307 (69%)]\tLoss: 0.380282 \tOP: 0.980000\tOR: 0.462264\tOF1: 0.628205\n",
      "Train Epoch: 5 [3072/4307 (71%)]\tLoss: 0.427277 \tOP: 1.000000\tOR: 0.454545\tOF1: 0.625000\n",
      "Train Epoch: 5 [3136/4307 (72%)]\tLoss: 0.401331 \tOP: 1.000000\tOR: 0.471698\tOF1: 0.641026\n",
      "Train Epoch: 5 [3200/4307 (74%)]\tLoss: 0.436697 \tOP: 1.000000\tOR: 0.432432\tOF1: 0.603774\n",
      "Train Epoch: 5 [3264/4307 (75%)]\tLoss: 0.394829 \tOP: 0.957447\tOR: 0.473684\tOF1: 0.633803\n",
      "Train Epoch: 5 [3328/4307 (76%)]\tLoss: 0.429126 \tOP: 1.000000\tOR: 0.353535\tOF1: 0.522388\n",
      "Train Epoch: 5 [3392/4307 (78%)]\tLoss: 0.386274 \tOP: 0.960784\tOR: 0.490000\tOF1: 0.649007\n",
      "Train Epoch: 5 [3456/4307 (79%)]\tLoss: 0.387186 \tOP: 0.975000\tOR: 0.393939\tOF1: 0.561151\n",
      "Train Epoch: 5 [3520/4307 (81%)]\tLoss: 0.427179 \tOP: 0.925000\tOR: 0.373737\tOF1: 0.532374\n",
      "Train Epoch: 5 [3584/4307 (82%)]\tLoss: 0.401379 \tOP: 0.978723\tOR: 0.433962\tOF1: 0.601307\n",
      "Train Epoch: 5 [3648/4307 (84%)]\tLoss: 0.418345 \tOP: 0.954545\tOR: 0.428571\tOF1: 0.591549\n",
      "Train Epoch: 5 [3712/4307 (85%)]\tLoss: 0.382959 \tOP: 1.000000\tOR: 0.406250\tOF1: 0.577778\n",
      "Train Epoch: 5 [3776/4307 (87%)]\tLoss: 0.429499 \tOP: 0.951220\tOR: 0.382353\tOF1: 0.545455\n",
      "Train Epoch: 5 [3840/4307 (88%)]\tLoss: 0.400829 \tOP: 0.961538\tOR: 0.495050\tOF1: 0.653595\n",
      "Train Epoch: 5 [3904/4307 (90%)]\tLoss: 0.426819 \tOP: 0.938776\tOR: 0.484211\tOF1: 0.638889\n",
      "Train Epoch: 5 [3968/4307 (91%)]\tLoss: 0.412876 \tOP: 0.959184\tOR: 0.460784\tOF1: 0.622517\n",
      "Train Epoch: 5 [4032/4307 (93%)]\tLoss: 0.416625 \tOP: 0.977273\tOR: 0.447917\tOF1: 0.614286\n",
      "Train Epoch: 5 [4096/4307 (94%)]\tLoss: 0.381919 \tOP: 0.960784\tOR: 0.466667\tOF1: 0.628205\n",
      "Train Epoch: 5 [4160/4307 (96%)]\tLoss: 0.391750 \tOP: 0.960000\tOR: 0.475248\tOF1: 0.635762\n",
      "Train Epoch: 5 [4224/4307 (97%)]\tLoss: 0.336640 \tOP: 0.946429\tOR: 0.535354\tOF1: 0.683871\n",
      "Train Epoch: 5 [1273/4307 (99%)]\tLoss: 0.381804 \tOP: 0.928571\tOR: 0.481481\tOF1: 0.634146\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5211 \n",
      "OP: 0.701945\n",
      "OR: 0.456665\n",
      "OF1: 0.552338\n",
      "\n",
      "Train Epoch: 6 [0/4307 (0%)]\tLoss: 0.371433 \tOP: 0.980000\tOR: 0.485149\tOF1: 0.649007\n",
      "Train Epoch: 6 [64/4307 (1%)]\tLoss: 0.409966 \tOP: 1.000000\tOR: 0.402174\tOF1: 0.573643\n",
      "Train Epoch: 6 [128/4307 (3%)]\tLoss: 0.349115 \tOP: 0.977273\tOR: 0.467391\tOF1: 0.632353\n",
      "Train Epoch: 6 [192/4307 (4%)]\tLoss: 0.372755 \tOP: 1.000000\tOR: 0.525253\tOF1: 0.688742\n",
      "Train Epoch: 6 [256/4307 (6%)]\tLoss: 0.444272 \tOP: 0.976744\tOR: 0.428571\tOF1: 0.595745\n",
      "Train Epoch: 6 [320/4307 (7%)]\tLoss: 0.379410 \tOP: 0.976744\tOR: 0.456522\tOF1: 0.622222\n",
      "Train Epoch: 6 [384/4307 (9%)]\tLoss: 0.423691 \tOP: 1.000000\tOR: 0.418367\tOF1: 0.589928\n",
      "Train Epoch: 6 [448/4307 (10%)]\tLoss: 0.434120 \tOP: 0.981818\tOR: 0.514286\tOF1: 0.675000\n",
      "Train Epoch: 6 [512/4307 (12%)]\tLoss: 0.403481 \tOP: 1.000000\tOR: 0.463918\tOF1: 0.633803\n",
      "Train Epoch: 6 [576/4307 (13%)]\tLoss: 0.353623 \tOP: 0.983051\tOR: 0.580000\tOF1: 0.729560\n",
      "Train Epoch: 6 [640/4307 (15%)]\tLoss: 0.404986 \tOP: 0.975610\tOR: 0.353982\tOF1: 0.519481\n",
      "Train Epoch: 6 [704/4307 (16%)]\tLoss: 0.406452 \tOP: 0.954545\tOR: 0.428571\tOF1: 0.591549\n",
      "Train Epoch: 6 [768/4307 (18%)]\tLoss: 0.423979 \tOP: 0.978723\tOR: 0.474227\tOF1: 0.638889\n",
      "Train Epoch: 6 [832/4307 (19%)]\tLoss: 0.406564 \tOP: 0.979592\tOR: 0.461538\tOF1: 0.627451\n",
      "Train Epoch: 6 [896/4307 (21%)]\tLoss: 0.370952 \tOP: 0.981132\tOR: 0.477064\tOF1: 0.641975\n",
      "Train Epoch: 6 [960/4307 (22%)]\tLoss: 0.375828 \tOP: 0.979167\tOR: 0.484536\tOF1: 0.648276\n",
      "Train Epoch: 6 [1024/4307 (24%)]\tLoss: 0.358988 \tOP: 0.978261\tOR: 0.459184\tOF1: 0.625000\n",
      "Train Epoch: 6 [1088/4307 (25%)]\tLoss: 0.370297 \tOP: 1.000000\tOR: 0.410000\tOF1: 0.581560\n",
      "Train Epoch: 6 [1152/4307 (26%)]\tLoss: 0.373668 \tOP: 1.000000\tOR: 0.443182\tOF1: 0.614173\n",
      "Train Epoch: 6 [1216/4307 (28%)]\tLoss: 0.383076 \tOP: 0.981132\tOR: 0.504854\tOF1: 0.666667\n",
      "Train Epoch: 6 [1280/4307 (29%)]\tLoss: 0.371580 \tOP: 0.964912\tOR: 0.561224\tOF1: 0.709677\n",
      "Train Epoch: 6 [1344/4307 (31%)]\tLoss: 0.405862 \tOP: 0.955556\tOR: 0.472527\tOF1: 0.632353\n",
      "Train Epoch: 6 [1408/4307 (32%)]\tLoss: 0.359578 \tOP: 1.000000\tOR: 0.567308\tOF1: 0.723926\n",
      "Train Epoch: 6 [1472/4307 (34%)]\tLoss: 0.405418 \tOP: 0.977273\tOR: 0.500000\tOF1: 0.661538\n",
      "Train Epoch: 6 [1536/4307 (35%)]\tLoss: 0.381544 \tOP: 0.979592\tOR: 0.470588\tOF1: 0.635762\n",
      "Train Epoch: 6 [1600/4307 (37%)]\tLoss: 0.414240 \tOP: 0.937500\tOR: 0.463918\tOF1: 0.620690\n",
      "Train Epoch: 6 [1664/4307 (38%)]\tLoss: 0.395403 \tOP: 1.000000\tOR: 0.504950\tOF1: 0.671053\n",
      "Train Epoch: 6 [1728/4307 (40%)]\tLoss: 0.357009 \tOP: 0.978261\tOR: 0.405405\tOF1: 0.573248\n",
      "Train Epoch: 6 [1792/4307 (41%)]\tLoss: 0.380871 \tOP: 1.000000\tOR: 0.457143\tOF1: 0.627451\n",
      "Train Epoch: 6 [1856/4307 (43%)]\tLoss: 0.384882 \tOP: 1.000000\tOR: 0.469388\tOF1: 0.638889\n",
      "Train Epoch: 6 [1920/4307 (44%)]\tLoss: 0.416008 \tOP: 1.000000\tOR: 0.425926\tOF1: 0.597403\n",
      "Train Epoch: 6 [1984/4307 (46%)]\tLoss: 0.424393 \tOP: 0.981132\tOR: 0.500000\tOF1: 0.662420\n",
      "Train Epoch: 6 [2048/4307 (47%)]\tLoss: 0.405062 \tOP: 0.957447\tOR: 0.450000\tOF1: 0.612245\n",
      "Train Epoch: 6 [2112/4307 (49%)]\tLoss: 0.398105 \tOP: 0.979592\tOR: 0.466019\tOF1: 0.631579\n",
      "Train Epoch: 6 [2176/4307 (50%)]\tLoss: 0.369648 \tOP: 0.979167\tOR: 0.500000\tOF1: 0.661972\n",
      "Train Epoch: 6 [2240/4307 (51%)]\tLoss: 0.426792 \tOP: 0.974359\tOR: 0.368932\tOF1: 0.535211\n",
      "Train Epoch: 6 [2304/4307 (53%)]\tLoss: 0.398178 \tOP: 0.957447\tOR: 0.432692\tOF1: 0.596026\n",
      "Train Epoch: 6 [2368/4307 (54%)]\tLoss: 0.380525 \tOP: 0.981132\tOR: 0.448276\tOF1: 0.615385\n",
      "Train Epoch: 6 [2432/4307 (56%)]\tLoss: 0.380802 \tOP: 0.976744\tOR: 0.446809\tOF1: 0.613139\n",
      "Train Epoch: 6 [2496/4307 (57%)]\tLoss: 0.391200 \tOP: 0.956522\tOR: 0.440000\tOF1: 0.602740\n",
      "Train Epoch: 6 [2560/4307 (59%)]\tLoss: 0.399185 \tOP: 1.000000\tOR: 0.505051\tOF1: 0.671141\n",
      "Train Epoch: 6 [2624/4307 (60%)]\tLoss: 0.377948 \tOP: 1.000000\tOR: 0.481132\tOF1: 0.649682\n",
      "Train Epoch: 6 [2688/4307 (62%)]\tLoss: 0.407575 \tOP: 1.000000\tOR: 0.423077\tOF1: 0.594595\n",
      "Train Epoch: 6 [2752/4307 (63%)]\tLoss: 0.403231 \tOP: 0.955556\tOR: 0.425743\tOF1: 0.589041\n",
      "Train Epoch: 6 [2816/4307 (65%)]\tLoss: 0.416612 \tOP: 0.971429\tOR: 0.382022\tOF1: 0.548387\n",
      "Train Epoch: 6 [2880/4307 (66%)]\tLoss: 0.412221 \tOP: 0.911111\tOR: 0.445652\tOF1: 0.598540\n",
      "Train Epoch: 6 [2944/4307 (68%)]\tLoss: 0.385480 \tOP: 0.980392\tOR: 0.467290\tOF1: 0.632911\n",
      "Train Epoch: 6 [3008/4307 (69%)]\tLoss: 0.375682 \tOP: 1.000000\tOR: 0.509434\tOF1: 0.675000\n",
      "Train Epoch: 6 [3072/4307 (71%)]\tLoss: 0.375763 \tOP: 1.000000\tOR: 0.490196\tOF1: 0.657895\n",
      "Train Epoch: 6 [3136/4307 (72%)]\tLoss: 0.383721 \tOP: 1.000000\tOR: 0.514286\tOF1: 0.679245\n",
      "Train Epoch: 6 [3200/4307 (74%)]\tLoss: 0.403378 \tOP: 1.000000\tOR: 0.519608\tOF1: 0.683871\n",
      "Train Epoch: 6 [3264/4307 (75%)]\tLoss: 0.410663 \tOP: 1.000000\tOR: 0.382979\tOF1: 0.553846\n",
      "Train Epoch: 6 [3328/4307 (76%)]\tLoss: 0.421977 \tOP: 0.975610\tOR: 0.430108\tOF1: 0.597015\n",
      "Train Epoch: 6 [3392/4307 (78%)]\tLoss: 0.341988 \tOP: 0.983051\tOR: 0.563107\tOF1: 0.716049\n",
      "Train Epoch: 6 [3456/4307 (79%)]\tLoss: 0.395327 \tOP: 0.959184\tOR: 0.489583\tOF1: 0.648276\n",
      "Train Epoch: 6 [3520/4307 (81%)]\tLoss: 0.387525 \tOP: 0.976190\tOR: 0.465909\tOF1: 0.630769\n",
      "Train Epoch: 6 [3584/4307 (82%)]\tLoss: 0.393704 \tOP: 0.975610\tOR: 0.416667\tOF1: 0.583942\n",
      "Train Epoch: 6 [3648/4307 (84%)]\tLoss: 0.399908 \tOP: 0.958333\tOR: 0.494624\tOF1: 0.652482\n",
      "Train Epoch: 6 [3712/4307 (85%)]\tLoss: 0.386310 \tOP: 0.979592\tOR: 0.475248\tOF1: 0.640000\n",
      "Train Epoch: 6 [3776/4307 (87%)]\tLoss: 0.413236 \tOP: 0.981132\tOR: 0.477064\tOF1: 0.641975\n",
      "Train Epoch: 6 [3840/4307 (88%)]\tLoss: 0.415841 \tOP: 0.963636\tOR: 0.546392\tOF1: 0.697368\n",
      "Train Epoch: 6 [3904/4307 (90%)]\tLoss: 0.402516 \tOP: 0.953488\tOR: 0.383178\tOF1: 0.546667\n",
      "Train Epoch: 6 [3968/4307 (91%)]\tLoss: 0.398109 \tOP: 0.979167\tOR: 0.489583\tOF1: 0.652778\n",
      "Train Epoch: 6 [4032/4307 (93%)]\tLoss: 0.394553 \tOP: 0.980000\tOR: 0.471154\tOF1: 0.636364\n",
      "Train Epoch: 6 [4096/4307 (94%)]\tLoss: 0.382501 \tOP: 0.978261\tOR: 0.468750\tOF1: 0.633803\n",
      "Train Epoch: 6 [4160/4307 (96%)]\tLoss: 0.382556 \tOP: 0.982759\tOR: 0.548077\tOF1: 0.703704\n",
      "Train Epoch: 6 [4224/4307 (97%)]\tLoss: 0.359269 \tOP: 0.980000\tOR: 0.505155\tOF1: 0.666667\n",
      "Train Epoch: 6 [1273/4307 (99%)]\tLoss: 0.339313 \tOP: 1.000000\tOR: 0.387097\tOF1: 0.558140\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5469 \n",
      "OP: 0.632710\n",
      "OR: 0.440999\n",
      "OF1: 0.518679\n",
      "\n",
      "Train Epoch: 7 [0/4307 (0%)]\tLoss: 0.390864 \tOP: 0.958333\tOR: 0.429907\tOF1: 0.593548\n",
      "Train Epoch: 7 [64/4307 (1%)]\tLoss: 0.366091 \tOP: 1.000000\tOR: 0.435185\tOF1: 0.606452\n",
      "Train Epoch: 7 [128/4307 (3%)]\tLoss: 0.407320 \tOP: 0.930233\tOR: 0.404040\tOF1: 0.563380\n",
      "Train Epoch: 7 [192/4307 (4%)]\tLoss: 0.385200 \tOP: 0.933333\tOR: 0.461538\tOF1: 0.617647\n",
      "Train Epoch: 7 [256/4307 (6%)]\tLoss: 0.377274 \tOP: 0.981818\tOR: 0.568421\tOF1: 0.720000\n",
      "Train Epoch: 7 [320/4307 (7%)]\tLoss: 0.369742 \tOP: 1.000000\tOR: 0.581633\tOF1: 0.735484\n",
      "Train Epoch: 7 [384/4307 (9%)]\tLoss: 0.389902 \tOP: 0.980000\tOR: 0.462264\tOF1: 0.628205\n",
      "Train Epoch: 7 [448/4307 (10%)]\tLoss: 0.382385 \tOP: 1.000000\tOR: 0.486239\tOF1: 0.654321\n",
      "Train Epoch: 7 [512/4307 (12%)]\tLoss: 0.391011 \tOP: 0.980392\tOR: 0.500000\tOF1: 0.662252\n",
      "Train Epoch: 7 [576/4307 (13%)]\tLoss: 0.390620 \tOP: 1.000000\tOR: 0.505051\tOF1: 0.671141\n",
      "Train Epoch: 7 [640/4307 (15%)]\tLoss: 0.353152 \tOP: 0.981818\tOR: 0.568421\tOF1: 0.720000\n",
      "Train Epoch: 7 [704/4307 (16%)]\tLoss: 0.416022 \tOP: 0.957447\tOR: 0.428571\tOF1: 0.592105\n",
      "Train Epoch: 7 [768/4307 (18%)]\tLoss: 0.395326 \tOP: 1.000000\tOR: 0.473684\tOF1: 0.642857\n",
      "Train Epoch: 7 [832/4307 (19%)]\tLoss: 0.353377 \tOP: 0.982759\tOR: 0.570000\tOF1: 0.721519\n",
      "Train Epoch: 7 [896/4307 (21%)]\tLoss: 0.399841 \tOP: 0.975000\tOR: 0.406250\tOF1: 0.573529\n",
      "Train Epoch: 7 [960/4307 (22%)]\tLoss: 0.399218 \tOP: 0.923077\tOR: 0.484848\tOF1: 0.635762\n",
      "Train Epoch: 7 [1024/4307 (24%)]\tLoss: 0.433930 \tOP: 0.975610\tOR: 0.392157\tOF1: 0.559441\n",
      "Train Epoch: 7 [1088/4307 (25%)]\tLoss: 0.357679 \tOP: 1.000000\tOR: 0.568627\tOF1: 0.725000\n",
      "Train Epoch: 7 [1152/4307 (26%)]\tLoss: 0.355721 \tOP: 0.977273\tOR: 0.443299\tOF1: 0.609929\n",
      "Train Epoch: 7 [1216/4307 (28%)]\tLoss: 0.396794 \tOP: 0.980392\tOR: 0.510204\tOF1: 0.671141\n",
      "Train Epoch: 7 [1280/4307 (29%)]\tLoss: 0.359673 \tOP: 0.980769\tOR: 0.495146\tOF1: 0.658065\n",
      "Train Epoch: 7 [1344/4307 (31%)]\tLoss: 0.401446 \tOP: 0.952381\tOR: 0.430108\tOF1: 0.592593\n",
      "Train Epoch: 7 [1408/4307 (32%)]\tLoss: 0.375392 \tOP: 0.977778\tOR: 0.435644\tOF1: 0.602740\n",
      "Train Epoch: 7 [1472/4307 (34%)]\tLoss: 0.343918 \tOP: 1.000000\tOR: 0.485714\tOF1: 0.653846\n",
      "Train Epoch: 7 [1536/4307 (35%)]\tLoss: 0.384525 \tOP: 0.974359\tOR: 0.395833\tOF1: 0.562963\n",
      "Train Epoch: 7 [1600/4307 (37%)]\tLoss: 0.354752 \tOP: 0.983333\tOR: 0.590000\tOF1: 0.737500\n",
      "Train Epoch: 7 [1664/4307 (38%)]\tLoss: 0.406225 \tOP: 0.933333\tOR: 0.411765\tOF1: 0.571429\n",
      "Train Epoch: 7 [1728/4307 (40%)]\tLoss: 0.357757 \tOP: 1.000000\tOR: 0.519608\tOF1: 0.683871\n",
      "Train Epoch: 7 [1792/4307 (41%)]\tLoss: 0.413306 \tOP: 1.000000\tOR: 0.410000\tOF1: 0.581560\n",
      "Train Epoch: 7 [1856/4307 (43%)]\tLoss: 0.372256 \tOP: 0.976190\tOR: 0.410000\tOF1: 0.577465\n",
      "Train Epoch: 7 [1920/4307 (44%)]\tLoss: 0.417097 \tOP: 0.957447\tOR: 0.505618\tOF1: 0.661765\n",
      "Train Epoch: 7 [1984/4307 (46%)]\tLoss: 0.389380 \tOP: 0.975610\tOR: 0.421053\tOF1: 0.588235\n",
      "Train Epoch: 7 [2048/4307 (47%)]\tLoss: 0.354947 \tOP: 1.000000\tOR: 0.481132\tOF1: 0.649682\n",
      "Train Epoch: 7 [2112/4307 (49%)]\tLoss: 0.381127 \tOP: 0.962264\tOR: 0.485714\tOF1: 0.645570\n",
      "Train Epoch: 7 [2176/4307 (50%)]\tLoss: 0.350876 \tOP: 1.000000\tOR: 0.510638\tOF1: 0.676056\n",
      "Train Epoch: 7 [2240/4307 (51%)]\tLoss: 0.362961 \tOP: 1.000000\tOR: 0.544554\tOF1: 0.705128\n",
      "Train Epoch: 7 [2304/4307 (53%)]\tLoss: 0.369549 \tOP: 0.980000\tOR: 0.494949\tOF1: 0.657718\n",
      "Train Epoch: 7 [2368/4307 (54%)]\tLoss: 0.371120 \tOP: 1.000000\tOR: 0.424242\tOF1: 0.595745\n",
      "Train Epoch: 7 [2432/4307 (56%)]\tLoss: 0.414232 \tOP: 0.957447\tOR: 0.445545\tOF1: 0.608108\n",
      "Train Epoch: 7 [2496/4307 (57%)]\tLoss: 0.381723 \tOP: 0.977778\tOR: 0.435644\tOF1: 0.602740\n",
      "Train Epoch: 7 [2560/4307 (59%)]\tLoss: 0.395481 \tOP: 0.964286\tOR: 0.529412\tOF1: 0.683544\n",
      "Train Epoch: 7 [2624/4307 (60%)]\tLoss: 0.374111 \tOP: 0.980769\tOR: 0.531250\tOF1: 0.689189\n",
      "Train Epoch: 7 [2688/4307 (62%)]\tLoss: 0.386773 \tOP: 0.953488\tOR: 0.394231\tOF1: 0.557823\n",
      "Train Epoch: 7 [2752/4307 (63%)]\tLoss: 0.434858 \tOP: 0.975610\tOR: 0.377358\tOF1: 0.544218\n",
      "Train Epoch: 7 [2816/4307 (65%)]\tLoss: 0.395422 \tOP: 0.977778\tOR: 0.440000\tOF1: 0.606897\n",
      "Train Epoch: 7 [2880/4307 (66%)]\tLoss: 0.412770 \tOP: 0.959184\tOR: 0.494737\tOF1: 0.652778\n",
      "Train Epoch: 7 [2944/4307 (68%)]\tLoss: 0.385495 \tOP: 0.952381\tOR: 0.396040\tOF1: 0.559441\n",
      "Train Epoch: 7 [3008/4307 (69%)]\tLoss: 0.349531 \tOP: 0.959184\tOR: 0.489583\tOF1: 0.648276\n",
      "Train Epoch: 7 [3072/4307 (71%)]\tLoss: 0.381889 \tOP: 0.981481\tOR: 0.452991\tOF1: 0.619883\n",
      "Train Epoch: 7 [3136/4307 (72%)]\tLoss: 0.364205 \tOP: 1.000000\tOR: 0.505263\tOF1: 0.671329\n",
      "Train Epoch: 7 [3200/4307 (74%)]\tLoss: 0.388444 \tOP: 1.000000\tOR: 0.405941\tOF1: 0.577465\n",
      "Train Epoch: 7 [3264/4307 (75%)]\tLoss: 0.372625 \tOP: 0.982759\tOR: 0.593750\tOF1: 0.740260\n",
      "Train Epoch: 7 [3328/4307 (76%)]\tLoss: 0.362533 \tOP: 1.000000\tOR: 0.520408\tOF1: 0.684564\n",
      "Train Epoch: 7 [3392/4307 (78%)]\tLoss: 0.386144 \tOP: 0.958333\tOR: 0.455446\tOF1: 0.617450\n",
      "Train Epoch: 7 [3456/4307 (79%)]\tLoss: 0.354521 \tOP: 1.000000\tOR: 0.446602\tOF1: 0.617450\n",
      "Train Epoch: 7 [3520/4307 (81%)]\tLoss: 0.376396 \tOP: 0.944444\tOR: 0.573034\tOF1: 0.713287\n",
      "Train Epoch: 7 [3584/4307 (82%)]\tLoss: 0.370266 \tOP: 0.976190\tOR: 0.440860\tOF1: 0.607407\n",
      "Train Epoch: 7 [3648/4307 (84%)]\tLoss: 0.407471 \tOP: 1.000000\tOR: 0.401961\tOF1: 0.573427\n",
      "Train Epoch: 7 [3712/4307 (85%)]\tLoss: 0.392009 \tOP: 1.000000\tOR: 0.480392\tOF1: 0.649007\n",
      "Train Epoch: 7 [3776/4307 (87%)]\tLoss: 0.414500 \tOP: 1.000000\tOR: 0.377551\tOF1: 0.548148\n",
      "Train Epoch: 7 [3840/4307 (88%)]\tLoss: 0.375290 \tOP: 0.976744\tOR: 0.442105\tOF1: 0.608696\n",
      "Train Epoch: 7 [3904/4307 (90%)]\tLoss: 0.382801 \tOP: 1.000000\tOR: 0.410000\tOF1: 0.581560\n",
      "Train Epoch: 7 [3968/4307 (91%)]\tLoss: 0.380593 \tOP: 0.961538\tOR: 0.505051\tOF1: 0.662252\n",
      "Train Epoch: 7 [4032/4307 (93%)]\tLoss: 0.378738 \tOP: 1.000000\tOR: 0.509615\tOF1: 0.675159\n",
      "Train Epoch: 7 [4096/4307 (94%)]\tLoss: 0.388267 \tOP: 0.982143\tOR: 0.533981\tOF1: 0.691824\n",
      "Train Epoch: 7 [4160/4307 (96%)]\tLoss: 0.366940 \tOP: 1.000000\tOR: 0.454545\tOF1: 0.625000\n",
      "Train Epoch: 7 [4224/4307 (97%)]\tLoss: 0.393320 \tOP: 0.955556\tOR: 0.413462\tOF1: 0.577181\n",
      "Train Epoch: 7 [1273/4307 (99%)]\tLoss: 0.358047 \tOP: 0.944444\tOR: 0.653846\tOF1: 0.772727\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5426 \n",
      "OP: 0.669159\n",
      "OR: 0.458163\n",
      "OF1: 0.542784\n",
      "\n",
      "Train Epoch: 8 [0/4307 (0%)]\tLoss: 0.368254 \tOP: 1.000000\tOR: 0.563830\tOF1: 0.721088\n",
      "Train Epoch: 8 [64/4307 (1%)]\tLoss: 0.351625 \tOP: 1.000000\tOR: 0.510000\tOF1: 0.675497\n",
      "Train Epoch: 8 [128/4307 (3%)]\tLoss: 0.383313 \tOP: 1.000000\tOR: 0.430000\tOF1: 0.601399\n",
      "Train Epoch: 8 [192/4307 (4%)]\tLoss: 0.361095 \tOP: 1.000000\tOR: 0.473118\tOF1: 0.642336\n",
      "Train Epoch: 8 [256/4307 (6%)]\tLoss: 0.343173 \tOP: 0.978261\tOR: 0.473684\tOF1: 0.638298\n",
      "Train Epoch: 8 [320/4307 (7%)]\tLoss: 0.377188 \tOP: 0.980392\tOR: 0.490196\tOF1: 0.653595\n",
      "Train Epoch: 8 [384/4307 (9%)]\tLoss: 0.366279 \tOP: 1.000000\tOR: 0.440367\tOF1: 0.611465\n",
      "Train Epoch: 8 [448/4307 (10%)]\tLoss: 0.367454 \tOP: 0.981818\tOR: 0.504673\tOF1: 0.666667\n",
      "Train Epoch: 8 [512/4307 (12%)]\tLoss: 0.348740 \tOP: 1.000000\tOR: 0.509804\tOF1: 0.675325\n",
      "Train Epoch: 8 [576/4307 (13%)]\tLoss: 0.421286 \tOP: 1.000000\tOR: 0.435644\tOF1: 0.606897\n",
      "Train Epoch: 8 [640/4307 (15%)]\tLoss: 0.399059 \tOP: 0.945946\tOR: 0.357143\tOF1: 0.518519\n",
      "Train Epoch: 8 [704/4307 (16%)]\tLoss: 0.378524 \tOP: 0.954545\tOR: 0.403846\tOF1: 0.567568\n",
      "Train Epoch: 8 [768/4307 (18%)]\tLoss: 0.372288 \tOP: 0.959184\tOR: 0.470000\tOF1: 0.630872\n",
      "Train Epoch: 8 [832/4307 (19%)]\tLoss: 0.369019 \tOP: 0.975000\tOR: 0.419355\tOF1: 0.586466\n",
      "Train Epoch: 8 [896/4307 (21%)]\tLoss: 0.365949 \tOP: 0.980769\tOR: 0.472222\tOF1: 0.637500\n",
      "Train Epoch: 8 [960/4307 (22%)]\tLoss: 0.363331 \tOP: 1.000000\tOR: 0.490000\tOF1: 0.657718\n",
      "Train Epoch: 8 [1024/4307 (24%)]\tLoss: 0.395207 \tOP: 0.980769\tOR: 0.510000\tOF1: 0.671053\n",
      "Train Epoch: 8 [1088/4307 (25%)]\tLoss: 0.361833 \tOP: 0.982759\tOR: 0.532710\tOF1: 0.690909\n",
      "Train Epoch: 8 [1152/4307 (26%)]\tLoss: 0.357789 \tOP: 0.982456\tOR: 0.565657\tOF1: 0.717949\n",
      "Train Epoch: 8 [1216/4307 (28%)]\tLoss: 0.420468 \tOP: 1.000000\tOR: 0.450000\tOF1: 0.620690\n",
      "Train Epoch: 8 [1280/4307 (29%)]\tLoss: 0.399808 \tOP: 0.978261\tOR: 0.428571\tOF1: 0.596026\n",
      "Train Epoch: 8 [1344/4307 (31%)]\tLoss: 0.388015 \tOP: 1.000000\tOR: 0.524272\tOF1: 0.687898\n",
      "Train Epoch: 8 [1408/4307 (32%)]\tLoss: 0.367104 \tOP: 1.000000\tOR: 0.514019\tOF1: 0.679012\n",
      "Train Epoch: 8 [1472/4307 (34%)]\tLoss: 0.375007 \tOP: 1.000000\tOR: 0.490385\tOF1: 0.658065\n",
      "Train Epoch: 8 [1536/4307 (35%)]\tLoss: 0.350081 \tOP: 1.000000\tOR: 0.457143\tOF1: 0.627451\n",
      "Train Epoch: 8 [1600/4307 (37%)]\tLoss: 0.422647 \tOP: 1.000000\tOR: 0.343434\tOF1: 0.511278\n",
      "Train Epoch: 8 [1664/4307 (38%)]\tLoss: 0.382533 \tOP: 1.000000\tOR: 0.431579\tOF1: 0.602941\n",
      "Train Epoch: 8 [1728/4307 (40%)]\tLoss: 0.383395 \tOP: 1.000000\tOR: 0.461538\tOF1: 0.631579\n",
      "Train Epoch: 8 [1792/4307 (41%)]\tLoss: 0.373104 \tOP: 1.000000\tOR: 0.470588\tOF1: 0.640000\n",
      "Train Epoch: 8 [1856/4307 (43%)]\tLoss: 0.349934 \tOP: 1.000000\tOR: 0.466019\tOF1: 0.635762\n",
      "Train Epoch: 8 [1920/4307 (44%)]\tLoss: 0.377748 \tOP: 0.959184\tOR: 0.451923\tOF1: 0.614379\n",
      "Train Epoch: 8 [1984/4307 (46%)]\tLoss: 0.379273 \tOP: 0.957447\tOR: 0.483871\tOF1: 0.642857\n",
      "Train Epoch: 8 [2048/4307 (47%)]\tLoss: 0.371932 \tOP: 1.000000\tOR: 0.545455\tOF1: 0.705882\n",
      "Train Epoch: 8 [2112/4307 (49%)]\tLoss: 0.376148 \tOP: 0.942308\tOR: 0.515789\tOF1: 0.666667\n",
      "Train Epoch: 8 [2176/4307 (50%)]\tLoss: 0.336838 \tOP: 1.000000\tOR: 0.630435\tOF1: 0.773333\n",
      "Train Epoch: 8 [2240/4307 (51%)]\tLoss: 0.375843 \tOP: 1.000000\tOR: 0.500000\tOF1: 0.666667\n",
      "Train Epoch: 8 [2304/4307 (53%)]\tLoss: 0.430045 \tOP: 1.000000\tOR: 0.407767\tOF1: 0.579310\n",
      "Train Epoch: 8 [2368/4307 (54%)]\tLoss: 0.406158 \tOP: 0.953488\tOR: 0.445652\tOF1: 0.607407\n",
      "Train Epoch: 8 [2432/4307 (56%)]\tLoss: 0.399497 \tOP: 0.937500\tOR: 0.445545\tOF1: 0.604027\n",
      "Train Epoch: 8 [2496/4307 (57%)]\tLoss: 0.352469 \tOP: 0.982456\tOR: 0.560000\tOF1: 0.713376\n",
      "Train Epoch: 8 [2560/4307 (59%)]\tLoss: 0.383061 \tOP: 0.979592\tOR: 0.500000\tOF1: 0.662069\n",
      "Train Epoch: 8 [2624/4307 (60%)]\tLoss: 0.399255 \tOP: 0.977273\tOR: 0.457447\tOF1: 0.623188\n",
      "Train Epoch: 8 [2688/4307 (62%)]\tLoss: 0.374448 \tOP: 1.000000\tOR: 0.500000\tOF1: 0.666667\n",
      "Train Epoch: 8 [2752/4307 (63%)]\tLoss: 0.387103 \tOP: 1.000000\tOR: 0.426966\tOF1: 0.598425\n",
      "Train Epoch: 8 [2816/4307 (65%)]\tLoss: 0.352609 \tOP: 1.000000\tOR: 0.489796\tOF1: 0.657534\n",
      "Train Epoch: 8 [2880/4307 (66%)]\tLoss: 0.368551 \tOP: 0.979167\tOR: 0.465347\tOF1: 0.630872\n",
      "Train Epoch: 8 [2944/4307 (68%)]\tLoss: 0.406803 \tOP: 0.961538\tOR: 0.471698\tOF1: 0.632911\n",
      "Train Epoch: 8 [3008/4307 (69%)]\tLoss: 0.334660 \tOP: 0.980769\tOR: 0.536842\tOF1: 0.693878\n",
      "Train Epoch: 8 [3072/4307 (71%)]\tLoss: 0.391553 \tOP: 1.000000\tOR: 0.466019\tOF1: 0.635762\n",
      "Train Epoch: 8 [3136/4307 (72%)]\tLoss: 0.398139 \tOP: 1.000000\tOR: 0.457944\tOF1: 0.628205\n",
      "Train Epoch: 8 [3200/4307 (74%)]\tLoss: 0.335711 \tOP: 0.979592\tOR: 0.466019\tOF1: 0.631579\n",
      "Train Epoch: 8 [3264/4307 (75%)]\tLoss: 0.360611 \tOP: 0.982143\tOR: 0.561224\tOF1: 0.714286\n",
      "Train Epoch: 8 [3328/4307 (76%)]\tLoss: 0.357655 \tOP: 1.000000\tOR: 0.539216\tOF1: 0.700637\n",
      "Train Epoch: 8 [3392/4307 (78%)]\tLoss: 0.379683 \tOP: 0.972973\tOR: 0.400000\tOF1: 0.566929\n",
      "Train Epoch: 8 [3456/4307 (79%)]\tLoss: 0.378718 \tOP: 0.980392\tOR: 0.476190\tOF1: 0.641026\n",
      "Train Epoch: 8 [3520/4307 (81%)]\tLoss: 0.384085 \tOP: 1.000000\tOR: 0.415094\tOF1: 0.586667\n",
      "Train Epoch: 8 [3584/4307 (82%)]\tLoss: 0.390664 \tOP: 0.979592\tOR: 0.516129\tOF1: 0.676056\n",
      "Train Epoch: 8 [3648/4307 (84%)]\tLoss: 0.382058 \tOP: 0.976190\tOR: 0.431579\tOF1: 0.598540\n",
      "Train Epoch: 8 [3712/4307 (85%)]\tLoss: 0.397187 \tOP: 0.980000\tOR: 0.521277\tOF1: 0.680556\n",
      "Train Epoch: 8 [3776/4307 (87%)]\tLoss: 0.354719 \tOP: 0.938776\tOR: 0.494624\tOF1: 0.647887\n",
      "Train Epoch: 8 [3840/4307 (88%)]\tLoss: 0.366535 \tOP: 1.000000\tOR: 0.490566\tOF1: 0.658228\n",
      "Train Epoch: 8 [3904/4307 (90%)]\tLoss: 0.364668 \tOP: 1.000000\tOR: 0.505263\tOF1: 0.671329\n",
      "Train Epoch: 8 [3968/4307 (91%)]\tLoss: 0.405701 \tOP: 1.000000\tOR: 0.466667\tOF1: 0.636364\n",
      "Train Epoch: 8 [4032/4307 (93%)]\tLoss: 0.385479 \tOP: 0.981132\tOR: 0.514851\tOF1: 0.675325\n",
      "Train Epoch: 8 [4096/4307 (94%)]\tLoss: 0.399791 \tOP: 1.000000\tOR: 0.448598\tOF1: 0.619355\n",
      "Train Epoch: 8 [4160/4307 (96%)]\tLoss: 0.402629 \tOP: 0.976190\tOR: 0.405941\tOF1: 0.573427\n",
      "Train Epoch: 8 [4224/4307 (97%)]\tLoss: 0.394637 \tOP: 0.978261\tOR: 0.459184\tOF1: 0.625000\n",
      "Train Epoch: 8 [1273/4307 (99%)]\tLoss: 0.385652 \tOP: 0.923077\tOR: 0.521739\tOF1: 0.666667\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5446 \n",
      "OP: 0.674973\n",
      "OR: 0.481784\n",
      "OF1: 0.561597\n",
      "\n",
      "Train Epoch: 9 [0/4307 (0%)]\tLoss: 0.398050 \tOP: 0.952381\tOR: 0.444444\tOF1: 0.606061\n",
      "Train Epoch: 9 [64/4307 (1%)]\tLoss: 0.371715 \tOP: 0.977778\tOR: 0.427184\tOF1: 0.594595\n",
      "Train Epoch: 9 [128/4307 (3%)]\tLoss: 0.370621 \tOP: 1.000000\tOR: 0.452632\tOF1: 0.623188\n",
      "Train Epoch: 9 [192/4307 (4%)]\tLoss: 0.355305 \tOP: 0.957447\tOR: 0.468750\tOF1: 0.629371\n",
      "Train Epoch: 9 [256/4307 (6%)]\tLoss: 0.379638 \tOP: 0.954545\tOR: 0.415842\tOF1: 0.579310\n",
      "Train Epoch: 9 [320/4307 (7%)]\tLoss: 0.396792 \tOP: 0.975000\tOR: 0.364486\tOF1: 0.530612\n",
      "Train Epoch: 9 [384/4307 (9%)]\tLoss: 0.366648 \tOP: 0.960784\tOR: 0.485149\tOF1: 0.644737\n",
      "Train Epoch: 9 [448/4307 (10%)]\tLoss: 0.364955 \tOP: 1.000000\tOR: 0.448598\tOF1: 0.619355\n",
      "Train Epoch: 9 [512/4307 (12%)]\tLoss: 0.356296 \tOP: 1.000000\tOR: 0.532710\tOF1: 0.695122\n",
      "Train Epoch: 9 [576/4307 (13%)]\tLoss: 0.367891 \tOP: 0.980000\tOR: 0.500000\tOF1: 0.662162\n",
      "Train Epoch: 9 [640/4307 (15%)]\tLoss: 0.386156 \tOP: 0.977778\tOR: 0.440000\tOF1: 0.606897\n",
      "Train Epoch: 9 [704/4307 (16%)]\tLoss: 0.369035 \tOP: 1.000000\tOR: 0.440367\tOF1: 0.611465\n",
      "Train Epoch: 9 [768/4307 (18%)]\tLoss: 0.352983 \tOP: 1.000000\tOR: 0.483871\tOF1: 0.652174\n",
      "Train Epoch: 9 [832/4307 (19%)]\tLoss: 0.397298 \tOP: 0.944444\tOR: 0.373626\tOF1: 0.535433\n",
      "Train Epoch: 9 [896/4307 (21%)]\tLoss: 0.377455 \tOP: 1.000000\tOR: 0.463918\tOF1: 0.633803\n",
      "Train Epoch: 9 [960/4307 (22%)]\tLoss: 0.384934 \tOP: 0.978723\tOR: 0.442308\tOF1: 0.609272\n",
      "Train Epoch: 9 [1024/4307 (24%)]\tLoss: 0.364980 \tOP: 0.978261\tOR: 0.483871\tOF1: 0.647482\n",
      "Train Epoch: 9 [1088/4307 (25%)]\tLoss: 0.371510 \tOP: 0.982456\tOR: 0.543689\tOF1: 0.700000\n",
      "Train Epoch: 9 [1152/4307 (26%)]\tLoss: 0.390241 \tOP: 1.000000\tOR: 0.490741\tOF1: 0.658385\n",
      "Train Epoch: 9 [1216/4307 (28%)]\tLoss: 0.378259 \tOP: 0.951220\tOR: 0.428571\tOF1: 0.590909\n",
      "Train Epoch: 9 [1280/4307 (29%)]\tLoss: 0.360250 \tOP: 0.980769\tOR: 0.510000\tOF1: 0.671053\n",
      "Train Epoch: 9 [1344/4307 (31%)]\tLoss: 0.353029 \tOP: 1.000000\tOR: 0.494949\tOF1: 0.662162\n",
      "Train Epoch: 9 [1408/4307 (32%)]\tLoss: 0.385674 \tOP: 0.978261\tOR: 0.424528\tOF1: 0.592105\n",
      "Train Epoch: 9 [1472/4307 (34%)]\tLoss: 0.388179 \tOP: 1.000000\tOR: 0.431193\tOF1: 0.602564\n",
      "Train Epoch: 9 [1536/4307 (35%)]\tLoss: 0.364836 \tOP: 1.000000\tOR: 0.520000\tOF1: 0.684211\n",
      "Train Epoch: 9 [1600/4307 (37%)]\tLoss: 0.375781 \tOP: 0.980769\tOR: 0.510000\tOF1: 0.671053\n",
      "Train Epoch: 9 [1664/4307 (38%)]\tLoss: 0.406261 \tOP: 0.980392\tOR: 0.480769\tOF1: 0.645161\n",
      "Train Epoch: 9 [1728/4307 (40%)]\tLoss: 0.374733 \tOP: 1.000000\tOR: 0.458333\tOF1: 0.628571\n",
      "Train Epoch: 9 [1792/4307 (41%)]\tLoss: 0.374887 \tOP: 1.000000\tOR: 0.513761\tOF1: 0.678788\n",
      "Train Epoch: 9 [1856/4307 (43%)]\tLoss: 0.359646 \tOP: 1.000000\tOR: 0.448980\tOF1: 0.619718\n",
      "Train Epoch: 9 [1920/4307 (44%)]\tLoss: 0.346112 \tOP: 1.000000\tOR: 0.509615\tOF1: 0.675159\n",
      "Train Epoch: 9 [1984/4307 (46%)]\tLoss: 0.352671 \tOP: 1.000000\tOR: 0.466667\tOF1: 0.636364\n",
      "Train Epoch: 9 [2048/4307 (47%)]\tLoss: 0.342689 \tOP: 1.000000\tOR: 0.598039\tOF1: 0.748466\n",
      "Train Epoch: 9 [2112/4307 (49%)]\tLoss: 0.372433 \tOP: 1.000000\tOR: 0.460784\tOF1: 0.630872\n",
      "Train Epoch: 9 [2176/4307 (50%)]\tLoss: 0.368787 \tOP: 0.979167\tOR: 0.460784\tOF1: 0.626667\n",
      "Train Epoch: 9 [2240/4307 (51%)]\tLoss: 0.390287 \tOP: 0.978261\tOR: 0.459184\tOF1: 0.625000\n",
      "Train Epoch: 9 [2304/4307 (53%)]\tLoss: 0.339083 \tOP: 0.981132\tOR: 0.541667\tOF1: 0.697987\n",
      "Train Epoch: 9 [2368/4307 (54%)]\tLoss: 0.358532 \tOP: 1.000000\tOR: 0.494737\tOF1: 0.661972\n",
      "Train Epoch: 9 [2432/4307 (56%)]\tLoss: 0.357609 \tOP: 1.000000\tOR: 0.505376\tOF1: 0.671429\n",
      "Train Epoch: 9 [2496/4307 (57%)]\tLoss: 0.383569 \tOP: 0.956522\tOR: 0.458333\tOF1: 0.619718\n",
      "Train Epoch: 9 [2560/4307 (59%)]\tLoss: 0.339818 \tOP: 1.000000\tOR: 0.524272\tOF1: 0.687898\n",
      "Train Epoch: 9 [2624/4307 (60%)]\tLoss: 0.411307 \tOP: 0.916667\tOR: 0.468085\tOF1: 0.619718\n",
      "Train Epoch: 9 [2688/4307 (62%)]\tLoss: 0.390374 \tOP: 1.000000\tOR: 0.489796\tOF1: 0.657534\n",
      "Train Epoch: 9 [2752/4307 (63%)]\tLoss: 0.354070 \tOP: 0.978723\tOR: 0.500000\tOF1: 0.661871\n",
      "Train Epoch: 9 [2816/4307 (65%)]\tLoss: 0.379091 \tOP: 1.000000\tOR: 0.460000\tOF1: 0.630137\n",
      "Train Epoch: 9 [2880/4307 (66%)]\tLoss: 0.386904 \tOP: 0.977273\tOR: 0.413462\tOF1: 0.581081\n",
      "Train Epoch: 9 [2944/4307 (68%)]\tLoss: 0.379106 \tOP: 0.979167\tOR: 0.456311\tOF1: 0.622517\n",
      "Train Epoch: 9 [3008/4307 (69%)]\tLoss: 0.362945 \tOP: 0.982759\tOR: 0.600000\tOF1: 0.745098\n",
      "Train Epoch: 9 [3072/4307 (71%)]\tLoss: 0.348159 \tOP: 1.000000\tOR: 0.484211\tOF1: 0.652482\n",
      "Train Epoch: 9 [3136/4307 (72%)]\tLoss: 0.334349 \tOP: 1.000000\tOR: 0.563107\tOF1: 0.720497\n",
      "Train Epoch: 9 [3200/4307 (74%)]\tLoss: 0.371284 \tOP: 1.000000\tOR: 0.392157\tOF1: 0.563380\n",
      "Train Epoch: 9 [3264/4307 (75%)]\tLoss: 0.352780 \tOP: 1.000000\tOR: 0.514019\tOF1: 0.679012\n",
      "Train Epoch: 9 [3328/4307 (76%)]\tLoss: 0.393444 \tOP: 1.000000\tOR: 0.422680\tOF1: 0.594203\n",
      "Train Epoch: 9 [3392/4307 (78%)]\tLoss: 0.396055 \tOP: 0.979167\tOR: 0.484536\tOF1: 0.648276\n",
      "Train Epoch: 9 [3456/4307 (79%)]\tLoss: 0.361170 \tOP: 1.000000\tOR: 0.451923\tOF1: 0.622517\n",
      "Train Epoch: 9 [3520/4307 (81%)]\tLoss: 0.386078 \tOP: 1.000000\tOR: 0.475728\tOF1: 0.644737\n",
      "Train Epoch: 9 [3584/4307 (82%)]\tLoss: 0.366412 \tOP: 0.960784\tOR: 0.471154\tOF1: 0.632258\n",
      "Train Epoch: 9 [3648/4307 (84%)]\tLoss: 0.359052 \tOP: 0.978261\tOR: 0.473684\tOF1: 0.638298\n",
      "Train Epoch: 9 [3712/4307 (85%)]\tLoss: 0.381976 \tOP: 0.953488\tOR: 0.450549\tOF1: 0.611940\n",
      "Train Epoch: 9 [3776/4307 (87%)]\tLoss: 0.357152 \tOP: 0.980392\tOR: 0.495050\tOF1: 0.657895\n",
      "Train Epoch: 9 [3840/4307 (88%)]\tLoss: 0.341085 \tOP: 1.000000\tOR: 0.425743\tOF1: 0.597222\n",
      "Train Epoch: 9 [3904/4307 (90%)]\tLoss: 0.351260 \tOP: 0.980769\tOR: 0.463636\tOF1: 0.629630\n",
      "Train Epoch: 9 [3968/4307 (91%)]\tLoss: 0.390144 \tOP: 0.959184\tOR: 0.500000\tOF1: 0.657343\n",
      "Train Epoch: 9 [4032/4307 (93%)]\tLoss: 0.362268 \tOP: 0.981132\tOR: 0.525253\tOF1: 0.684211\n",
      "Train Epoch: 9 [4096/4307 (94%)]\tLoss: 0.346113 \tOP: 0.981132\tOR: 0.584270\tOF1: 0.732394\n",
      "Train Epoch: 9 [4160/4307 (96%)]\tLoss: 0.353577 \tOP: 1.000000\tOR: 0.565657\tOF1: 0.722581\n",
      "Train Epoch: 9 [4224/4307 (97%)]\tLoss: 0.385623 \tOP: 1.000000\tOR: 0.460784\tOF1: 0.630872\n",
      "Train Epoch: 9 [1273/4307 (99%)]\tLoss: 0.359289 \tOP: 0.928571\tOR: 0.464286\tOF1: 0.619048\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5760 \n",
      "OP: 0.652187\n",
      "OR: 0.479571\n",
      "OF1: 0.551719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        loss_lst_train.append(loss.data.item())\n",
    "        OF1_lst_train.append(OF1)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    OP_final = 0\n",
    "    OR_final = 0\n",
    "    OF1_final = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        OP_final += OP\n",
    "        OR_final += OR\n",
    "        OF1_final += OF1\n",
    "        \n",
    "    loss_lst_test.append(test_loss.data.item()/i)\n",
    "    OF1_lst_test.append(OF1_final/i)\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP_final/i, OR_final/i, OF1_final/i))\n",
    "\n",
    "loss_lst_train = []\n",
    "OF1_lst_train = []\n",
    "\n",
    "loss_lst_test = []\n",
    "OF1_lst_test = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "model2 = torchvision.models.densenet201(pretrained=True)\n",
    "num_ftrs = model2.classifier.in_features\n",
    "model2.classifier = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4307 (0%)]\tLoss: 0.685201 \tOP: 0.333333\tOR: 0.028846\tOF1: 0.053097\n",
      "Train Epoch: 0 [64/4307 (1%)]\tLoss: 0.666958 \tOP: 0.416667\tOR: 0.050000\tOF1: 0.089286\n",
      "Train Epoch: 0 [128/4307 (3%)]\tLoss: 0.643762 \tOP: 0.444444\tOR: 0.038095\tOF1: 0.070175\n",
      "Train Epoch: 0 [192/4307 (4%)]\tLoss: 0.613103 \tOP: 0.666667\tOR: 0.081633\tOF1: 0.145455\n",
      "Train Epoch: 0 [256/4307 (6%)]\tLoss: 0.645037 \tOP: 0.500000\tOR: 0.086538\tOF1: 0.147541\n",
      "Train Epoch: 0 [320/4307 (7%)]\tLoss: 0.617840 \tOP: 0.500000\tOR: 0.095238\tOF1: 0.160000\n",
      "Train Epoch: 0 [384/4307 (9%)]\tLoss: 0.604458 \tOP: 0.571429\tOR: 0.121212\tOF1: 0.200000\n",
      "Train Epoch: 0 [448/4307 (10%)]\tLoss: 0.622469 \tOP: 0.555556\tOR: 0.142857\tOF1: 0.227273\n",
      "Train Epoch: 0 [512/4307 (12%)]\tLoss: 0.571185 \tOP: 0.666667\tOR: 0.156863\tOF1: 0.253968\n",
      "Train Epoch: 0 [576/4307 (13%)]\tLoss: 0.586665 \tOP: 0.700000\tOR: 0.205882\tOF1: 0.318182\n",
      "Train Epoch: 0 [640/4307 (15%)]\tLoss: 0.580106 \tOP: 0.656250\tOR: 0.203883\tOF1: 0.311111\n",
      "Train Epoch: 0 [704/4307 (16%)]\tLoss: 0.563004 \tOP: 0.625000\tOR: 0.202020\tOF1: 0.305344\n",
      "Train Epoch: 0 [768/4307 (18%)]\tLoss: 0.563117 \tOP: 0.735294\tOR: 0.242718\tOF1: 0.364964\n",
      "Train Epoch: 0 [832/4307 (19%)]\tLoss: 0.521921 \tOP: 0.729730\tOR: 0.287234\tOF1: 0.412214\n",
      "Train Epoch: 0 [896/4307 (21%)]\tLoss: 0.517405 \tOP: 0.767442\tOR: 0.343750\tOF1: 0.474820\n",
      "Train Epoch: 0 [960/4307 (22%)]\tLoss: 0.570969 \tOP: 0.627907\tOR: 0.245455\tOF1: 0.352941\n",
      "Train Epoch: 0 [1024/4307 (24%)]\tLoss: 0.535171 \tOP: 0.589744\tOR: 0.239583\tOF1: 0.340741\n",
      "Train Epoch: 0 [1088/4307 (25%)]\tLoss: 0.516398 \tOP: 0.714286\tOR: 0.360825\tOF1: 0.479452\n",
      "Train Epoch: 0 [1152/4307 (26%)]\tLoss: 0.545650 \tOP: 0.574468\tOR: 0.284211\tOF1: 0.380282\n",
      "Train Epoch: 0 [1216/4307 (28%)]\tLoss: 0.465848 \tOP: 0.750000\tOR: 0.375000\tOF1: 0.500000\n",
      "Train Epoch: 0 [1280/4307 (29%)]\tLoss: 0.556189 \tOP: 0.638298\tOR: 0.288462\tOF1: 0.397351\n",
      "Train Epoch: 0 [1344/4307 (31%)]\tLoss: 0.503775 \tOP: 0.760870\tOR: 0.346535\tOF1: 0.476190\n",
      "Train Epoch: 0 [1408/4307 (32%)]\tLoss: 0.471505 \tOP: 0.755102\tOR: 0.389474\tOF1: 0.513889\n",
      "Train Epoch: 0 [1472/4307 (34%)]\tLoss: 0.523663 \tOP: 0.792453\tOR: 0.388889\tOF1: 0.521739\n",
      "Train Epoch: 0 [1536/4307 (35%)]\tLoss: 0.520891 \tOP: 0.600000\tOR: 0.252632\tOF1: 0.355556\n",
      "Train Epoch: 0 [1600/4307 (37%)]\tLoss: 0.486823 \tOP: 0.685185\tOR: 0.377551\tOF1: 0.486842\n",
      "Train Epoch: 0 [1664/4307 (38%)]\tLoss: 0.502431 \tOP: 0.769231\tOR: 0.392157\tOF1: 0.519481\n",
      "Train Epoch: 0 [1728/4307 (40%)]\tLoss: 0.511264 \tOP: 0.604651\tOR: 0.257426\tOF1: 0.361111\n",
      "Train Epoch: 0 [1792/4307 (41%)]\tLoss: 0.464401 \tOP: 0.788462\tOR: 0.440860\tOF1: 0.565517\n",
      "Train Epoch: 0 [1856/4307 (43%)]\tLoss: 0.492299 \tOP: 0.720000\tOR: 0.356436\tOF1: 0.476821\n",
      "Train Epoch: 0 [1920/4307 (44%)]\tLoss: 0.475741 \tOP: 0.763636\tOR: 0.420000\tOF1: 0.541935\n",
      "Train Epoch: 0 [1984/4307 (46%)]\tLoss: 0.473353 \tOP: 0.780000\tOR: 0.378641\tOF1: 0.509804\n",
      "Train Epoch: 0 [2048/4307 (47%)]\tLoss: 0.485498 \tOP: 0.759259\tOR: 0.422680\tOF1: 0.543046\n",
      "Train Epoch: 0 [2112/4307 (49%)]\tLoss: 0.506415 \tOP: 0.711538\tOR: 0.377551\tOF1: 0.493333\n",
      "Train Epoch: 0 [2176/4307 (50%)]\tLoss: 0.526062 \tOP: 0.719298\tOR: 0.390476\tOF1: 0.506173\n",
      "Train Epoch: 0 [2240/4307 (51%)]\tLoss: 0.473856 \tOP: 0.763636\tOR: 0.420000\tOF1: 0.541935\n",
      "Train Epoch: 0 [2304/4307 (53%)]\tLoss: 0.482922 \tOP: 0.780000\tOR: 0.378641\tOF1: 0.509804\n",
      "Train Epoch: 0 [2368/4307 (54%)]\tLoss: 0.481918 \tOP: 0.661290\tOR: 0.440860\tOF1: 0.529032\n",
      "Train Epoch: 0 [2432/4307 (56%)]\tLoss: 0.524842 \tOP: 0.686275\tOR: 0.336538\tOF1: 0.451613\n",
      "Train Epoch: 0 [2496/4307 (57%)]\tLoss: 0.533300 \tOP: 0.589286\tOR: 0.340206\tOF1: 0.431373\n",
      "Train Epoch: 0 [2560/4307 (59%)]\tLoss: 0.534499 \tOP: 0.673469\tOR: 0.314286\tOF1: 0.428571\n",
      "Train Epoch: 0 [2624/4307 (60%)]\tLoss: 0.495856 \tOP: 0.641509\tOR: 0.365591\tOF1: 0.465753\n",
      "Train Epoch: 0 [2688/4307 (62%)]\tLoss: 0.527930 \tOP: 0.725490\tOR: 0.339450\tOF1: 0.462500\n",
      "Train Epoch: 0 [2752/4307 (63%)]\tLoss: 0.451145 \tOP: 0.750000\tOR: 0.419355\tOF1: 0.537931\n",
      "Train Epoch: 0 [2816/4307 (65%)]\tLoss: 0.488551 \tOP: 0.764706\tOR: 0.393939\tOF1: 0.520000\n",
      "Train Epoch: 0 [2880/4307 (66%)]\tLoss: 0.498736 \tOP: 0.660000\tOR: 0.340206\tOF1: 0.448980\n",
      "Train Epoch: 0 [2944/4307 (68%)]\tLoss: 0.524021 \tOP: 0.653846\tOR: 0.340000\tOF1: 0.447368\n",
      "Train Epoch: 0 [3008/4307 (69%)]\tLoss: 0.500416 \tOP: 0.745098\tOR: 0.365385\tOF1: 0.490323\n",
      "Train Epoch: 0 [3072/4307 (71%)]\tLoss: 0.434579 \tOP: 0.814815\tOR: 0.463158\tOF1: 0.590604\n",
      "Train Epoch: 0 [3136/4307 (72%)]\tLoss: 0.457145 \tOP: 0.795918\tOR: 0.397959\tOF1: 0.530612\n",
      "Train Epoch: 0 [3200/4307 (74%)]\tLoss: 0.509566 \tOP: 0.647059\tOR: 0.333333\tOF1: 0.440000\n",
      "Train Epoch: 0 [3264/4307 (75%)]\tLoss: 0.434150 \tOP: 0.812500\tOR: 0.406250\tOF1: 0.541667\n",
      "Train Epoch: 0 [3328/4307 (76%)]\tLoss: 0.525639 \tOP: 0.660000\tOR: 0.317308\tOF1: 0.428571\n",
      "Train Epoch: 0 [3392/4307 (78%)]\tLoss: 0.418668 \tOP: 0.814815\tOR: 0.444444\tOF1: 0.575163\n",
      "Train Epoch: 0 [3456/4307 (79%)]\tLoss: 0.512787 \tOP: 0.666667\tOR: 0.369565\tOF1: 0.475524\n",
      "Train Epoch: 0 [3520/4307 (81%)]\tLoss: 0.501360 \tOP: 0.744681\tOR: 0.333333\tOF1: 0.460526\n",
      "Train Epoch: 0 [3584/4307 (82%)]\tLoss: 0.467287 \tOP: 0.666667\tOR: 0.319149\tOF1: 0.431655\n",
      "Train Epoch: 0 [3648/4307 (84%)]\tLoss: 0.490529 \tOP: 0.732143\tOR: 0.398058\tOF1: 0.515723\n",
      "Train Epoch: 0 [3712/4307 (85%)]\tLoss: 0.440916 \tOP: 0.745098\tOR: 0.408602\tOF1: 0.527778\n",
      "Train Epoch: 0 [3776/4307 (87%)]\tLoss: 0.520163 \tOP: 0.571429\tOR: 0.294737\tOF1: 0.388889\n",
      "Train Epoch: 0 [3840/4307 (88%)]\tLoss: 0.438778 \tOP: 0.725490\tOR: 0.393617\tOF1: 0.510345\n",
      "Train Epoch: 0 [3904/4307 (90%)]\tLoss: 0.429678 \tOP: 0.800000\tOR: 0.427184\tOF1: 0.556962\n",
      "Train Epoch: 0 [3968/4307 (91%)]\tLoss: 0.495068 \tOP: 0.688889\tOR: 0.303922\tOF1: 0.421769\n",
      "Train Epoch: 0 [4032/4307 (93%)]\tLoss: 0.485567 \tOP: 0.720000\tOR: 0.342857\tOF1: 0.464516\n",
      "Train Epoch: 0 [4096/4307 (94%)]\tLoss: 0.484077 \tOP: 0.744186\tOR: 0.301887\tOF1: 0.429530\n",
      "Train Epoch: 0 [4160/4307 (96%)]\tLoss: 0.496203 \tOP: 0.734694\tOR: 0.356436\tOF1: 0.480000\n",
      "Train Epoch: 0 [4224/4307 (97%)]\tLoss: 0.475505 \tOP: 0.812500\tOR: 0.390000\tOF1: 0.527027\n",
      "Train Epoch: 0 [1273/4307 (99%)]\tLoss: 0.546904 \tOP: 0.611111\tOR: 0.354839\tOF1: 0.448980\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-eea8f805c656>:41: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.4667 \n",
      "OP: 0.688889\n",
      "OR: 0.407895\n",
      "OF1: 0.512397\n",
      "\n",
      "Train Epoch: 1 [0/4307 (0%)]\tLoss: 0.357526 \tOP: 0.836364\tOR: 0.450980\tOF1: 0.585987\n",
      "Train Epoch: 1 [64/4307 (1%)]\tLoss: 0.382521 \tOP: 0.862745\tOR: 0.431373\tOF1: 0.575163\n",
      "Train Epoch: 1 [128/4307 (3%)]\tLoss: 0.417747 \tOP: 0.888889\tOR: 0.444444\tOF1: 0.592593\n",
      "Train Epoch: 1 [192/4307 (4%)]\tLoss: 0.381194 \tOP: 0.909091\tOR: 0.485437\tOF1: 0.632911\n",
      "Train Epoch: 1 [256/4307 (6%)]\tLoss: 0.393850 \tOP: 0.903846\tOR: 0.456311\tOF1: 0.606452\n",
      "Train Epoch: 1 [320/4307 (7%)]\tLoss: 0.328996 \tOP: 0.950000\tOR: 0.600000\tOF1: 0.735484\n",
      "Train Epoch: 1 [384/4307 (9%)]\tLoss: 0.333777 \tOP: 0.962963\tOR: 0.509804\tOF1: 0.666667\n",
      "Train Epoch: 1 [448/4307 (10%)]\tLoss: 0.373049 \tOP: 0.854839\tOR: 0.535354\tOF1: 0.658385\n",
      "Train Epoch: 1 [512/4307 (12%)]\tLoss: 0.350237 \tOP: 0.884615\tOR: 0.489362\tOF1: 0.630137\n",
      "Train Epoch: 1 [576/4307 (13%)]\tLoss: 0.351693 \tOP: 0.909091\tOR: 0.520833\tOF1: 0.662252\n",
      "Train Epoch: 1 [640/4307 (15%)]\tLoss: 0.381123 \tOP: 0.895833\tOR: 0.417476\tOF1: 0.569536\n",
      "Train Epoch: 1 [704/4307 (16%)]\tLoss: 0.349556 \tOP: 0.947368\tOR: 0.586957\tOF1: 0.724832\n",
      "Train Epoch: 1 [768/4307 (18%)]\tLoss: 0.375744 \tOP: 0.851852\tOR: 0.455446\tOF1: 0.593548\n",
      "Train Epoch: 1 [832/4307 (19%)]\tLoss: 0.377704 \tOP: 0.875000\tOR: 0.453704\tOF1: 0.597561\n",
      "Train Epoch: 1 [896/4307 (21%)]\tLoss: 0.346583 \tOP: 0.847458\tOR: 0.520833\tOF1: 0.645161\n",
      "Train Epoch: 1 [960/4307 (22%)]\tLoss: 0.417039 \tOP: 0.796610\tOR: 0.465347\tOF1: 0.587500\n",
      "Train Epoch: 1 [1024/4307 (24%)]\tLoss: 0.342926 \tOP: 0.875000\tOR: 0.538462\tOF1: 0.666667\n",
      "Train Epoch: 1 [1088/4307 (25%)]\tLoss: 0.368048 \tOP: 0.883333\tOR: 0.500000\tOF1: 0.638554\n",
      "Train Epoch: 1 [1152/4307 (26%)]\tLoss: 0.408319 \tOP: 0.890909\tOR: 0.437500\tOF1: 0.586826\n",
      "Train Epoch: 1 [1216/4307 (28%)]\tLoss: 0.311904 \tOP: 0.898305\tOR: 0.552083\tOF1: 0.683871\n",
      "Train Epoch: 1 [1280/4307 (29%)]\tLoss: 0.341531 \tOP: 0.935484\tOR: 0.557692\tOF1: 0.698795\n",
      "Train Epoch: 1 [1344/4307 (31%)]\tLoss: 0.365782 \tOP: 0.839286\tOR: 0.460784\tOF1: 0.594937\n",
      "Train Epoch: 1 [1408/4307 (32%)]\tLoss: 0.327835 \tOP: 0.947368\tOR: 0.568421\tOF1: 0.710526\n",
      "Train Epoch: 1 [1472/4307 (34%)]\tLoss: 0.354762 \tOP: 0.877193\tOR: 0.510204\tOF1: 0.645161\n",
      "Train Epoch: 1 [1536/4307 (35%)]\tLoss: 0.334042 \tOP: 0.933333\tOR: 0.533333\tOF1: 0.678788\n",
      "Train Epoch: 1 [1600/4307 (37%)]\tLoss: 0.310840 \tOP: 0.928571\tOR: 0.541667\tOF1: 0.684211\n",
      "Train Epoch: 1 [1664/4307 (38%)]\tLoss: 0.341953 \tOP: 0.859649\tOR: 0.500000\tOF1: 0.632258\n",
      "Train Epoch: 1 [1728/4307 (40%)]\tLoss: 0.360941 \tOP: 0.887097\tOR: 0.544554\tOF1: 0.674847\n",
      "Train Epoch: 1 [1792/4307 (41%)]\tLoss: 0.354338 \tOP: 0.813559\tOR: 0.516129\tOF1: 0.631579\n",
      "Train Epoch: 1 [1856/4307 (43%)]\tLoss: 0.327149 \tOP: 0.903226\tOR: 0.577320\tOF1: 0.704403\n",
      "Train Epoch: 1 [1920/4307 (44%)]\tLoss: 0.346879 \tOP: 0.862069\tOR: 0.515464\tOF1: 0.645161\n",
      "Train Epoch: 1 [1984/4307 (46%)]\tLoss: 0.361081 \tOP: 0.866667\tOR: 0.495238\tOF1: 0.630303\n",
      "Train Epoch: 1 [2048/4307 (47%)]\tLoss: 0.316383 \tOP: 0.918033\tOR: 0.615385\tOF1: 0.736842\n",
      "Train Epoch: 1 [2112/4307 (49%)]\tLoss: 0.345394 \tOP: 0.888889\tOR: 0.484848\tOF1: 0.627451\n",
      "Train Epoch: 1 [2176/4307 (50%)]\tLoss: 0.345798 \tOP: 0.901639\tOR: 0.533981\tOF1: 0.670732\n",
      "Train Epoch: 1 [2240/4307 (51%)]\tLoss: 0.353530 \tOP: 0.870370\tOR: 0.460784\tOF1: 0.602564\n",
      "Train Epoch: 1 [2304/4307 (53%)]\tLoss: 0.306477 \tOP: 0.916667\tOR: 0.539216\tOF1: 0.679012\n",
      "Train Epoch: 1 [2368/4307 (54%)]\tLoss: 0.301081 \tOP: 0.949153\tOR: 0.571429\tOF1: 0.713376\n",
      "Train Epoch: 1 [2432/4307 (56%)]\tLoss: 0.351875 \tOP: 0.824561\tOR: 0.489583\tOF1: 0.614379\n",
      "Train Epoch: 1 [2496/4307 (57%)]\tLoss: 0.343559 \tOP: 0.839286\tOR: 0.474747\tOF1: 0.606452\n",
      "Train Epoch: 1 [2560/4307 (59%)]\tLoss: 0.343817 \tOP: 0.873016\tOR: 0.523810\tOF1: 0.654762\n",
      "Train Epoch: 1 [2624/4307 (60%)]\tLoss: 0.324126 \tOP: 0.898305\tOR: 0.540816\tOF1: 0.675159\n",
      "Train Epoch: 1 [2688/4307 (62%)]\tLoss: 0.351075 \tOP: 0.894737\tOR: 0.490385\tOF1: 0.633540\n",
      "Train Epoch: 1 [2752/4307 (63%)]\tLoss: 0.352503 \tOP: 0.881356\tOR: 0.495238\tOF1: 0.634146\n",
      "Train Epoch: 1 [2816/4307 (65%)]\tLoss: 0.315837 \tOP: 0.951613\tOR: 0.572816\tOF1: 0.715152\n",
      "Train Epoch: 1 [2880/4307 (66%)]\tLoss: 0.315035 \tOP: 0.898305\tOR: 0.540816\tOF1: 0.675159\n",
      "Train Epoch: 1 [2944/4307 (68%)]\tLoss: 0.352039 \tOP: 0.859649\tOR: 0.485149\tOF1: 0.620253\n",
      "Train Epoch: 1 [3008/4307 (69%)]\tLoss: 0.320542 \tOP: 0.935484\tOR: 0.563107\tOF1: 0.703030\n",
      "Train Epoch: 1 [3072/4307 (71%)]\tLoss: 0.327188 \tOP: 0.901639\tOR: 0.567010\tOF1: 0.696203\n",
      "Train Epoch: 1 [3136/4307 (72%)]\tLoss: 0.347852 \tOP: 0.887097\tOR: 0.550000\tOF1: 0.679012\n",
      "Train Epoch: 1 [3200/4307 (74%)]\tLoss: 0.307904 \tOP: 0.919355\tOR: 0.593750\tOF1: 0.721519\n",
      "Train Epoch: 1 [3264/4307 (75%)]\tLoss: 0.350344 \tOP: 0.813559\tOR: 0.470588\tOF1: 0.596273\n",
      "Train Epoch: 1 [3328/4307 (76%)]\tLoss: 0.342466 \tOP: 0.916667\tOR: 0.533981\tOF1: 0.674847\n",
      "Train Epoch: 1 [3392/4307 (78%)]\tLoss: 0.348476 \tOP: 0.859375\tOR: 0.585106\tOF1: 0.696203\n",
      "Train Epoch: 1 [3456/4307 (79%)]\tLoss: 0.355829 \tOP: 0.779661\tOR: 0.484211\tOF1: 0.597403\n",
      "Train Epoch: 1 [3520/4307 (81%)]\tLoss: 0.302105 \tOP: 0.919355\tOR: 0.587629\tOF1: 0.716981\n",
      "Train Epoch: 1 [3584/4307 (82%)]\tLoss: 0.339593 \tOP: 0.879310\tOR: 0.531250\tOF1: 0.662338\n",
      "Train Epoch: 1 [3648/4307 (84%)]\tLoss: 0.319074 \tOP: 0.890625\tOR: 0.581633\tOF1: 0.703704\n",
      "Train Epoch: 1 [3712/4307 (85%)]\tLoss: 0.319481 \tOP: 0.883333\tOR: 0.582418\tOF1: 0.701987\n",
      "Train Epoch: 1 [3776/4307 (87%)]\tLoss: 0.291184 \tOP: 0.982759\tOR: 0.564356\tOF1: 0.716981\n",
      "Train Epoch: 1 [3840/4307 (88%)]\tLoss: 0.330118 \tOP: 0.923077\tOR: 0.618557\tOF1: 0.740741\n",
      "Train Epoch: 1 [3904/4307 (90%)]\tLoss: 0.339058 \tOP: 0.826923\tOR: 0.447917\tOF1: 0.581081\n",
      "Train Epoch: 1 [3968/4307 (91%)]\tLoss: 0.336322 \tOP: 0.929825\tOR: 0.535354\tOF1: 0.679487\n",
      "Train Epoch: 1 [4032/4307 (93%)]\tLoss: 0.367689 \tOP: 0.807692\tOR: 0.424242\tOF1: 0.556291\n",
      "Train Epoch: 1 [4096/4307 (94%)]\tLoss: 0.363962 \tOP: 0.965517\tOR: 0.491228\tOF1: 0.651163\n",
      "Train Epoch: 1 [4160/4307 (96%)]\tLoss: 0.346288 \tOP: 0.862069\tOR: 0.500000\tOF1: 0.632911\n",
      "Train Epoch: 1 [4224/4307 (97%)]\tLoss: 0.327651 \tOP: 0.888889\tOR: 0.577320\tOF1: 0.700000\n",
      "Train Epoch: 1 [1273/4307 (99%)]\tLoss: 0.251614 \tOP: 0.900000\tOR: 0.720000\tOF1: 0.800000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.4679 \n",
      "OP: 0.772727\n",
      "OR: 0.447368\n",
      "OF1: 0.566667\n",
      "\n",
      "Train Epoch: 2 [0/4307 (0%)]\tLoss: 0.264182 \tOP: 1.000000\tOR: 0.574257\tOF1: 0.729560\n",
      "Train Epoch: 2 [64/4307 (1%)]\tLoss: 0.245534 \tOP: 0.970149\tOR: 0.607477\tOF1: 0.747126\n",
      "Train Epoch: 2 [128/4307 (3%)]\tLoss: 0.231284 \tOP: 1.000000\tOR: 0.653846\tOF1: 0.790698\n",
      "Train Epoch: 2 [192/4307 (4%)]\tLoss: 0.229176 \tOP: 0.968750\tOR: 0.632653\tOF1: 0.765432\n",
      "Train Epoch: 2 [256/4307 (6%)]\tLoss: 0.227346 \tOP: 1.000000\tOR: 0.649485\tOF1: 0.787500\n",
      "Train Epoch: 2 [320/4307 (7%)]\tLoss: 0.208136 \tOP: 1.000000\tOR: 0.679245\tOF1: 0.808989\n",
      "Train Epoch: 2 [384/4307 (9%)]\tLoss: 0.229035 \tOP: 0.942857\tOR: 0.666667\tOF1: 0.781065\n",
      "Train Epoch: 2 [448/4307 (10%)]\tLoss: 0.226107 \tOP: 0.971014\tOR: 0.670000\tOF1: 0.792899\n",
      "Train Epoch: 2 [512/4307 (12%)]\tLoss: 0.206397 \tOP: 0.985294\tOR: 0.770115\tOF1: 0.864516\n",
      "Train Epoch: 2 [576/4307 (13%)]\tLoss: 0.195092 \tOP: 1.000000\tOR: 0.742268\tOF1: 0.852071\n",
      "Train Epoch: 2 [640/4307 (15%)]\tLoss: 0.214528 \tOP: 0.953846\tOR: 0.688889\tOF1: 0.800000\n",
      "Train Epoch: 2 [704/4307 (16%)]\tLoss: 0.251894 \tOP: 0.969231\tOR: 0.600000\tOF1: 0.741176\n",
      "Train Epoch: 2 [768/4307 (18%)]\tLoss: 0.229010 \tOP: 0.942029\tOR: 0.684211\tOF1: 0.792683\n",
      "Train Epoch: 2 [832/4307 (19%)]\tLoss: 0.206073 \tOP: 0.960526\tOR: 0.752577\tOF1: 0.843931\n",
      "Train Epoch: 2 [896/4307 (21%)]\tLoss: 0.213271 \tOP: 1.000000\tOR: 0.669903\tOF1: 0.802326\n",
      "Train Epoch: 2 [960/4307 (22%)]\tLoss: 0.230623 \tOP: 0.971831\tOR: 0.657143\tOF1: 0.784091\n",
      "Train Epoch: 2 [1024/4307 (24%)]\tLoss: 0.235897 \tOP: 0.971014\tOR: 0.598214\tOF1: 0.740331\n",
      "Train Epoch: 2 [1088/4307 (25%)]\tLoss: 0.223216 \tOP: 0.954545\tOR: 0.649485\tOF1: 0.773006\n",
      "Train Epoch: 2 [1152/4307 (26%)]\tLoss: 0.229689 \tOP: 0.958904\tOR: 0.636364\tOF1: 0.765027\n",
      "Train Epoch: 2 [1216/4307 (28%)]\tLoss: 0.188681 \tOP: 0.986486\tOR: 0.784946\tOF1: 0.874251\n",
      "Train Epoch: 2 [1280/4307 (29%)]\tLoss: 0.234091 \tOP: 0.957746\tOR: 0.701031\tOF1: 0.809524\n",
      "Train Epoch: 2 [1344/4307 (31%)]\tLoss: 0.207198 \tOP: 0.986111\tOR: 0.724490\tOF1: 0.835294\n",
      "Train Epoch: 2 [1408/4307 (32%)]\tLoss: 0.181649 \tOP: 1.000000\tOR: 0.808511\tOF1: 0.894118\n",
      "Train Epoch: 2 [1472/4307 (34%)]\tLoss: 0.186136 \tOP: 0.987179\tOR: 0.785714\tOF1: 0.875000\n",
      "Train Epoch: 2 [1536/4307 (35%)]\tLoss: 0.200225 \tOP: 0.984615\tOR: 0.688172\tOF1: 0.810127\n",
      "Train Epoch: 2 [1600/4307 (37%)]\tLoss: 0.183446 \tOP: 0.988095\tOR: 0.805825\tOF1: 0.887701\n",
      "Train Epoch: 2 [1664/4307 (38%)]\tLoss: 0.180509 \tOP: 0.975309\tOR: 0.766990\tOF1: 0.858696\n",
      "Train Epoch: 2 [1728/4307 (40%)]\tLoss: 0.187966 \tOP: 1.000000\tOR: 0.719626\tOF1: 0.836957\n",
      "Train Epoch: 2 [1792/4307 (41%)]\tLoss: 0.194694 \tOP: 0.987654\tOR: 0.784314\tOF1: 0.874317\n",
      "Train Epoch: 2 [1856/4307 (43%)]\tLoss: 0.186237 \tOP: 0.987500\tOR: 0.782178\tOF1: 0.872928\n",
      "Train Epoch: 2 [1920/4307 (44%)]\tLoss: 0.181575 \tOP: 0.972603\tOR: 0.797753\tOF1: 0.876543\n",
      "Train Epoch: 2 [1984/4307 (46%)]\tLoss: 0.214506 \tOP: 0.960526\tOR: 0.752577\tOF1: 0.843931\n",
      "Train Epoch: 2 [2048/4307 (47%)]\tLoss: 0.194248 \tOP: 0.986842\tOR: 0.742574\tOF1: 0.847458\n",
      "Train Epoch: 2 [2112/4307 (49%)]\tLoss: 0.205730 \tOP: 0.971831\tOR: 0.726316\tOF1: 0.831325\n",
      "Train Epoch: 2 [2176/4307 (50%)]\tLoss: 0.180379 \tOP: 0.986486\tOR: 0.776596\tOF1: 0.869048\n",
      "Train Epoch: 2 [2240/4307 (51%)]\tLoss: 0.213633 \tOP: 0.945205\tOR: 0.696970\tOF1: 0.802326\n",
      "Train Epoch: 2 [2304/4307 (53%)]\tLoss: 0.158111 \tOP: 1.000000\tOR: 0.770000\tOF1: 0.870056\n",
      "Train Epoch: 2 [2368/4307 (54%)]\tLoss: 0.198798 \tOP: 0.959459\tOR: 0.747368\tOF1: 0.840237\n",
      "Train Epoch: 2 [2432/4307 (56%)]\tLoss: 0.187848 \tOP: 1.000000\tOR: 0.711538\tOF1: 0.831461\n",
      "Train Epoch: 2 [2496/4307 (57%)]\tLoss: 0.202279 \tOP: 0.960000\tOR: 0.727273\tOF1: 0.827586\n",
      "Train Epoch: 2 [2560/4307 (59%)]\tLoss: 0.206960 \tOP: 0.960000\tOR: 0.727273\tOF1: 0.827586\n",
      "Train Epoch: 2 [2624/4307 (60%)]\tLoss: 0.171911 \tOP: 0.986301\tOR: 0.750000\tOF1: 0.852071\n",
      "Train Epoch: 2 [2688/4307 (62%)]\tLoss: 0.181679 \tOP: 0.986667\tOR: 0.718447\tOF1: 0.831461\n",
      "Train Epoch: 2 [2752/4307 (63%)]\tLoss: 0.242034 \tOP: 0.968750\tOR: 0.626263\tOF1: 0.760736\n",
      "Train Epoch: 2 [2816/4307 (65%)]\tLoss: 0.182295 \tOP: 1.000000\tOR: 0.711538\tOF1: 0.831461\n",
      "Train Epoch: 2 [2880/4307 (66%)]\tLoss: 0.163670 \tOP: 0.958904\tOR: 0.786517\tOF1: 0.864198\n",
      "Train Epoch: 2 [2944/4307 (68%)]\tLoss: 0.194827 \tOP: 0.960526\tOR: 0.737374\tOF1: 0.834286\n",
      "Train Epoch: 2 [3008/4307 (69%)]\tLoss: 0.178133 \tOP: 1.000000\tOR: 0.747368\tOF1: 0.855422\n",
      "Train Epoch: 2 [3072/4307 (71%)]\tLoss: 0.202542 \tOP: 0.986842\tOR: 0.675676\tOF1: 0.802139\n",
      "Train Epoch: 2 [3136/4307 (72%)]\tLoss: 0.193793 \tOP: 0.947368\tOR: 0.742268\tOF1: 0.832370\n",
      "Train Epoch: 2 [3200/4307 (74%)]\tLoss: 0.179262 \tOP: 1.000000\tOR: 0.760000\tOF1: 0.863636\n",
      "Train Epoch: 2 [3264/4307 (75%)]\tLoss: 0.181437 \tOP: 0.986111\tOR: 0.717172\tOF1: 0.830409\n",
      "Train Epoch: 2 [3328/4307 (76%)]\tLoss: 0.185429 \tOP: 0.975000\tOR: 0.757282\tOF1: 0.852459\n",
      "Train Epoch: 2 [3392/4307 (78%)]\tLoss: 0.196411 \tOP: 0.940476\tOR: 0.766990\tOF1: 0.844920\n",
      "Train Epoch: 2 [3456/4307 (79%)]\tLoss: 0.199767 \tOP: 0.986667\tOR: 0.725490\tOF1: 0.836158\n",
      "Train Epoch: 2 [3520/4307 (81%)]\tLoss: 0.164345 \tOP: 0.962025\tOR: 0.817204\tOF1: 0.883721\n",
      "Train Epoch: 2 [3584/4307 (82%)]\tLoss: 0.189237 \tOP: 0.974359\tOR: 0.760000\tOF1: 0.853933\n",
      "Train Epoch: 2 [3648/4307 (84%)]\tLoss: 0.191608 \tOP: 0.959459\tOR: 0.731959\tOF1: 0.830409\n",
      "Train Epoch: 2 [3712/4307 (85%)]\tLoss: 0.186999 \tOP: 1.000000\tOR: 0.754717\tOF1: 0.860215\n",
      "Train Epoch: 2 [3776/4307 (87%)]\tLoss: 0.226821 \tOP: 0.947368\tOR: 0.705882\tOF1: 0.808989\n",
      "Train Epoch: 2 [3840/4307 (88%)]\tLoss: 0.159734 \tOP: 1.000000\tOR: 0.755102\tOF1: 0.860465\n",
      "Train Epoch: 2 [3904/4307 (90%)]\tLoss: 0.179048 \tOP: 1.000000\tOR: 0.769231\tOF1: 0.869565\n",
      "Train Epoch: 2 [3968/4307 (91%)]\tLoss: 0.189216 \tOP: 0.986667\tOR: 0.732673\tOF1: 0.840909\n",
      "Train Epoch: 2 [4032/4307 (93%)]\tLoss: 0.225310 \tOP: 0.930556\tOR: 0.663366\tOF1: 0.774566\n",
      "Train Epoch: 2 [4096/4307 (94%)]\tLoss: 0.178722 \tOP: 0.987654\tOR: 0.747664\tOF1: 0.851064\n",
      "Train Epoch: 2 [4160/4307 (96%)]\tLoss: 0.183960 \tOP: 0.987342\tOR: 0.735849\tOF1: 0.843243\n",
      "Train Epoch: 2 [4224/4307 (97%)]\tLoss: 0.184231 \tOP: 0.952941\tOR: 0.778846\tOF1: 0.857143\n",
      "Train Epoch: 2 [1273/4307 (99%)]\tLoss: 0.238003 \tOP: 0.900000\tOR: 0.642857\tOF1: 0.750000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5171 \n",
      "OP: 0.714286\n",
      "OR: 0.592105\n",
      "OF1: 0.647482\n",
      "\n",
      "Train Epoch: 3 [0/4307 (0%)]\tLoss: 0.129760 \tOP: 1.000000\tOR: 0.920455\tOF1: 0.958580\n",
      "Train Epoch: 3 [64/4307 (1%)]\tLoss: 0.110476 \tOP: 1.000000\tOR: 0.907216\tOF1: 0.951351\n",
      "Train Epoch: 3 [128/4307 (3%)]\tLoss: 0.100413 \tOP: 0.989691\tOR: 0.960000\tOF1: 0.974619\n",
      "Train Epoch: 3 [192/4307 (4%)]\tLoss: 0.101243 \tOP: 1.000000\tOR: 0.919192\tOF1: 0.957895\n",
      "Train Epoch: 3 [256/4307 (6%)]\tLoss: 0.121048 \tOP: 0.978723\tOR: 0.901961\tOF1: 0.938776\n",
      "Train Epoch: 3 [320/4307 (7%)]\tLoss: 0.139323 \tOP: 1.000000\tOR: 0.841121\tOF1: 0.913706\n",
      "Train Epoch: 3 [384/4307 (9%)]\tLoss: 0.095688 \tOP: 1.000000\tOR: 0.934066\tOF1: 0.965909\n",
      "Train Epoch: 3 [448/4307 (10%)]\tLoss: 0.127003 \tOP: 1.000000\tOR: 0.871287\tOF1: 0.931217\n",
      "Train Epoch: 3 [512/4307 (12%)]\tLoss: 0.135378 \tOP: 0.988636\tOR: 0.836538\tOF1: 0.906250\n",
      "Train Epoch: 3 [576/4307 (13%)]\tLoss: 0.127586 \tOP: 0.976471\tOR: 0.912088\tOF1: 0.943182\n",
      "Train Epoch: 3 [640/4307 (15%)]\tLoss: 0.103250 \tOP: 1.000000\tOR: 0.955056\tOF1: 0.977011\n",
      "Train Epoch: 3 [704/4307 (16%)]\tLoss: 0.113708 \tOP: 1.000000\tOR: 0.885714\tOF1: 0.939394\n",
      "Train Epoch: 3 [768/4307 (18%)]\tLoss: 0.109979 \tOP: 0.978947\tOR: 0.869159\tOF1: 0.920792\n",
      "Train Epoch: 3 [832/4307 (19%)]\tLoss: 0.098138 \tOP: 1.000000\tOR: 0.928571\tOF1: 0.962963\n",
      "Train Epoch: 3 [896/4307 (21%)]\tLoss: 0.139388 \tOP: 0.965116\tOR: 0.846939\tOF1: 0.902174\n",
      "Train Epoch: 3 [960/4307 (22%)]\tLoss: 0.107498 \tOP: 1.000000\tOR: 0.864078\tOF1: 0.927083\n",
      "Train Epoch: 3 [1024/4307 (24%)]\tLoss: 0.103610 \tOP: 1.000000\tOR: 0.885417\tOF1: 0.939227\n",
      "Train Epoch: 3 [1088/4307 (25%)]\tLoss: 0.111815 \tOP: 0.979798\tOR: 0.906542\tOF1: 0.941748\n",
      "Train Epoch: 3 [1152/4307 (26%)]\tLoss: 0.119145 \tOP: 0.965116\tOR: 0.892473\tOF1: 0.927374\n",
      "Train Epoch: 3 [1216/4307 (28%)]\tLoss: 0.115526 \tOP: 1.000000\tOR: 0.884211\tOF1: 0.938547\n",
      "Train Epoch: 3 [1280/4307 (29%)]\tLoss: 0.106577 \tOP: 1.000000\tOR: 0.890000\tOF1: 0.941799\n",
      "Train Epoch: 3 [1344/4307 (31%)]\tLoss: 0.092610 \tOP: 1.000000\tOR: 0.939394\tOF1: 0.968750\n",
      "Train Epoch: 3 [1408/4307 (32%)]\tLoss: 0.110575 \tOP: 1.000000\tOR: 0.864078\tOF1: 0.927083\n",
      "Train Epoch: 3 [1472/4307 (34%)]\tLoss: 0.102826 \tOP: 0.988235\tOR: 0.857143\tOF1: 0.918033\n",
      "Train Epoch: 3 [1536/4307 (35%)]\tLoss: 0.109085 \tOP: 1.000000\tOR: 0.875000\tOF1: 0.933333\n",
      "Train Epoch: 3 [1600/4307 (37%)]\tLoss: 0.107823 \tOP: 1.000000\tOR: 0.892473\tOF1: 0.943182\n",
      "Train Epoch: 3 [1664/4307 (38%)]\tLoss: 0.109060 \tOP: 1.000000\tOR: 0.880734\tOF1: 0.936585\n",
      "Train Epoch: 3 [1728/4307 (40%)]\tLoss: 0.115467 \tOP: 0.954023\tOR: 0.864583\tOF1: 0.907104\n",
      "Train Epoch: 3 [1792/4307 (41%)]\tLoss: 0.103109 \tOP: 1.000000\tOR: 0.904255\tOF1: 0.949721\n",
      "Train Epoch: 3 [1856/4307 (43%)]\tLoss: 0.118958 \tOP: 0.989247\tOR: 0.867925\tOF1: 0.924623\n",
      "Train Epoch: 3 [1920/4307 (44%)]\tLoss: 0.104927 \tOP: 0.989247\tOR: 0.901961\tOF1: 0.943590\n",
      "Train Epoch: 3 [1984/4307 (46%)]\tLoss: 0.102751 \tOP: 1.000000\tOR: 0.930000\tOF1: 0.963731\n",
      "Train Epoch: 3 [2048/4307 (47%)]\tLoss: 0.122128 \tOP: 0.978723\tOR: 0.844037\tOF1: 0.906404\n",
      "Train Epoch: 3 [2112/4307 (49%)]\tLoss: 0.109778 \tOP: 0.988764\tOR: 0.854369\tOF1: 0.916667\n",
      "Train Epoch: 3 [2176/4307 (50%)]\tLoss: 0.111756 \tOP: 1.000000\tOR: 0.852941\tOF1: 0.920635\n",
      "Train Epoch: 3 [2240/4307 (51%)]\tLoss: 0.106472 \tOP: 0.989474\tOR: 0.870370\tOF1: 0.926108\n",
      "Train Epoch: 3 [2304/4307 (53%)]\tLoss: 0.117460 \tOP: 0.976471\tOR: 0.855670\tOF1: 0.912088\n",
      "Train Epoch: 3 [2368/4307 (54%)]\tLoss: 0.105666 \tOP: 1.000000\tOR: 0.900990\tOF1: 0.947917\n",
      "Train Epoch: 3 [2432/4307 (56%)]\tLoss: 0.116904 \tOP: 0.966667\tOR: 0.878788\tOF1: 0.920635\n",
      "Train Epoch: 3 [2496/4307 (57%)]\tLoss: 0.098875 \tOP: 0.989362\tOR: 0.894231\tOF1: 0.939394\n",
      "Train Epoch: 3 [2560/4307 (59%)]\tLoss: 0.113544 \tOP: 0.988506\tOR: 0.851485\tOF1: 0.914894\n",
      "Train Epoch: 3 [2624/4307 (60%)]\tLoss: 0.082418 \tOP: 1.000000\tOR: 0.930693\tOF1: 0.964103\n",
      "Train Epoch: 3 [2688/4307 (62%)]\tLoss: 0.122288 \tOP: 0.988636\tOR: 0.852941\tOF1: 0.915789\n",
      "Train Epoch: 3 [2752/4307 (63%)]\tLoss: 0.099648 \tOP: 1.000000\tOR: 0.871560\tOF1: 0.931373\n",
      "Train Epoch: 3 [2816/4307 (65%)]\tLoss: 0.086458 \tOP: 1.000000\tOR: 0.932692\tOF1: 0.965174\n",
      "Train Epoch: 3 [2880/4307 (66%)]\tLoss: 0.075882 \tOP: 1.000000\tOR: 0.935484\tOF1: 0.966667\n",
      "Train Epoch: 3 [2944/4307 (68%)]\tLoss: 0.123963 \tOP: 0.967742\tOR: 0.882353\tOF1: 0.923077\n",
      "Train Epoch: 3 [3008/4307 (69%)]\tLoss: 0.083399 \tOP: 1.000000\tOR: 0.947368\tOF1: 0.972973\n",
      "Train Epoch: 3 [3072/4307 (71%)]\tLoss: 0.098572 \tOP: 1.000000\tOR: 0.909091\tOF1: 0.952381\n",
      "Train Epoch: 3 [3136/4307 (72%)]\tLoss: 0.101003 \tOP: 1.000000\tOR: 0.905660\tOF1: 0.950495\n",
      "Train Epoch: 3 [3200/4307 (74%)]\tLoss: 0.094456 \tOP: 1.000000\tOR: 0.890000\tOF1: 0.941799\n",
      "Train Epoch: 3 [3264/4307 (75%)]\tLoss: 0.104730 \tOP: 0.988889\tOR: 0.908163\tOF1: 0.946809\n",
      "Train Epoch: 3 [3328/4307 (76%)]\tLoss: 0.094393 \tOP: 0.977528\tOR: 0.896907\tOF1: 0.935484\n",
      "Train Epoch: 3 [3392/4307 (78%)]\tLoss: 0.100938 \tOP: 0.989691\tOR: 0.905660\tOF1: 0.945813\n",
      "Train Epoch: 3 [3456/4307 (79%)]\tLoss: 0.088172 \tOP: 1.000000\tOR: 0.922330\tOF1: 0.959596\n",
      "Train Epoch: 3 [3520/4307 (81%)]\tLoss: 0.092963 \tOP: 0.989247\tOR: 0.929293\tOF1: 0.958333\n",
      "Train Epoch: 3 [3584/4307 (82%)]\tLoss: 0.096957 \tOP: 0.988235\tOR: 0.903226\tOF1: 0.943820\n",
      "Train Epoch: 3 [3648/4307 (84%)]\tLoss: 0.121137 \tOP: 0.979167\tOR: 0.870370\tOF1: 0.921569\n",
      "Train Epoch: 3 [3712/4307 (85%)]\tLoss: 0.078201 \tOP: 1.000000\tOR: 0.907407\tOF1: 0.951456\n",
      "Train Epoch: 3 [3776/4307 (87%)]\tLoss: 0.079006 \tOP: 1.000000\tOR: 0.934783\tOF1: 0.966292\n",
      "Train Epoch: 3 [3840/4307 (88%)]\tLoss: 0.105854 \tOP: 1.000000\tOR: 0.900990\tOF1: 0.947917\n",
      "Train Epoch: 3 [3904/4307 (90%)]\tLoss: 0.099166 \tOP: 1.000000\tOR: 0.890909\tOF1: 0.942308\n",
      "Train Epoch: 3 [3968/4307 (91%)]\tLoss: 0.084961 \tOP: 0.989474\tOR: 0.912621\tOF1: 0.949495\n",
      "Train Epoch: 3 [4032/4307 (93%)]\tLoss: 0.083397 \tOP: 1.000000\tOR: 0.958333\tOF1: 0.978723\n",
      "Train Epoch: 3 [4096/4307 (94%)]\tLoss: 0.064062 \tOP: 0.988235\tOR: 0.954545\tOF1: 0.971098\n",
      "Train Epoch: 3 [4160/4307 (96%)]\tLoss: 0.102801 \tOP: 0.977778\tOR: 0.916667\tOF1: 0.946237\n",
      "Train Epoch: 3 [4224/4307 (97%)]\tLoss: 0.102600 \tOP: 0.977778\tOR: 0.926316\tOF1: 0.951351\n",
      "Train Epoch: 3 [1273/4307 (99%)]\tLoss: 0.126010 \tOP: 1.000000\tOR: 0.806452\tOF1: 0.892857\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5683 \n",
      "OP: 0.728814\n",
      "OR: 0.565789\n",
      "OF1: 0.637037\n",
      "\n",
      "Train Epoch: 4 [0/4307 (0%)]\tLoss: 0.064706 \tOP: 1.000000\tOR: 0.958333\tOF1: 0.978723\n",
      "Train Epoch: 4 [64/4307 (1%)]\tLoss: 0.048121 \tOP: 1.000000\tOR: 0.979381\tOF1: 0.989583\n",
      "Train Epoch: 4 [128/4307 (3%)]\tLoss: 0.071557 \tOP: 1.000000\tOR: 0.950980\tOF1: 0.974874\n",
      "Train Epoch: 4 [192/4307 (4%)]\tLoss: 0.082268 \tOP: 1.000000\tOR: 0.913462\tOF1: 0.954774\n",
      "Train Epoch: 4 [256/4307 (6%)]\tLoss: 0.094541 \tOP: 0.978723\tOR: 0.929293\tOF1: 0.953368\n",
      "Train Epoch: 4 [320/4307 (7%)]\tLoss: 0.058545 \tOP: 1.000000\tOR: 0.968421\tOF1: 0.983957\n",
      "Train Epoch: 4 [384/4307 (9%)]\tLoss: 0.057766 \tOP: 1.000000\tOR: 0.952381\tOF1: 0.975610\n",
      "Train Epoch: 4 [448/4307 (10%)]\tLoss: 0.072171 \tOP: 1.000000\tOR: 0.932039\tOF1: 0.964824\n",
      "Train Epoch: 4 [512/4307 (12%)]\tLoss: 0.062512 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 4 [576/4307 (13%)]\tLoss: 0.069081 \tOP: 1.000000\tOR: 0.950980\tOF1: 0.974874\n",
      "Train Epoch: 4 [640/4307 (15%)]\tLoss: 0.063739 \tOP: 0.989583\tOR: 0.969388\tOF1: 0.979381\n",
      "Train Epoch: 4 [704/4307 (16%)]\tLoss: 0.066631 \tOP: 0.989796\tOR: 0.941748\tOF1: 0.965174\n",
      "Train Epoch: 4 [768/4307 (18%)]\tLoss: 0.074449 \tOP: 0.989011\tOR: 0.947368\tOF1: 0.967742\n",
      "Train Epoch: 4 [832/4307 (19%)]\tLoss: 0.063541 \tOP: 0.989899\tOR: 0.980000\tOF1: 0.984925\n",
      "Train Epoch: 4 [896/4307 (21%)]\tLoss: 0.061132 \tOP: 0.990000\tOR: 0.961165\tOF1: 0.975369\n",
      "Train Epoch: 4 [960/4307 (22%)]\tLoss: 0.066279 \tOP: 0.989796\tOR: 0.979798\tOF1: 0.984772\n",
      "Train Epoch: 4 [1024/4307 (24%)]\tLoss: 0.064133 \tOP: 1.000000\tOR: 0.978723\tOF1: 0.989247\n",
      "Train Epoch: 4 [1088/4307 (25%)]\tLoss: 0.078059 \tOP: 1.000000\tOR: 0.935780\tOF1: 0.966825\n",
      "Train Epoch: 4 [1152/4307 (26%)]\tLoss: 0.061036 \tOP: 0.990099\tOR: 0.961538\tOF1: 0.975610\n",
      "Train Epoch: 4 [1216/4307 (28%)]\tLoss: 0.056792 \tOP: 1.000000\tOR: 0.970297\tOF1: 0.984925\n",
      "Train Epoch: 4 [1280/4307 (29%)]\tLoss: 0.055294 \tOP: 1.000000\tOR: 0.980198\tOF1: 0.990000\n",
      "Train Epoch: 4 [1344/4307 (31%)]\tLoss: 0.071768 \tOP: 0.979798\tOR: 0.970000\tOF1: 0.974874\n",
      "Train Epoch: 4 [1408/4307 (32%)]\tLoss: 0.044258 \tOP: 1.000000\tOR: 0.989130\tOF1: 0.994536\n",
      "Train Epoch: 4 [1472/4307 (34%)]\tLoss: 0.073064 \tOP: 0.979167\tOR: 0.949495\tOF1: 0.964103\n",
      "Train Epoch: 4 [1536/4307 (35%)]\tLoss: 0.081398 \tOP: 1.000000\tOR: 0.883495\tOF1: 0.938144\n",
      "Train Epoch: 4 [1600/4307 (37%)]\tLoss: 0.070107 \tOP: 1.000000\tOR: 0.943396\tOF1: 0.970874\n",
      "Train Epoch: 4 [1664/4307 (38%)]\tLoss: 0.066390 \tOP: 1.000000\tOR: 0.917431\tOF1: 0.956938\n",
      "Train Epoch: 4 [1728/4307 (40%)]\tLoss: 0.075408 \tOP: 0.989691\tOR: 0.932039\tOF1: 0.960000\n",
      "Train Epoch: 4 [1792/4307 (41%)]\tLoss: 0.049528 \tOP: 1.000000\tOR: 0.970588\tOF1: 0.985075\n",
      "Train Epoch: 4 [1856/4307 (43%)]\tLoss: 0.054714 \tOP: 1.000000\tOR: 0.960784\tOF1: 0.980000\n",
      "Train Epoch: 4 [1920/4307 (44%)]\tLoss: 0.043550 \tOP: 1.000000\tOR: 0.978723\tOF1: 0.989247\n",
      "Train Epoch: 4 [1984/4307 (46%)]\tLoss: 0.088255 \tOP: 1.000000\tOR: 0.893805\tOF1: 0.943925\n",
      "Train Epoch: 4 [2048/4307 (47%)]\tLoss: 0.068763 \tOP: 1.000000\tOR: 0.955357\tOF1: 0.977169\n",
      "Train Epoch: 4 [2112/4307 (49%)]\tLoss: 0.058372 \tOP: 1.000000\tOR: 0.969072\tOF1: 0.984293\n",
      "Train Epoch: 4 [2176/4307 (50%)]\tLoss: 0.056944 \tOP: 0.989362\tOR: 0.968750\tOF1: 0.978947\n",
      "Train Epoch: 4 [2240/4307 (51%)]\tLoss: 0.076432 \tOP: 0.989011\tOR: 0.927835\tOF1: 0.957447\n",
      "Train Epoch: 4 [2304/4307 (53%)]\tLoss: 0.064287 \tOP: 1.000000\tOR: 0.921569\tOF1: 0.959184\n",
      "Train Epoch: 4 [2368/4307 (54%)]\tLoss: 0.050774 \tOP: 1.000000\tOR: 0.956044\tOF1: 0.977528\n",
      "Train Epoch: 4 [2432/4307 (56%)]\tLoss: 0.047615 \tOP: 1.000000\tOR: 0.980000\tOF1: 0.989899\n",
      "Train Epoch: 4 [2496/4307 (57%)]\tLoss: 0.079039 \tOP: 0.978022\tOR: 0.967391\tOF1: 0.972678\n",
      "Train Epoch: 4 [2560/4307 (59%)]\tLoss: 0.065034 \tOP: 0.989474\tOR: 0.959184\tOF1: 0.974093\n",
      "Train Epoch: 4 [2624/4307 (60%)]\tLoss: 0.061414 \tOP: 0.989583\tOR: 0.931373\tOF1: 0.959596\n",
      "Train Epoch: 4 [2688/4307 (62%)]\tLoss: 0.055050 \tOP: 1.000000\tOR: 0.990099\tOF1: 0.995025\n",
      "Train Epoch: 4 [2752/4307 (63%)]\tLoss: 0.055423 \tOP: 1.000000\tOR: 0.942308\tOF1: 0.970297\n",
      "Train Epoch: 4 [2816/4307 (65%)]\tLoss: 0.057222 \tOP: 1.000000\tOR: 0.979592\tOF1: 0.989691\n",
      "Train Epoch: 4 [2880/4307 (66%)]\tLoss: 0.052831 \tOP: 1.000000\tOR: 0.980198\tOF1: 0.990000\n",
      "Train Epoch: 4 [2944/4307 (68%)]\tLoss: 0.055127 \tOP: 1.000000\tOR: 0.945455\tOF1: 0.971963\n",
      "Train Epoch: 4 [3008/4307 (69%)]\tLoss: 0.047389 \tOP: 1.000000\tOR: 0.989583\tOF1: 0.994764\n",
      "Train Epoch: 4 [3072/4307 (71%)]\tLoss: 0.050525 \tOP: 1.000000\tOR: 0.970874\tOF1: 0.985222\n",
      "Train Epoch: 4 [3136/4307 (72%)]\tLoss: 0.054635 \tOP: 1.000000\tOR: 0.959184\tOF1: 0.979167\n",
      "Train Epoch: 4 [3200/4307 (74%)]\tLoss: 0.043653 \tOP: 1.000000\tOR: 0.989130\tOF1: 0.994536\n",
      "Train Epoch: 4 [3264/4307 (75%)]\tLoss: 0.060188 \tOP: 0.989691\tOR: 0.950495\tOF1: 0.969697\n",
      "Train Epoch: 4 [3328/4307 (76%)]\tLoss: 0.055531 \tOP: 0.978495\tOR: 1.000000\tOF1: 0.989130\n",
      "Train Epoch: 4 [3392/4307 (78%)]\tLoss: 0.054127 \tOP: 1.000000\tOR: 0.978495\tOF1: 0.989130\n",
      "Train Epoch: 4 [3456/4307 (79%)]\tLoss: 0.051396 \tOP: 1.000000\tOR: 0.931373\tOF1: 0.964467\n",
      "Train Epoch: 4 [3520/4307 (81%)]\tLoss: 0.056236 \tOP: 1.000000\tOR: 0.952381\tOF1: 0.975610\n",
      "Train Epoch: 4 [3584/4307 (82%)]\tLoss: 0.050113 \tOP: 1.000000\tOR: 0.967391\tOF1: 0.983425\n",
      "Train Epoch: 4 [3648/4307 (84%)]\tLoss: 0.047699 \tOP: 1.000000\tOR: 0.961538\tOF1: 0.980392\n",
      "Train Epoch: 4 [3712/4307 (85%)]\tLoss: 0.049149 \tOP: 1.000000\tOR: 0.960396\tOF1: 0.979798\n",
      "Train Epoch: 4 [3776/4307 (87%)]\tLoss: 0.049110 \tOP: 1.000000\tOR: 0.969388\tOF1: 0.984456\n",
      "Train Epoch: 4 [3840/4307 (88%)]\tLoss: 0.050677 \tOP: 1.000000\tOR: 0.957895\tOF1: 0.978495\n",
      "Train Epoch: 4 [3904/4307 (90%)]\tLoss: 0.048186 \tOP: 1.000000\tOR: 0.958763\tOF1: 0.978947\n",
      "Train Epoch: 4 [3968/4307 (91%)]\tLoss: 0.088245 \tOP: 0.968085\tOR: 0.900990\tOF1: 0.933333\n",
      "Train Epoch: 4 [4032/4307 (93%)]\tLoss: 0.058151 \tOP: 0.989583\tOR: 0.922330\tOF1: 0.954774\n",
      "Train Epoch: 4 [4096/4307 (94%)]\tLoss: 0.048324 \tOP: 1.000000\tOR: 0.940594\tOF1: 0.969388\n",
      "Train Epoch: 4 [4160/4307 (96%)]\tLoss: 0.071416 \tOP: 1.000000\tOR: 0.892473\tOF1: 0.943182\n",
      "Train Epoch: 4 [4224/4307 (97%)]\tLoss: 0.053904 \tOP: 0.989362\tOR: 0.958763\tOF1: 0.973822\n",
      "Train Epoch: 4 [1273/4307 (99%)]\tLoss: 0.085483 \tOP: 0.916667\tOR: 0.956522\tOF1: 0.936170\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6111 \n",
      "OP: 0.692308\n",
      "OR: 0.592105\n",
      "OF1: 0.638298\n",
      "\n",
      "Train Epoch: 5 [0/4307 (0%)]\tLoss: 0.035319 \tOP: 1.000000\tOR: 0.970297\tOF1: 0.984925\n",
      "Train Epoch: 5 [64/4307 (1%)]\tLoss: 0.038626 \tOP: 1.000000\tOR: 0.980583\tOF1: 0.990196\n",
      "Train Epoch: 5 [128/4307 (3%)]\tLoss: 0.042851 \tOP: 1.000000\tOR: 0.989691\tOF1: 0.994819\n",
      "Train Epoch: 5 [192/4307 (4%)]\tLoss: 0.038283 \tOP: 1.000000\tOR: 0.958333\tOF1: 0.978723\n",
      "Train Epoch: 5 [256/4307 (6%)]\tLoss: 0.049156 \tOP: 1.000000\tOR: 0.969697\tOF1: 0.984615\n",
      "Train Epoch: 5 [320/4307 (7%)]\tLoss: 0.030783 \tOP: 1.000000\tOR: 0.968421\tOF1: 0.983957\n",
      "Train Epoch: 5 [384/4307 (9%)]\tLoss: 0.038591 \tOP: 0.990000\tOR: 0.990000\tOF1: 0.990000\n",
      "Train Epoch: 5 [448/4307 (10%)]\tLoss: 0.047322 \tOP: 1.000000\tOR: 0.953704\tOF1: 0.976303\n",
      "Train Epoch: 5 [512/4307 (12%)]\tLoss: 0.040411 \tOP: 1.000000\tOR: 0.970297\tOF1: 0.984925\n",
      "Train Epoch: 5 [576/4307 (13%)]\tLoss: 0.045390 \tOP: 0.989899\tOR: 0.960784\tOF1: 0.975124\n",
      "Train Epoch: 5 [640/4307 (15%)]\tLoss: 0.042647 \tOP: 1.000000\tOR: 0.970297\tOF1: 0.984925\n",
      "Train Epoch: 5 [704/4307 (16%)]\tLoss: 0.032737 \tOP: 1.000000\tOR: 0.990000\tOF1: 0.994975\n",
      "Train Epoch: 5 [768/4307 (18%)]\tLoss: 0.037301 \tOP: 1.000000\tOR: 0.989691\tOF1: 0.994819\n",
      "Train Epoch: 5 [832/4307 (19%)]\tLoss: 0.048234 \tOP: 1.000000\tOR: 0.951923\tOF1: 0.975369\n",
      "Train Epoch: 5 [896/4307 (21%)]\tLoss: 0.027229 \tOP: 1.000000\tOR: 0.989899\tOF1: 0.994924\n",
      "Train Epoch: 5 [960/4307 (22%)]\tLoss: 0.031347 \tOP: 1.000000\tOR: 0.989474\tOF1: 0.994709\n",
      "Train Epoch: 5 [1024/4307 (24%)]\tLoss: 0.030645 \tOP: 1.000000\tOR: 0.990196\tOF1: 0.995074\n",
      "Train Epoch: 5 [1088/4307 (25%)]\tLoss: 0.048693 \tOP: 1.000000\tOR: 0.970588\tOF1: 0.985075\n",
      "Train Epoch: 5 [1152/4307 (26%)]\tLoss: 0.058202 \tOP: 0.979167\tOR: 0.959184\tOF1: 0.969072\n",
      "Train Epoch: 5 [1216/4307 (28%)]\tLoss: 0.044935 \tOP: 0.989247\tOR: 0.989247\tOF1: 0.989247\n",
      "Train Epoch: 5 [1280/4307 (29%)]\tLoss: 0.040646 \tOP: 1.000000\tOR: 0.969388\tOF1: 0.984456\n",
      "Train Epoch: 5 [1344/4307 (31%)]\tLoss: 0.036710 \tOP: 0.989362\tOR: 0.989362\tOF1: 0.989362\n",
      "Train Epoch: 5 [1408/4307 (32%)]\tLoss: 0.043808 \tOP: 0.989247\tOR: 0.968421\tOF1: 0.978723\n",
      "Train Epoch: 5 [1472/4307 (34%)]\tLoss: 0.055512 \tOP: 0.989583\tOR: 0.959596\tOF1: 0.974359\n",
      "Train Epoch: 5 [1536/4307 (35%)]\tLoss: 0.032346 \tOP: 1.000000\tOR: 0.990000\tOF1: 0.994975\n",
      "Train Epoch: 5 [1600/4307 (37%)]\tLoss: 0.035651 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 5 [1664/4307 (38%)]\tLoss: 0.034678 \tOP: 1.000000\tOR: 0.990385\tOF1: 0.995169\n",
      "Train Epoch: 5 [1728/4307 (40%)]\tLoss: 0.040661 \tOP: 1.000000\tOR: 0.957895\tOF1: 0.978495\n",
      "Train Epoch: 5 [1792/4307 (41%)]\tLoss: 0.038308 \tOP: 1.000000\tOR: 0.989899\tOF1: 0.994924\n",
      "Train Epoch: 5 [1856/4307 (43%)]\tLoss: 0.033389 \tOP: 1.000000\tOR: 0.980198\tOF1: 0.990000\n",
      "Train Epoch: 5 [1920/4307 (44%)]\tLoss: 0.025275 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 5 [1984/4307 (46%)]\tLoss: 0.031127 \tOP: 1.000000\tOR: 0.979592\tOF1: 0.989691\n",
      "Train Epoch: 5 [2048/4307 (47%)]\tLoss: 0.028248 \tOP: 1.000000\tOR: 0.980769\tOF1: 0.990291\n",
      "Train Epoch: 5 [2112/4307 (49%)]\tLoss: 0.029481 \tOP: 1.000000\tOR: 0.989474\tOF1: 0.994709\n",
      "Train Epoch: 5 [2176/4307 (50%)]\tLoss: 0.035606 \tOP: 1.000000\tOR: 0.969072\tOF1: 0.984293\n",
      "Train Epoch: 5 [2240/4307 (51%)]\tLoss: 0.039063 \tOP: 0.990196\tOR: 0.971154\tOF1: 0.980583\n",
      "Train Epoch: 5 [2304/4307 (53%)]\tLoss: 0.043707 \tOP: 0.990909\tOR: 0.981982\tOF1: 0.986425\n",
      "Train Epoch: 5 [2368/4307 (54%)]\tLoss: 0.034249 \tOP: 1.000000\tOR: 0.979592\tOF1: 0.989691\n",
      "Train Epoch: 5 [2432/4307 (56%)]\tLoss: 0.038388 \tOP: 1.000000\tOR: 0.980583\tOF1: 0.990196\n",
      "Train Epoch: 5 [2496/4307 (57%)]\tLoss: 0.030666 \tOP: 1.000000\tOR: 0.990741\tOF1: 0.995349\n",
      "Train Epoch: 5 [2560/4307 (59%)]\tLoss: 0.038298 \tOP: 0.990385\tOR: 0.980952\tOF1: 0.985646\n",
      "Train Epoch: 5 [2624/4307 (60%)]\tLoss: 0.031882 \tOP: 1.000000\tOR: 0.980392\tOF1: 0.990099\n",
      "Train Epoch: 5 [2688/4307 (62%)]\tLoss: 0.032076 \tOP: 1.000000\tOR: 0.959184\tOF1: 0.979167\n",
      "Train Epoch: 5 [2752/4307 (63%)]\tLoss: 0.034479 \tOP: 1.000000\tOR: 0.988764\tOF1: 0.994350\n",
      "Train Epoch: 5 [2816/4307 (65%)]\tLoss: 0.032418 \tOP: 1.000000\tOR: 0.989362\tOF1: 0.994652\n",
      "Train Epoch: 5 [2880/4307 (66%)]\tLoss: 0.042645 \tOP: 1.000000\tOR: 0.950980\tOF1: 0.974874\n",
      "Train Epoch: 5 [2944/4307 (68%)]\tLoss: 0.033930 \tOP: 1.000000\tOR: 0.943925\tOF1: 0.971154\n",
      "Train Epoch: 5 [3008/4307 (69%)]\tLoss: 0.029749 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 5 [3072/4307 (71%)]\tLoss: 0.029985 \tOP: 1.000000\tOR: 0.980583\tOF1: 0.990196\n",
      "Train Epoch: 5 [3136/4307 (72%)]\tLoss: 0.034548 \tOP: 0.989247\tOR: 0.989247\tOF1: 0.989247\n",
      "Train Epoch: 5 [3200/4307 (74%)]\tLoss: 0.055760 \tOP: 1.000000\tOR: 0.921739\tOF1: 0.959276\n",
      "Train Epoch: 5 [3264/4307 (75%)]\tLoss: 0.042967 \tOP: 1.000000\tOR: 0.962617\tOF1: 0.980952\n",
      "Train Epoch: 5 [3328/4307 (76%)]\tLoss: 0.043731 \tOP: 0.989583\tOR: 0.969388\tOF1: 0.979381\n",
      "Train Epoch: 5 [3392/4307 (78%)]\tLoss: 0.036670 \tOP: 1.000000\tOR: 0.960000\tOF1: 0.979592\n",
      "Train Epoch: 5 [3456/4307 (79%)]\tLoss: 0.036003 \tOP: 0.988889\tOR: 0.988889\tOF1: 0.988889\n",
      "Train Epoch: 5 [3520/4307 (81%)]\tLoss: 0.045167 \tOP: 1.000000\tOR: 0.952830\tOF1: 0.975845\n",
      "Train Epoch: 5 [3584/4307 (82%)]\tLoss: 0.033152 \tOP: 1.000000\tOR: 0.980198\tOF1: 0.990000\n",
      "Train Epoch: 5 [3648/4307 (84%)]\tLoss: 0.028588 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 5 [3712/4307 (85%)]\tLoss: 0.034815 \tOP: 0.990291\tOR: 0.980769\tOF1: 0.985507\n",
      "Train Epoch: 5 [3776/4307 (87%)]\tLoss: 0.037270 \tOP: 1.000000\tOR: 0.961165\tOF1: 0.980198\n",
      "Train Epoch: 5 [3840/4307 (88%)]\tLoss: 0.024574 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 5 [3904/4307 (90%)]\tLoss: 0.040961 \tOP: 1.000000\tOR: 0.961538\tOF1: 0.980392\n",
      "Train Epoch: 5 [3968/4307 (91%)]\tLoss: 0.037186 \tOP: 1.000000\tOR: 0.969388\tOF1: 0.984456\n",
      "Train Epoch: 5 [4032/4307 (93%)]\tLoss: 0.035477 \tOP: 1.000000\tOR: 0.980392\tOF1: 0.990099\n",
      "Train Epoch: 5 [4096/4307 (94%)]\tLoss: 0.033066 \tOP: 1.000000\tOR: 0.980952\tOF1: 0.990385\n",
      "Train Epoch: 5 [4160/4307 (96%)]\tLoss: 0.064374 \tOP: 0.990196\tOR: 0.943925\tOF1: 0.966507\n",
      "Train Epoch: 5 [4224/4307 (97%)]\tLoss: 0.028094 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 5 [1273/4307 (99%)]\tLoss: 0.025200 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6404 \n",
      "OP: 0.704918\n",
      "OR: 0.565789\n",
      "OF1: 0.627737\n",
      "\n",
      "Train Epoch: 6 [0/4307 (0%)]\tLoss: 0.018825 \tOP: 1.000000\tOR: 0.989899\tOF1: 0.994924\n",
      "Train Epoch: 6 [64/4307 (1%)]\tLoss: 0.028306 \tOP: 1.000000\tOR: 0.979167\tOF1: 0.989474\n",
      "Train Epoch: 6 [128/4307 (3%)]\tLoss: 0.024468 \tOP: 1.000000\tOR: 0.990291\tOF1: 0.995122\n",
      "Train Epoch: 6 [192/4307 (4%)]\tLoss: 0.019181 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [256/4307 (6%)]\tLoss: 0.022017 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [320/4307 (7%)]\tLoss: 0.019769 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [384/4307 (9%)]\tLoss: 0.024397 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [448/4307 (10%)]\tLoss: 0.034144 \tOP: 1.000000\tOR: 0.980198\tOF1: 0.990000\n",
      "Train Epoch: 6 [512/4307 (12%)]\tLoss: 0.020344 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [576/4307 (13%)]\tLoss: 0.045097 \tOP: 1.000000\tOR: 0.951923\tOF1: 0.975369\n",
      "Train Epoch: 6 [640/4307 (15%)]\tLoss: 0.029389 \tOP: 1.000000\tOR: 0.971154\tOF1: 0.985366\n",
      "Train Epoch: 6 [704/4307 (16%)]\tLoss: 0.021686 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [768/4307 (18%)]\tLoss: 0.027324 \tOP: 1.000000\tOR: 0.979381\tOF1: 0.989583\n",
      "Train Epoch: 6 [832/4307 (19%)]\tLoss: 0.021653 \tOP: 0.990000\tOR: 1.000000\tOF1: 0.994975\n",
      "Train Epoch: 6 [896/4307 (21%)]\tLoss: 0.024165 \tOP: 1.000000\tOR: 0.990476\tOF1: 0.995215\n",
      "Train Epoch: 6 [960/4307 (22%)]\tLoss: 0.014850 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [1024/4307 (24%)]\tLoss: 0.026666 \tOP: 1.000000\tOR: 0.990291\tOF1: 0.995122\n",
      "Train Epoch: 6 [1088/4307 (25%)]\tLoss: 0.030979 \tOP: 1.000000\tOR: 0.989691\tOF1: 0.994819\n",
      "Train Epoch: 6 [1152/4307 (26%)]\tLoss: 0.019114 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [1216/4307 (28%)]\tLoss: 0.016781 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [1280/4307 (29%)]\tLoss: 0.016272 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [1344/4307 (31%)]\tLoss: 0.020058 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [1408/4307 (32%)]\tLoss: 0.021412 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [1472/4307 (34%)]\tLoss: 0.028526 \tOP: 1.000000\tOR: 0.990099\tOF1: 0.995025\n",
      "Train Epoch: 6 [1536/4307 (35%)]\tLoss: 0.019440 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [1600/4307 (37%)]\tLoss: 0.018679 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [1664/4307 (38%)]\tLoss: 0.023498 \tOP: 1.000000\tOR: 0.971154\tOF1: 0.985366\n",
      "Train Epoch: 6 [1728/4307 (40%)]\tLoss: 0.025540 \tOP: 1.000000\tOR: 0.980392\tOF1: 0.990099\n",
      "Train Epoch: 6 [1792/4307 (41%)]\tLoss: 0.029919 \tOP: 1.000000\tOR: 0.981308\tOF1: 0.990566\n",
      "Train Epoch: 6 [1856/4307 (43%)]\tLoss: 0.027862 \tOP: 1.000000\tOR: 0.989130\tOF1: 0.994536\n",
      "Train Epoch: 6 [1920/4307 (44%)]\tLoss: 0.022480 \tOP: 1.000000\tOR: 0.990099\tOF1: 0.995025\n",
      "Train Epoch: 6 [1984/4307 (46%)]\tLoss: 0.019306 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [2048/4307 (47%)]\tLoss: 0.017945 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [2112/4307 (49%)]\tLoss: 0.025783 \tOP: 0.989583\tOR: 0.989583\tOF1: 0.989583\n",
      "Train Epoch: 6 [2176/4307 (50%)]\tLoss: 0.017390 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [2240/4307 (51%)]\tLoss: 0.017635 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [2304/4307 (53%)]\tLoss: 0.041962 \tOP: 0.989899\tOR: 0.970297\tOF1: 0.980000\n",
      "Train Epoch: 6 [2368/4307 (54%)]\tLoss: 0.033713 \tOP: 0.989474\tOR: 0.969072\tOF1: 0.979167\n",
      "Train Epoch: 6 [2432/4307 (56%)]\tLoss: 0.018221 \tOP: 0.989691\tOR: 1.000000\tOF1: 0.994819\n",
      "Train Epoch: 6 [2496/4307 (57%)]\tLoss: 0.030075 \tOP: 0.989362\tOR: 0.989362\tOF1: 0.989362\n",
      "Train Epoch: 6 [2560/4307 (59%)]\tLoss: 0.027984 \tOP: 0.979592\tOR: 1.000000\tOF1: 0.989691\n",
      "Train Epoch: 6 [2624/4307 (60%)]\tLoss: 0.020220 \tOP: 1.000000\tOR: 0.980769\tOF1: 0.990291\n",
      "Train Epoch: 6 [2688/4307 (62%)]\tLoss: 0.021301 \tOP: 1.000000\tOR: 0.989247\tOF1: 0.994595\n",
      "Train Epoch: 6 [2752/4307 (63%)]\tLoss: 0.026313 \tOP: 1.000000\tOR: 0.990196\tOF1: 0.995074\n",
      "Train Epoch: 6 [2816/4307 (65%)]\tLoss: 0.021186 \tOP: 1.000000\tOR: 0.979798\tOF1: 0.989796\n",
      "Train Epoch: 6 [2880/4307 (66%)]\tLoss: 0.019876 \tOP: 1.000000\tOR: 0.980198\tOF1: 0.990000\n",
      "Train Epoch: 6 [2944/4307 (68%)]\tLoss: 0.037662 \tOP: 1.000000\tOR: 0.971963\tOF1: 0.985782\n",
      "Train Epoch: 6 [3008/4307 (69%)]\tLoss: 0.031671 \tOP: 1.000000\tOR: 0.970297\tOF1: 0.984925\n",
      "Train Epoch: 6 [3072/4307 (71%)]\tLoss: 0.017501 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [3136/4307 (72%)]\tLoss: 0.016614 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [3200/4307 (74%)]\tLoss: 0.018164 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [3264/4307 (75%)]\tLoss: 0.021368 \tOP: 1.000000\tOR: 0.981481\tOF1: 0.990654\n",
      "Train Epoch: 6 [3328/4307 (76%)]\tLoss: 0.015450 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [3392/4307 (78%)]\tLoss: 0.022987 \tOP: 1.000000\tOR: 0.990196\tOF1: 0.995074\n",
      "Train Epoch: 6 [3456/4307 (79%)]\tLoss: 0.027178 \tOP: 1.000000\tOR: 0.989247\tOF1: 0.994595\n",
      "Train Epoch: 6 [3520/4307 (81%)]\tLoss: 0.017378 \tOP: 1.000000\tOR: 0.989899\tOF1: 0.994924\n",
      "Train Epoch: 6 [3584/4307 (82%)]\tLoss: 0.021529 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [3648/4307 (84%)]\tLoss: 0.020539 \tOP: 1.000000\tOR: 0.989899\tOF1: 0.994924\n",
      "Train Epoch: 6 [3712/4307 (85%)]\tLoss: 0.028575 \tOP: 1.000000\tOR: 0.972222\tOF1: 0.985915\n",
      "Train Epoch: 6 [3776/4307 (87%)]\tLoss: 0.036825 \tOP: 1.000000\tOR: 0.937500\tOF1: 0.967742\n",
      "Train Epoch: 6 [3840/4307 (88%)]\tLoss: 0.016245 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 6 [3904/4307 (90%)]\tLoss: 0.018408 \tOP: 1.000000\tOR: 0.989362\tOF1: 0.994652\n",
      "Train Epoch: 6 [3968/4307 (91%)]\tLoss: 0.017021 \tOP: 1.000000\tOR: 0.990196\tOF1: 0.995074\n",
      "Train Epoch: 6 [4032/4307 (93%)]\tLoss: 0.028114 \tOP: 1.000000\tOR: 0.991228\tOF1: 0.995595\n",
      "Train Epoch: 6 [4096/4307 (94%)]\tLoss: 0.022307 \tOP: 1.000000\tOR: 0.978723\tOF1: 0.989247\n",
      "Train Epoch: 6 [4160/4307 (96%)]\tLoss: 0.028903 \tOP: 1.000000\tOR: 0.990099\tOF1: 0.995025\n",
      "Train Epoch: 6 [4224/4307 (97%)]\tLoss: 0.018553 \tOP: 1.000000\tOR: 0.990196\tOF1: 0.995074\n",
      "Train Epoch: 6 [1273/4307 (99%)]\tLoss: 0.024938 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6535 \n",
      "OP: 0.733333\n",
      "OR: 0.578947\n",
      "OF1: 0.647059\n",
      "\n",
      "Train Epoch: 7 [0/4307 (0%)]\tLoss: 0.020032 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [64/4307 (1%)]\tLoss: 0.011547 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [128/4307 (3%)]\tLoss: 0.015343 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [192/4307 (4%)]\tLoss: 0.013476 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [256/4307 (6%)]\tLoss: 0.013102 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [320/4307 (7%)]\tLoss: 0.014007 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [384/4307 (9%)]\tLoss: 0.017463 \tOP: 1.000000\tOR: 0.990099\tOF1: 0.995025\n",
      "Train Epoch: 7 [448/4307 (10%)]\tLoss: 0.015002 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [512/4307 (12%)]\tLoss: 0.014499 \tOP: 1.000000\tOR: 0.989899\tOF1: 0.994924\n",
      "Train Epoch: 7 [576/4307 (13%)]\tLoss: 0.021079 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [640/4307 (15%)]\tLoss: 0.022841 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [704/4307 (16%)]\tLoss: 0.009453 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [768/4307 (18%)]\tLoss: 0.013361 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [832/4307 (19%)]\tLoss: 0.016440 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [896/4307 (21%)]\tLoss: 0.015880 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [960/4307 (22%)]\tLoss: 0.016301 \tOP: 1.000000\tOR: 0.990991\tOF1: 0.995475\n",
      "Train Epoch: 7 [1024/4307 (24%)]\tLoss: 0.021083 \tOP: 0.989899\tOR: 1.000000\tOF1: 0.994924\n",
      "Train Epoch: 7 [1088/4307 (25%)]\tLoss: 0.013432 \tOP: 1.000000\tOR: 0.990566\tOF1: 0.995261\n",
      "Train Epoch: 7 [1152/4307 (26%)]\tLoss: 0.015329 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1216/4307 (28%)]\tLoss: 0.014840 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1280/4307 (29%)]\tLoss: 0.012292 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1344/4307 (31%)]\tLoss: 0.011653 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1408/4307 (32%)]\tLoss: 0.017375 \tOP: 1.000000\tOR: 0.990476\tOF1: 0.995215\n",
      "Train Epoch: 7 [1472/4307 (34%)]\tLoss: 0.013052 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1536/4307 (35%)]\tLoss: 0.016259 \tOP: 1.000000\tOR: 0.990000\tOF1: 0.994975\n",
      "Train Epoch: 7 [1600/4307 (37%)]\tLoss: 0.019198 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1664/4307 (38%)]\tLoss: 0.015861 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1728/4307 (40%)]\tLoss: 0.013826 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1792/4307 (41%)]\tLoss: 0.017142 \tOP: 1.000000\tOR: 0.990385\tOF1: 0.995169\n",
      "Train Epoch: 7 [1856/4307 (43%)]\tLoss: 0.016014 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1920/4307 (44%)]\tLoss: 0.023734 \tOP: 1.000000\tOR: 0.980198\tOF1: 0.990000\n",
      "Train Epoch: 7 [1984/4307 (46%)]\tLoss: 0.016152 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2048/4307 (47%)]\tLoss: 0.016387 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2112/4307 (49%)]\tLoss: 0.009629 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2176/4307 (50%)]\tLoss: 0.011646 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2240/4307 (51%)]\tLoss: 0.011765 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2304/4307 (53%)]\tLoss: 0.012647 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2368/4307 (54%)]\tLoss: 0.011292 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2432/4307 (56%)]\tLoss: 0.012958 \tOP: 1.000000\tOR: 0.989474\tOF1: 0.994709\n",
      "Train Epoch: 7 [2496/4307 (57%)]\tLoss: 0.016814 \tOP: 1.000000\tOR: 0.989899\tOF1: 0.994924\n",
      "Train Epoch: 7 [2560/4307 (59%)]\tLoss: 0.010425 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2624/4307 (60%)]\tLoss: 0.013016 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2688/4307 (62%)]\tLoss: 0.014876 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2752/4307 (63%)]\tLoss: 0.016275 \tOP: 1.000000\tOR: 0.990099\tOF1: 0.995025\n",
      "Train Epoch: 7 [2816/4307 (65%)]\tLoss: 0.010751 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2880/4307 (66%)]\tLoss: 0.013966 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [2944/4307 (68%)]\tLoss: 0.012622 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3008/4307 (69%)]\tLoss: 0.010933 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3072/4307 (71%)]\tLoss: 0.018784 \tOP: 0.990099\tOR: 1.000000\tOF1: 0.995025\n",
      "Train Epoch: 7 [3136/4307 (72%)]\tLoss: 0.013271 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3200/4307 (74%)]\tLoss: 0.014789 \tOP: 1.000000\tOR: 0.990099\tOF1: 0.995025\n",
      "Train Epoch: 7 [3264/4307 (75%)]\tLoss: 0.009188 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3328/4307 (76%)]\tLoss: 0.009748 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3392/4307 (78%)]\tLoss: 0.015447 \tOP: 1.000000\tOR: 0.990291\tOF1: 0.995122\n",
      "Train Epoch: 7 [3456/4307 (79%)]\tLoss: 0.011713 \tOP: 1.000000\tOR: 0.990291\tOF1: 0.995122\n",
      "Train Epoch: 7 [3520/4307 (81%)]\tLoss: 0.011230 \tOP: 1.000000\tOR: 0.989691\tOF1: 0.994819\n",
      "Train Epoch: 7 [3584/4307 (82%)]\tLoss: 0.017126 \tOP: 1.000000\tOR: 0.990654\tOF1: 0.995305\n",
      "Train Epoch: 7 [3648/4307 (84%)]\tLoss: 0.008798 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3712/4307 (85%)]\tLoss: 0.011483 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3776/4307 (87%)]\tLoss: 0.007800 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3840/4307 (88%)]\tLoss: 0.009840 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3904/4307 (90%)]\tLoss: 0.012129 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [3968/4307 (91%)]\tLoss: 0.009441 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [4032/4307 (93%)]\tLoss: 0.010487 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [4096/4307 (94%)]\tLoss: 0.012502 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [4160/4307 (96%)]\tLoss: 0.012625 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [4224/4307 (97%)]\tLoss: 0.008321 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 7 [1273/4307 (99%)]\tLoss: 0.018315 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6814 \n",
      "OP: 0.711864\n",
      "OR: 0.552632\n",
      "OF1: 0.622222\n",
      "\n",
      "Train Epoch: 8 [0/4307 (0%)]\tLoss: 0.010510 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [64/4307 (1%)]\tLoss: 0.007386 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [128/4307 (3%)]\tLoss: 0.009889 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [192/4307 (4%)]\tLoss: 0.010238 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [256/4307 (6%)]\tLoss: 0.012071 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [320/4307 (7%)]\tLoss: 0.010547 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [384/4307 (9%)]\tLoss: 0.009845 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [448/4307 (10%)]\tLoss: 0.008012 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [512/4307 (12%)]\tLoss: 0.006777 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [576/4307 (13%)]\tLoss: 0.009603 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [640/4307 (15%)]\tLoss: 0.008274 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [704/4307 (16%)]\tLoss: 0.008085 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [768/4307 (18%)]\tLoss: 0.006320 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [832/4307 (19%)]\tLoss: 0.009404 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [896/4307 (21%)]\tLoss: 0.007484 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [960/4307 (22%)]\tLoss: 0.008444 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1024/4307 (24%)]\tLoss: 0.007921 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1088/4307 (25%)]\tLoss: 0.008198 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1152/4307 (26%)]\tLoss: 0.006463 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1216/4307 (28%)]\tLoss: 0.008239 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1280/4307 (29%)]\tLoss: 0.009759 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1344/4307 (31%)]\tLoss: 0.011606 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1408/4307 (32%)]\tLoss: 0.012008 \tOP: 1.000000\tOR: 0.990826\tOF1: 0.995392\n",
      "Train Epoch: 8 [1472/4307 (34%)]\tLoss: 0.007823 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1536/4307 (35%)]\tLoss: 0.007751 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1600/4307 (37%)]\tLoss: 0.006850 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1664/4307 (38%)]\tLoss: 0.006188 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1728/4307 (40%)]\tLoss: 0.007058 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1792/4307 (41%)]\tLoss: 0.007702 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1856/4307 (43%)]\tLoss: 0.008838 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1920/4307 (44%)]\tLoss: 0.008075 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1984/4307 (46%)]\tLoss: 0.006586 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2048/4307 (47%)]\tLoss: 0.006617 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2112/4307 (49%)]\tLoss: 0.007799 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2176/4307 (50%)]\tLoss: 0.007666 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2240/4307 (51%)]\tLoss: 0.010079 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2304/4307 (53%)]\tLoss: 0.010532 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2368/4307 (54%)]\tLoss: 0.007165 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2432/4307 (56%)]\tLoss: 0.011224 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2496/4307 (57%)]\tLoss: 0.009908 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2560/4307 (59%)]\tLoss: 0.006729 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2624/4307 (60%)]\tLoss: 0.009919 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2688/4307 (62%)]\tLoss: 0.009174 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2752/4307 (63%)]\tLoss: 0.021323 \tOP: 1.000000\tOR: 0.971154\tOF1: 0.985366\n",
      "Train Epoch: 8 [2816/4307 (65%)]\tLoss: 0.008205 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2880/4307 (66%)]\tLoss: 0.009340 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [2944/4307 (68%)]\tLoss: 0.006575 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3008/4307 (69%)]\tLoss: 0.010985 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3072/4307 (71%)]\tLoss: 0.009260 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3136/4307 (72%)]\tLoss: 0.007161 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3200/4307 (74%)]\tLoss: 0.009730 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3264/4307 (75%)]\tLoss: 0.008361 \tOP: 1.000000\tOR: 0.990000\tOF1: 0.994975\n",
      "Train Epoch: 8 [3328/4307 (76%)]\tLoss: 0.011896 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3392/4307 (78%)]\tLoss: 0.007217 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3456/4307 (79%)]\tLoss: 0.007013 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3520/4307 (81%)]\tLoss: 0.011113 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3584/4307 (82%)]\tLoss: 0.005882 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3648/4307 (84%)]\tLoss: 0.010220 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3712/4307 (85%)]\tLoss: 0.008569 \tOP: 1.000000\tOR: 0.990291\tOF1: 0.995122\n",
      "Train Epoch: 8 [3776/4307 (87%)]\tLoss: 0.006756 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3840/4307 (88%)]\tLoss: 0.017456 \tOP: 1.000000\tOR: 0.981481\tOF1: 0.990654\n",
      "Train Epoch: 8 [3904/4307 (90%)]\tLoss: 0.014362 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [3968/4307 (91%)]\tLoss: 0.006368 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [4032/4307 (93%)]\tLoss: 0.009196 \tOP: 1.000000\tOR: 0.989474\tOF1: 0.994709\n",
      "Train Epoch: 8 [4096/4307 (94%)]\tLoss: 0.006435 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [4160/4307 (96%)]\tLoss: 0.009608 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [4224/4307 (97%)]\tLoss: 0.013100 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 8 [1273/4307 (99%)]\tLoss: 0.022955 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.7223 \n",
      "OP: 0.671875\n",
      "OR: 0.565789\n",
      "OF1: 0.614286\n",
      "\n",
      "Train Epoch: 9 [0/4307 (0%)]\tLoss: 0.005722 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [64/4307 (1%)]\tLoss: 0.005853 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [128/4307 (3%)]\tLoss: 0.007465 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [192/4307 (4%)]\tLoss: 0.007748 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [256/4307 (6%)]\tLoss: 0.010509 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [320/4307 (7%)]\tLoss: 0.007480 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [384/4307 (9%)]\tLoss: 0.005411 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [448/4307 (10%)]\tLoss: 0.007323 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [512/4307 (12%)]\tLoss: 0.005825 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [576/4307 (13%)]\tLoss: 0.008634 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [640/4307 (15%)]\tLoss: 0.007765 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [704/4307 (16%)]\tLoss: 0.008033 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [768/4307 (18%)]\tLoss: 0.008856 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [832/4307 (19%)]\tLoss: 0.007127 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [896/4307 (21%)]\tLoss: 0.012574 \tOP: 1.000000\tOR: 0.989247\tOF1: 0.994595\n",
      "Train Epoch: 9 [960/4307 (22%)]\tLoss: 0.004676 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1024/4307 (24%)]\tLoss: 0.006124 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1088/4307 (25%)]\tLoss: 0.008434 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1152/4307 (26%)]\tLoss: 0.010816 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1216/4307 (28%)]\tLoss: 0.007471 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1280/4307 (29%)]\tLoss: 0.010224 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1344/4307 (31%)]\tLoss: 0.005920 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1408/4307 (32%)]\tLoss: 0.005923 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1472/4307 (34%)]\tLoss: 0.007518 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1536/4307 (35%)]\tLoss: 0.005546 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1600/4307 (37%)]\tLoss: 0.010033 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1664/4307 (38%)]\tLoss: 0.006918 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1728/4307 (40%)]\tLoss: 0.006070 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1792/4307 (41%)]\tLoss: 0.006564 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1856/4307 (43%)]\tLoss: 0.006440 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1920/4307 (44%)]\tLoss: 0.006183 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1984/4307 (46%)]\tLoss: 0.041659 \tOP: 0.988764\tOR: 0.988764\tOF1: 0.988764\n",
      "Train Epoch: 9 [2048/4307 (47%)]\tLoss: 0.005495 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2112/4307 (49%)]\tLoss: 0.009820 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2176/4307 (50%)]\tLoss: 0.006059 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2240/4307 (51%)]\tLoss: 0.009257 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2304/4307 (53%)]\tLoss: 0.005100 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2368/4307 (54%)]\tLoss: 0.005591 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2432/4307 (56%)]\tLoss: 0.006867 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2496/4307 (57%)]\tLoss: 0.007223 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2560/4307 (59%)]\tLoss: 0.005548 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2624/4307 (60%)]\tLoss: 0.006848 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2688/4307 (62%)]\tLoss: 0.010659 \tOP: 1.000000\tOR: 0.990476\tOF1: 0.995215\n",
      "Train Epoch: 9 [2752/4307 (63%)]\tLoss: 0.012097 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2816/4307 (65%)]\tLoss: 0.005717 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2880/4307 (66%)]\tLoss: 0.007040 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [2944/4307 (68%)]\tLoss: 0.004950 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3008/4307 (69%)]\tLoss: 0.005048 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3072/4307 (71%)]\tLoss: 0.005345 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3136/4307 (72%)]\tLoss: 0.007969 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3200/4307 (74%)]\tLoss: 0.006323 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3264/4307 (75%)]\tLoss: 0.007062 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3328/4307 (76%)]\tLoss: 0.005583 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3392/4307 (78%)]\tLoss: 0.026348 \tOP: 0.990291\tOR: 0.980769\tOF1: 0.985507\n",
      "Train Epoch: 9 [3456/4307 (79%)]\tLoss: 0.005739 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3520/4307 (81%)]\tLoss: 0.004968 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3584/4307 (82%)]\tLoss: 0.008850 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3648/4307 (84%)]\tLoss: 0.007303 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3712/4307 (85%)]\tLoss: 0.004743 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3776/4307 (87%)]\tLoss: 0.010111 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3840/4307 (88%)]\tLoss: 0.008252 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3904/4307 (90%)]\tLoss: 0.008842 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [3968/4307 (91%)]\tLoss: 0.008463 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [4032/4307 (93%)]\tLoss: 0.009048 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [4096/4307 (94%)]\tLoss: 0.009046 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [4160/4307 (96%)]\tLoss: 0.007941 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [4224/4307 (97%)]\tLoss: 0.007480 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "Train Epoch: 9 [1273/4307 (99%)]\tLoss: 0.009834 \tOP: 1.000000\tOR: 1.000000\tOF1: 1.000000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.7368 \n",
      "OP: 0.698413\n",
      "OR: 0.578947\n",
      "OF1: 0.633094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model2.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model2(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        loss_lst_train2.append(loss.data.item())\n",
    "        OF1_lst_train2.append(OF1)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    OP_final = 0\n",
    "    OR_final = 0\n",
    "    OF1_final = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model2(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        OP_final += OP\n",
    "        OR_final += OR\n",
    "        OF1_final += OF1\n",
    "        \n",
    "    loss_lst_test2.append(test_loss.data.item()/i)\n",
    "    OF1_lst_test2.append(OF1_final/i)\n",
    "        \n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "loss_lst_train2 = []\n",
    "OF1_lst_train2 = []\n",
    "\n",
    "loss_lst_test2 = []\n",
    "OF1_lst_test2 = []    \n",
    "    \n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "class ResNet152(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet152, self).__init__()\n",
    "        self.resnet = models.resnet152(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(2048, len(classes))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model3 = ResNet152()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4307 (0%)]\tLoss: 0.750986 \tOP: 0.402062\tOR: 0.378641\tOF1: 0.390000\n",
      "Train Epoch: 0 [64/4307 (1%)]\tLoss: 0.769544 \tOP: 0.301205\tOR: 0.260417\tOF1: 0.279330\n",
      "Train Epoch: 0 [128/4307 (3%)]\tLoss: 0.712539 \tOP: 0.426230\tOR: 0.247619\tOF1: 0.313253\n",
      "Train Epoch: 0 [192/4307 (4%)]\tLoss: 0.684681 \tOP: 0.488372\tOR: 0.198113\tOF1: 0.281879\n",
      "Train Epoch: 0 [256/4307 (6%)]\tLoss: 0.636422 \tOP: 0.730769\tOR: 0.204301\tOF1: 0.319328\n",
      "Train Epoch: 0 [320/4307 (7%)]\tLoss: 0.648396 \tOP: 0.586207\tOR: 0.175258\tOF1: 0.269841\n",
      "Train Epoch: 0 [384/4307 (9%)]\tLoss: 0.629176 \tOP: 0.666667\tOR: 0.142857\tOF1: 0.235294\n",
      "Train Epoch: 0 [448/4307 (10%)]\tLoss: 0.631451 \tOP: 0.357143\tOR: 0.049020\tOF1: 0.086207\n",
      "Train Epoch: 0 [512/4307 (12%)]\tLoss: 0.621434 \tOP: 0.727273\tOR: 0.155340\tOF1: 0.256000\n",
      "Train Epoch: 0 [576/4307 (13%)]\tLoss: 0.625002 \tOP: 0.550000\tOR: 0.110000\tOF1: 0.183333\n",
      "Train Epoch: 0 [640/4307 (15%)]\tLoss: 0.638334 \tOP: 0.733333\tOR: 0.108911\tOF1: 0.189655\n",
      "Train Epoch: 0 [704/4307 (16%)]\tLoss: 0.596029 \tOP: 0.625000\tOR: 0.102041\tOF1: 0.175439\n",
      "Train Epoch: 0 [768/4307 (18%)]\tLoss: 0.602807 \tOP: 0.619048\tOR: 0.135417\tOF1: 0.222222\n",
      "Train Epoch: 0 [832/4307 (19%)]\tLoss: 0.548704 \tOP: 0.619048\tOR: 0.151163\tOF1: 0.242991\n",
      "Train Epoch: 0 [896/4307 (21%)]\tLoss: 0.611157 \tOP: 0.615385\tOR: 0.158416\tOF1: 0.251969\n",
      "Train Epoch: 0 [960/4307 (22%)]\tLoss: 0.627505 \tOP: 0.608696\tOR: 0.128440\tOF1: 0.212121\n",
      "Train Epoch: 0 [1024/4307 (24%)]\tLoss: 0.599782 \tOP: 0.681818\tOR: 0.148515\tOF1: 0.243902\n",
      "Train Epoch: 0 [1088/4307 (25%)]\tLoss: 0.588308 \tOP: 0.592593\tOR: 0.161616\tOF1: 0.253968\n",
      "Train Epoch: 0 [1152/4307 (26%)]\tLoss: 0.606118 \tOP: 0.619048\tOR: 0.125000\tOF1: 0.208000\n",
      "Train Epoch: 0 [1216/4307 (28%)]\tLoss: 0.607251 \tOP: 0.727273\tOR: 0.170213\tOF1: 0.275862\n",
      "Train Epoch: 0 [1280/4307 (29%)]\tLoss: 0.631588 \tOP: 0.565217\tOR: 0.131313\tOF1: 0.213115\n",
      "Train Epoch: 0 [1344/4307 (31%)]\tLoss: 0.564064 \tOP: 0.703704\tOR: 0.184466\tOF1: 0.292308\n",
      "Train Epoch: 0 [1408/4307 (32%)]\tLoss: 0.626498 \tOP: 0.709677\tOR: 0.217822\tOF1: 0.333333\n",
      "Train Epoch: 0 [1472/4307 (34%)]\tLoss: 0.623103 \tOP: 0.478261\tOR: 0.111111\tOF1: 0.180328\n",
      "Train Epoch: 0 [1536/4307 (35%)]\tLoss: 0.594551 \tOP: 0.729730\tOR: 0.267327\tOF1: 0.391304\n",
      "Train Epoch: 0 [1600/4307 (37%)]\tLoss: 0.601677 \tOP: 0.612903\tOR: 0.191919\tOF1: 0.292308\n",
      "Train Epoch: 0 [1664/4307 (38%)]\tLoss: 0.593270 \tOP: 0.733333\tOR: 0.231579\tOF1: 0.352000\n",
      "Train Epoch: 0 [1728/4307 (40%)]\tLoss: 0.581939 \tOP: 0.677419\tOR: 0.221053\tOF1: 0.333333\n",
      "Train Epoch: 0 [1792/4307 (41%)]\tLoss: 0.580168 \tOP: 0.800000\tOR: 0.269231\tOF1: 0.402878\n",
      "Train Epoch: 0 [1856/4307 (43%)]\tLoss: 0.588738 \tOP: 0.600000\tOR: 0.147059\tOF1: 0.236220\n",
      "Train Epoch: 0 [1920/4307 (44%)]\tLoss: 0.649725 \tOP: 0.774194\tOR: 0.206897\tOF1: 0.326531\n",
      "Train Epoch: 0 [1984/4307 (46%)]\tLoss: 0.603014 \tOP: 0.709677\tOR: 0.231579\tOF1: 0.349206\n",
      "Train Epoch: 0 [2048/4307 (47%)]\tLoss: 0.616255 \tOP: 0.478261\tOR: 0.110000\tOF1: 0.178862\n",
      "Train Epoch: 0 [2112/4307 (49%)]\tLoss: 0.565307 \tOP: 0.625000\tOR: 0.151515\tOF1: 0.243902\n",
      "Train Epoch: 0 [2176/4307 (50%)]\tLoss: 0.608304 \tOP: 0.607143\tOR: 0.177083\tOF1: 0.274194\n",
      "Train Epoch: 0 [2240/4307 (51%)]\tLoss: 0.642075 \tOP: 0.566667\tOR: 0.170000\tOF1: 0.261538\n",
      "Train Epoch: 0 [2304/4307 (53%)]\tLoss: 0.611528 \tOP: 0.730769\tOR: 0.197917\tOF1: 0.311475\n",
      "Train Epoch: 0 [2368/4307 (54%)]\tLoss: 0.600406 \tOP: 0.607143\tOR: 0.182796\tOF1: 0.280992\n",
      "Train Epoch: 0 [2432/4307 (56%)]\tLoss: 0.600099 \tOP: 0.642857\tOR: 0.181818\tOF1: 0.283465\n",
      "Train Epoch: 0 [2496/4307 (57%)]\tLoss: 0.604887 \tOP: 0.531250\tOR: 0.163462\tOF1: 0.250000\n",
      "Train Epoch: 0 [2560/4307 (59%)]\tLoss: 0.607178 \tOP: 0.760000\tOR: 0.184466\tOF1: 0.296875\n",
      "Train Epoch: 0 [2624/4307 (60%)]\tLoss: 0.616792 \tOP: 0.700000\tOR: 0.200000\tOF1: 0.311111\n",
      "Train Epoch: 0 [2688/4307 (62%)]\tLoss: 0.569095 \tOP: 0.692308\tOR: 0.187500\tOF1: 0.295082\n",
      "Train Epoch: 0 [2752/4307 (63%)]\tLoss: 0.633951 \tOP: 0.695652\tOR: 0.156863\tOF1: 0.256000\n",
      "Train Epoch: 0 [2816/4307 (65%)]\tLoss: 0.554619 \tOP: 0.653846\tOR: 0.195402\tOF1: 0.300885\n",
      "Train Epoch: 0 [2880/4307 (66%)]\tLoss: 0.606959 \tOP: 0.500000\tOR: 0.128713\tOF1: 0.204724\n",
      "Train Epoch: 0 [2944/4307 (68%)]\tLoss: 0.617554 \tOP: 0.655172\tOR: 0.204301\tOF1: 0.311475\n",
      "Train Epoch: 0 [3008/4307 (69%)]\tLoss: 0.582861 \tOP: 0.761905\tOR: 0.152381\tOF1: 0.253968\n",
      "Train Epoch: 0 [3072/4307 (71%)]\tLoss: 0.584159 \tOP: 0.550000\tOR: 0.111111\tOF1: 0.184874\n",
      "Train Epoch: 0 [3136/4307 (72%)]\tLoss: 0.573993 \tOP: 0.806452\tOR: 0.245098\tOF1: 0.375940\n",
      "Train Epoch: 0 [3200/4307 (74%)]\tLoss: 0.580178 \tOP: 0.826087\tOR: 0.190000\tOF1: 0.308943\n",
      "Train Epoch: 0 [3264/4307 (75%)]\tLoss: 0.620743 \tOP: 0.714286\tOR: 0.145631\tOF1: 0.241935\n",
      "Train Epoch: 0 [3328/4307 (76%)]\tLoss: 0.556928 \tOP: 0.758621\tOR: 0.215686\tOF1: 0.335878\n",
      "Train Epoch: 0 [3392/4307 (78%)]\tLoss: 0.619030 \tOP: 0.600000\tOR: 0.156250\tOF1: 0.247934\n",
      "Train Epoch: 0 [3456/4307 (79%)]\tLoss: 0.587927 \tOP: 0.750000\tOR: 0.192661\tOF1: 0.306569\n",
      "Train Epoch: 0 [3520/4307 (81%)]\tLoss: 0.618471 \tOP: 0.705882\tOR: 0.240000\tOF1: 0.358209\n",
      "Train Epoch: 0 [3584/4307 (82%)]\tLoss: 0.581066 \tOP: 0.633333\tOR: 0.182692\tOF1: 0.283582\n",
      "Train Epoch: 0 [3648/4307 (84%)]\tLoss: 0.573308 \tOP: 0.642857\tOR: 0.176471\tOF1: 0.276923\n",
      "Train Epoch: 0 [3712/4307 (85%)]\tLoss: 0.545501 \tOP: 0.657143\tOR: 0.250000\tOF1: 0.362205\n",
      "Train Epoch: 0 [3776/4307 (87%)]\tLoss: 0.581557 \tOP: 0.733333\tOR: 0.226804\tOF1: 0.346457\n",
      "Train Epoch: 0 [3840/4307 (88%)]\tLoss: 0.603901 \tOP: 0.750000\tOR: 0.210000\tOF1: 0.328125\n",
      "Train Epoch: 0 [3904/4307 (90%)]\tLoss: 0.601277 \tOP: 0.612903\tOR: 0.197917\tOF1: 0.299213\n",
      "Train Epoch: 0 [3968/4307 (91%)]\tLoss: 0.626481 \tOP: 0.621622\tOR: 0.232323\tOF1: 0.338235\n",
      "Train Epoch: 0 [4032/4307 (93%)]\tLoss: 0.565340 \tOP: 0.677419\tOR: 0.212121\tOF1: 0.323077\n",
      "Train Epoch: 0 [4096/4307 (94%)]\tLoss: 0.591438 \tOP: 0.739130\tOR: 0.160377\tOF1: 0.263566\n",
      "Train Epoch: 0 [4160/4307 (96%)]\tLoss: 0.573656 \tOP: 0.653846\tOR: 0.171717\tOF1: 0.272000\n",
      "Train Epoch: 0 [4224/4307 (97%)]\tLoss: 0.581318 \tOP: 0.758621\tOR: 0.205607\tOF1: 0.323529\n",
      "Train Epoch: 0 [1273/4307 (99%)]\tLoss: 0.555472 \tOP: 0.555556\tOR: 0.192308\tOF1: 0.285714\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-22a4990e0b87>:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5218 \n",
      "OP: 0.750149\n",
      "OR: 0.266637\n",
      "OF1: 0.392530\n",
      "\n",
      "Train Epoch: 1 [0/4307 (0%)]\tLoss: 0.564273 \tOP: 0.760000\tOR: 0.182692\tOF1: 0.294574\n",
      "Train Epoch: 1 [64/4307 (1%)]\tLoss: 0.556396 \tOP: 0.736842\tOR: 0.132075\tOF1: 0.224000\n",
      "Train Epoch: 1 [128/4307 (3%)]\tLoss: 0.575893 \tOP: 0.700000\tOR: 0.150538\tOF1: 0.247788\n",
      "Train Epoch: 1 [192/4307 (4%)]\tLoss: 0.553828 \tOP: 0.777778\tOR: 0.203883\tOF1: 0.323077\n",
      "Train Epoch: 1 [256/4307 (6%)]\tLoss: 0.554175 \tOP: 0.807692\tOR: 0.205882\tOF1: 0.328125\n",
      "Train Epoch: 1 [320/4307 (7%)]\tLoss: 0.540455 \tOP: 0.733333\tOR: 0.229167\tOF1: 0.349206\n",
      "Train Epoch: 1 [384/4307 (9%)]\tLoss: 0.552138 \tOP: 0.794118\tOR: 0.272727\tOF1: 0.406015\n",
      "Train Epoch: 1 [448/4307 (10%)]\tLoss: 0.566291 \tOP: 0.709677\tOR: 0.217822\tOF1: 0.333333\n",
      "Train Epoch: 1 [512/4307 (12%)]\tLoss: 0.547407 \tOP: 0.916667\tOR: 0.222222\tOF1: 0.357724\n",
      "Train Epoch: 1 [576/4307 (13%)]\tLoss: 0.554361 \tOP: 0.846154\tOR: 0.220000\tOF1: 0.349206\n",
      "Train Epoch: 1 [640/4307 (15%)]\tLoss: 0.548095 \tOP: 0.800000\tOR: 0.269663\tOF1: 0.403361\n",
      "Train Epoch: 1 [704/4307 (16%)]\tLoss: 0.546646 \tOP: 0.888889\tOR: 0.250000\tOF1: 0.390244\n",
      "Train Epoch: 1 [768/4307 (18%)]\tLoss: 0.519343 \tOP: 0.868421\tOR: 0.343750\tOF1: 0.492537\n",
      "Train Epoch: 1 [832/4307 (19%)]\tLoss: 0.550052 \tOP: 0.766667\tOR: 0.216981\tOF1: 0.338235\n",
      "Train Epoch: 1 [896/4307 (21%)]\tLoss: 0.527318 \tOP: 0.896552\tOR: 0.270833\tOF1: 0.416000\n",
      "Train Epoch: 1 [960/4307 (22%)]\tLoss: 0.553907 \tOP: 0.870968\tOR: 0.252336\tOF1: 0.391304\n",
      "Train Epoch: 1 [1024/4307 (24%)]\tLoss: 0.552867 \tOP: 0.787879\tOR: 0.279570\tOF1: 0.412698\n",
      "Train Epoch: 1 [1088/4307 (25%)]\tLoss: 0.515494 \tOP: 0.868421\tOR: 0.330000\tOF1: 0.478261\n",
      "Train Epoch: 1 [1152/4307 (26%)]\tLoss: 0.589797 \tOP: 0.648649\tOR: 0.247423\tOF1: 0.358209\n",
      "Train Epoch: 1 [1216/4307 (28%)]\tLoss: 0.578197 \tOP: 0.810811\tOR: 0.303030\tOF1: 0.441176\n",
      "Train Epoch: 1 [1280/4307 (29%)]\tLoss: 0.541738 \tOP: 0.815789\tOR: 0.306931\tOF1: 0.446043\n",
      "Train Epoch: 1 [1344/4307 (31%)]\tLoss: 0.561082 \tOP: 0.705882\tOR: 0.235294\tOF1: 0.352941\n",
      "Train Epoch: 1 [1408/4307 (32%)]\tLoss: 0.541342 \tOP: 0.809524\tOR: 0.163462\tOF1: 0.272000\n",
      "Train Epoch: 1 [1472/4307 (34%)]\tLoss: 0.546551 \tOP: 0.800000\tOR: 0.307692\tOF1: 0.444444\n",
      "Train Epoch: 1 [1536/4307 (35%)]\tLoss: 0.538500 \tOP: 0.704545\tOR: 0.306931\tOF1: 0.427586\n",
      "Train Epoch: 1 [1600/4307 (37%)]\tLoss: 0.540341 \tOP: 0.842105\tOR: 0.329897\tOF1: 0.474074\n",
      "Train Epoch: 1 [1664/4307 (38%)]\tLoss: 0.569661 \tOP: 0.857143\tOR: 0.230769\tOF1: 0.363636\n",
      "Train Epoch: 1 [1728/4307 (40%)]\tLoss: 0.549042 \tOP: 0.800000\tOR: 0.235294\tOF1: 0.363636\n",
      "Train Epoch: 1 [1792/4307 (41%)]\tLoss: 0.543773 \tOP: 0.729730\tOR: 0.287234\tOF1: 0.412214\n",
      "Train Epoch: 1 [1856/4307 (43%)]\tLoss: 0.557247 \tOP: 0.833333\tOR: 0.222222\tOF1: 0.350877\n",
      "Train Epoch: 1 [1920/4307 (44%)]\tLoss: 0.577649 \tOP: 0.884615\tOR: 0.239583\tOF1: 0.377049\n",
      "Train Epoch: 1 [1984/4307 (46%)]\tLoss: 0.557555 \tOP: 0.804878\tOR: 0.330000\tOF1: 0.468085\n",
      "Train Epoch: 1 [2048/4307 (47%)]\tLoss: 0.557326 \tOP: 0.791667\tOR: 0.213483\tOF1: 0.336283\n",
      "Train Epoch: 1 [2112/4307 (49%)]\tLoss: 0.543859 \tOP: 0.866667\tOR: 0.375000\tOF1: 0.523490\n",
      "Train Epoch: 1 [2176/4307 (50%)]\tLoss: 0.572646 \tOP: 0.771429\tOR: 0.259615\tOF1: 0.388489\n",
      "Train Epoch: 1 [2240/4307 (51%)]\tLoss: 0.565097 \tOP: 0.750000\tOR: 0.225806\tOF1: 0.347107\n",
      "Train Epoch: 1 [2304/4307 (53%)]\tLoss: 0.522444 \tOP: 0.764706\tOR: 0.279570\tOF1: 0.409449\n",
      "Train Epoch: 1 [2368/4307 (54%)]\tLoss: 0.512252 \tOP: 0.925926\tOR: 0.250000\tOF1: 0.393701\n",
      "Train Epoch: 1 [2432/4307 (56%)]\tLoss: 0.553804 \tOP: 0.965517\tOR: 0.271845\tOF1: 0.424242\n",
      "Train Epoch: 1 [2496/4307 (57%)]\tLoss: 0.530301 \tOP: 0.781250\tOR: 0.252525\tOF1: 0.381679\n",
      "Train Epoch: 1 [2560/4307 (59%)]\tLoss: 0.518434 \tOP: 0.875000\tOR: 0.266667\tOF1: 0.408759\n",
      "Train Epoch: 1 [2624/4307 (60%)]\tLoss: 0.570689 \tOP: 0.653846\tOR: 0.171717\tOF1: 0.272000\n",
      "Train Epoch: 1 [2688/4307 (62%)]\tLoss: 0.560844 \tOP: 0.892857\tOR: 0.231481\tOF1: 0.367647\n",
      "Train Epoch: 1 [2752/4307 (63%)]\tLoss: 0.529118 \tOP: 0.862069\tOR: 0.231481\tOF1: 0.364964\n",
      "Train Epoch: 1 [2816/4307 (65%)]\tLoss: 0.551752 \tOP: 0.850000\tOR: 0.323810\tOF1: 0.468966\n",
      "Train Epoch: 1 [2880/4307 (66%)]\tLoss: 0.555791 \tOP: 0.742857\tOR: 0.242991\tOF1: 0.366197\n",
      "Train Epoch: 1 [2944/4307 (68%)]\tLoss: 0.563500 \tOP: 0.764706\tOR: 0.236364\tOF1: 0.361111\n",
      "Train Epoch: 1 [3008/4307 (69%)]\tLoss: 0.541316 \tOP: 0.718750\tOR: 0.234694\tOF1: 0.353846\n",
      "Train Epoch: 1 [3072/4307 (71%)]\tLoss: 0.559250 \tOP: 0.838710\tOR: 0.254902\tOF1: 0.390977\n",
      "Train Epoch: 1 [3136/4307 (72%)]\tLoss: 0.547790 \tOP: 0.777778\tOR: 0.225806\tOF1: 0.350000\n",
      "Train Epoch: 1 [3200/4307 (74%)]\tLoss: 0.577771 \tOP: 0.696970\tOR: 0.270588\tOF1: 0.389831\n",
      "Train Epoch: 1 [3264/4307 (75%)]\tLoss: 0.522952 \tOP: 0.906250\tOR: 0.258929\tOF1: 0.402778\n",
      "Train Epoch: 1 [3328/4307 (76%)]\tLoss: 0.558120 \tOP: 0.892857\tOR: 0.255102\tOF1: 0.396825\n",
      "Train Epoch: 1 [3392/4307 (78%)]\tLoss: 0.532456 \tOP: 0.947368\tOR: 0.363636\tOF1: 0.525547\n",
      "Train Epoch: 1 [3456/4307 (79%)]\tLoss: 0.532254 \tOP: 0.846154\tOR: 0.200000\tOF1: 0.323529\n",
      "Train Epoch: 1 [3520/4307 (81%)]\tLoss: 0.574152 \tOP: 0.866667\tOR: 0.247619\tOF1: 0.385185\n",
      "Train Epoch: 1 [3584/4307 (82%)]\tLoss: 0.529839 \tOP: 0.733333\tOR: 0.239130\tOF1: 0.360656\n",
      "Train Epoch: 1 [3648/4307 (84%)]\tLoss: 0.569394 \tOP: 0.777778\tOR: 0.282828\tOF1: 0.414815\n",
      "Train Epoch: 1 [3712/4307 (85%)]\tLoss: 0.541825 \tOP: 0.680000\tOR: 0.191011\tOF1: 0.298246\n",
      "Train Epoch: 1 [3776/4307 (87%)]\tLoss: 0.543995 \tOP: 0.766667\tOR: 0.234694\tOF1: 0.359375\n",
      "Train Epoch: 1 [3840/4307 (88%)]\tLoss: 0.512185 \tOP: 0.741935\tOR: 0.216981\tOF1: 0.335766\n",
      "Train Epoch: 1 [3904/4307 (90%)]\tLoss: 0.524165 \tOP: 0.851852\tOR: 0.230000\tOF1: 0.362205\n",
      "Train Epoch: 1 [3968/4307 (91%)]\tLoss: 0.546825 \tOP: 0.736842\tOR: 0.282828\tOF1: 0.408759\n",
      "Train Epoch: 1 [4032/4307 (93%)]\tLoss: 0.567040 \tOP: 0.750000\tOR: 0.281250\tOF1: 0.409091\n",
      "Train Epoch: 1 [4096/4307 (94%)]\tLoss: 0.562752 \tOP: 0.787879\tOR: 0.260000\tOF1: 0.390977\n",
      "Train Epoch: 1 [4160/4307 (96%)]\tLoss: 0.571393 \tOP: 0.888889\tOR: 0.226415\tOF1: 0.360902\n",
      "Train Epoch: 1 [4224/4307 (97%)]\tLoss: 0.503983 \tOP: 0.882353\tOR: 0.309278\tOF1: 0.458015\n",
      "Train Epoch: 1 [1273/4307 (99%)]\tLoss: 0.534999 \tOP: 0.666667\tOR: 0.400000\tOF1: 0.500000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5173 \n",
      "OP: 0.713472\n",
      "OR: 0.398632\n",
      "OF1: 0.510682\n",
      "\n",
      "Train Epoch: 2 [0/4307 (0%)]\tLoss: 0.502650 \tOP: 0.970588\tOR: 0.326733\tOF1: 0.488889\n",
      "Train Epoch: 2 [64/4307 (1%)]\tLoss: 0.487354 \tOP: 0.968750\tOR: 0.303922\tOF1: 0.462687\n",
      "Train Epoch: 2 [128/4307 (3%)]\tLoss: 0.489577 \tOP: 0.869565\tOR: 0.219780\tOF1: 0.350877\n",
      "Train Epoch: 2 [192/4307 (4%)]\tLoss: 0.459044 \tOP: 0.968750\tOR: 0.292453\tOF1: 0.449275\n",
      "Train Epoch: 2 [256/4307 (6%)]\tLoss: 0.515433 \tOP: 0.903226\tOR: 0.280000\tOF1: 0.427481\n",
      "Train Epoch: 2 [320/4307 (7%)]\tLoss: 0.500707 \tOP: 0.857143\tOR: 0.315789\tOF1: 0.461538\n",
      "Train Epoch: 2 [384/4307 (9%)]\tLoss: 0.488309 \tOP: 1.000000\tOR: 0.294118\tOF1: 0.454545\n",
      "Train Epoch: 2 [448/4307 (10%)]\tLoss: 0.518739 \tOP: 0.931034\tOR: 0.257143\tOF1: 0.402985\n",
      "Train Epoch: 2 [512/4307 (12%)]\tLoss: 0.479743 \tOP: 0.818182\tOR: 0.363636\tOF1: 0.503497\n",
      "Train Epoch: 2 [576/4307 (13%)]\tLoss: 0.524283 \tOP: 0.842105\tOR: 0.333333\tOF1: 0.477612\n",
      "Train Epoch: 2 [640/4307 (15%)]\tLoss: 0.559234 \tOP: 0.805556\tOR: 0.292929\tOF1: 0.429630\n",
      "Train Epoch: 2 [704/4307 (16%)]\tLoss: 0.499432 \tOP: 0.818182\tOR: 0.267327\tOF1: 0.402985\n",
      "Train Epoch: 2 [768/4307 (18%)]\tLoss: 0.495911 \tOP: 0.900000\tOR: 0.281250\tOF1: 0.428571\n",
      "Train Epoch: 2 [832/4307 (19%)]\tLoss: 0.517663 \tOP: 0.815789\tOR: 0.310000\tOF1: 0.449275\n",
      "Train Epoch: 2 [896/4307 (21%)]\tLoss: 0.548810 \tOP: 0.800000\tOR: 0.269231\tOF1: 0.402878\n",
      "Train Epoch: 2 [960/4307 (22%)]\tLoss: 0.507083 \tOP: 0.882353\tOR: 0.294118\tOF1: 0.441176\n",
      "Train Epoch: 2 [1024/4307 (24%)]\tLoss: 0.504385 \tOP: 0.793103\tOR: 0.239583\tOF1: 0.368000\n",
      "Train Epoch: 2 [1088/4307 (25%)]\tLoss: 0.523719 \tOP: 0.970588\tOR: 0.294643\tOF1: 0.452055\n",
      "Train Epoch: 2 [1152/4307 (26%)]\tLoss: 0.563715 \tOP: 0.789474\tOR: 0.312500\tOF1: 0.447761\n",
      "Train Epoch: 2 [1216/4307 (28%)]\tLoss: 0.500742 \tOP: 0.921053\tOR: 0.346535\tOF1: 0.503597\n",
      "Train Epoch: 2 [1280/4307 (29%)]\tLoss: 0.496145 \tOP: 0.973684\tOR: 0.359223\tOF1: 0.524823\n",
      "Train Epoch: 2 [1344/4307 (31%)]\tLoss: 0.502378 \tOP: 0.843750\tOR: 0.287234\tOF1: 0.428571\n",
      "Train Epoch: 2 [1408/4307 (32%)]\tLoss: 0.487365 \tOP: 0.833333\tOR: 0.300000\tOF1: 0.441176\n",
      "Train Epoch: 2 [1472/4307 (34%)]\tLoss: 0.556243 \tOP: 0.800000\tOR: 0.252252\tOF1: 0.383562\n",
      "Train Epoch: 2 [1536/4307 (35%)]\tLoss: 0.489352 \tOP: 0.864865\tOR: 0.340426\tOF1: 0.488550\n",
      "Train Epoch: 2 [1600/4307 (37%)]\tLoss: 0.505961 \tOP: 0.875000\tOR: 0.360825\tOF1: 0.510949\n",
      "Train Epoch: 2 [1664/4307 (38%)]\tLoss: 0.497860 \tOP: 0.903226\tOR: 0.294737\tOF1: 0.444444\n",
      "Train Epoch: 2 [1728/4307 (40%)]\tLoss: 0.519002 \tOP: 0.822222\tOR: 0.359223\tOF1: 0.500000\n",
      "Train Epoch: 2 [1792/4307 (41%)]\tLoss: 0.452570 \tOP: 0.945946\tOR: 0.339806\tOF1: 0.500000\n",
      "Train Epoch: 2 [1856/4307 (43%)]\tLoss: 0.505050 \tOP: 0.853659\tOR: 0.372340\tOF1: 0.518519\n",
      "Train Epoch: 2 [1920/4307 (44%)]\tLoss: 0.512853 \tOP: 0.892857\tOR: 0.238095\tOF1: 0.375940\n",
      "Train Epoch: 2 [1984/4307 (46%)]\tLoss: 0.540086 \tOP: 0.823529\tOR: 0.304348\tOF1: 0.444444\n",
      "Train Epoch: 2 [2048/4307 (47%)]\tLoss: 0.476286 \tOP: 0.885714\tOR: 0.326316\tOF1: 0.476923\n",
      "Train Epoch: 2 [2112/4307 (49%)]\tLoss: 0.523955 \tOP: 0.888889\tOR: 0.285714\tOF1: 0.432432\n",
      "Train Epoch: 2 [2176/4307 (50%)]\tLoss: 0.472464 \tOP: 0.925926\tOR: 0.227273\tOF1: 0.364964\n",
      "Train Epoch: 2 [2240/4307 (51%)]\tLoss: 0.511818 \tOP: 0.906977\tOR: 0.397959\tOF1: 0.553191\n",
      "Train Epoch: 2 [2304/4307 (53%)]\tLoss: 0.524089 \tOP: 0.897436\tOR: 0.343137\tOF1: 0.496454\n",
      "Train Epoch: 2 [2368/4307 (54%)]\tLoss: 0.480308 \tOP: 0.972222\tOR: 0.364583\tOF1: 0.530303\n",
      "Train Epoch: 2 [2432/4307 (56%)]\tLoss: 0.537423 \tOP: 0.810811\tOR: 0.300000\tOF1: 0.437956\n",
      "Train Epoch: 2 [2496/4307 (57%)]\tLoss: 0.489509 \tOP: 0.930233\tOR: 0.396040\tOF1: 0.555556\n",
      "Train Epoch: 2 [2560/4307 (59%)]\tLoss: 0.451978 \tOP: 0.947368\tOR: 0.371134\tOF1: 0.533333\n",
      "Train Epoch: 2 [2624/4307 (60%)]\tLoss: 0.563481 \tOP: 0.837209\tOR: 0.346154\tOF1: 0.489796\n",
      "Train Epoch: 2 [2688/4307 (62%)]\tLoss: 0.459599 \tOP: 0.897436\tOR: 0.376344\tOF1: 0.530303\n",
      "Train Epoch: 2 [2752/4307 (63%)]\tLoss: 0.503260 \tOP: 0.840909\tOR: 0.397849\tOF1: 0.540146\n",
      "Train Epoch: 2 [2816/4307 (65%)]\tLoss: 0.458654 \tOP: 0.897959\tOR: 0.440000\tOF1: 0.590604\n",
      "Train Epoch: 2 [2880/4307 (66%)]\tLoss: 0.496222 \tOP: 0.853659\tOR: 0.350000\tOF1: 0.496454\n",
      "Train Epoch: 2 [2944/4307 (68%)]\tLoss: 0.516907 \tOP: 0.777778\tOR: 0.266667\tOF1: 0.397163\n",
      "Train Epoch: 2 [3008/4307 (69%)]\tLoss: 0.497673 \tOP: 0.888889\tOR: 0.333333\tOF1: 0.484848\n",
      "Train Epoch: 2 [3072/4307 (71%)]\tLoss: 0.474212 \tOP: 0.833333\tOR: 0.285714\tOF1: 0.425532\n",
      "Train Epoch: 2 [3136/4307 (72%)]\tLoss: 0.457133 \tOP: 0.886364\tOR: 0.393939\tOF1: 0.545455\n",
      "Train Epoch: 2 [3200/4307 (74%)]\tLoss: 0.515074 \tOP: 0.827586\tOR: 0.263736\tOF1: 0.400000\n",
      "Train Epoch: 2 [3264/4307 (75%)]\tLoss: 0.525018 \tOP: 0.772727\tOR: 0.163462\tOF1: 0.269841\n",
      "Train Epoch: 2 [3328/4307 (76%)]\tLoss: 0.485685 \tOP: 0.894737\tOR: 0.330097\tOF1: 0.482270\n",
      "Train Epoch: 2 [3392/4307 (78%)]\tLoss: 0.473107 \tOP: 0.861111\tOR: 0.319588\tOF1: 0.466165\n",
      "Train Epoch: 2 [3456/4307 (79%)]\tLoss: 0.492243 \tOP: 0.921053\tOR: 0.360825\tOF1: 0.518519\n",
      "Train Epoch: 2 [3520/4307 (81%)]\tLoss: 0.500119 \tOP: 0.906977\tOR: 0.361111\tOF1: 0.516556\n",
      "Train Epoch: 2 [3584/4307 (82%)]\tLoss: 0.519789 \tOP: 0.846154\tOR: 0.340206\tOF1: 0.485294\n",
      "Train Epoch: 2 [3648/4307 (84%)]\tLoss: 0.516239 \tOP: 0.852941\tOR: 0.298969\tOF1: 0.442748\n",
      "Train Epoch: 2 [3712/4307 (85%)]\tLoss: 0.496165 \tOP: 0.894737\tOR: 0.317757\tOF1: 0.468966\n",
      "Train Epoch: 2 [3776/4307 (87%)]\tLoss: 0.532195 \tOP: 0.875000\tOR: 0.372340\tOF1: 0.522388\n",
      "Train Epoch: 2 [3840/4307 (88%)]\tLoss: 0.546225 \tOP: 0.717949\tOR: 0.291667\tOF1: 0.414815\n",
      "Train Epoch: 2 [3904/4307 (90%)]\tLoss: 0.498057 \tOP: 0.875000\tOR: 0.384615\tOF1: 0.534351\n",
      "Train Epoch: 2 [3968/4307 (91%)]\tLoss: 0.514425 \tOP: 0.800000\tOR: 0.346154\tOF1: 0.483221\n",
      "Train Epoch: 2 [4032/4307 (93%)]\tLoss: 0.532877 \tOP: 0.909091\tOR: 0.288462\tOF1: 0.437956\n",
      "Train Epoch: 2 [4096/4307 (94%)]\tLoss: 0.486567 \tOP: 0.857143\tOR: 0.346154\tOF1: 0.493151\n",
      "Train Epoch: 2 [4160/4307 (96%)]\tLoss: 0.499099 \tOP: 0.829268\tOR: 0.354167\tOF1: 0.496350\n",
      "Train Epoch: 2 [4224/4307 (97%)]\tLoss: 0.476673 \tOP: 0.804878\tOR: 0.336735\tOF1: 0.474820\n",
      "Train Epoch: 2 [1273/4307 (99%)]\tLoss: 0.465295 \tOP: 0.666667\tOR: 0.275862\tOF1: 0.390244\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5126 \n",
      "OP: 0.691262\n",
      "OR: 0.422158\n",
      "OF1: 0.523467\n",
      "\n",
      "Train Epoch: 3 [0/4307 (0%)]\tLoss: 0.481618 \tOP: 0.902439\tOR: 0.352381\tOF1: 0.506849\n",
      "Train Epoch: 3 [64/4307 (1%)]\tLoss: 0.445667 \tOP: 0.957447\tOR: 0.454545\tOF1: 0.616438\n",
      "Train Epoch: 3 [128/4307 (3%)]\tLoss: 0.454185 \tOP: 0.909091\tOR: 0.300000\tOF1: 0.451128\n",
      "Train Epoch: 3 [192/4307 (4%)]\tLoss: 0.513303 \tOP: 0.880952\tOR: 0.370000\tOF1: 0.521127\n",
      "Train Epoch: 3 [256/4307 (6%)]\tLoss: 0.439661 \tOP: 0.930233\tOR: 0.408163\tOF1: 0.567376\n",
      "Train Epoch: 3 [320/4307 (7%)]\tLoss: 0.454403 \tOP: 0.925000\tOR: 0.370000\tOF1: 0.528571\n",
      "Train Epoch: 3 [384/4307 (9%)]\tLoss: 0.445898 \tOP: 0.900000\tOR: 0.468750\tOF1: 0.616438\n",
      "Train Epoch: 3 [448/4307 (10%)]\tLoss: 0.454680 \tOP: 0.911111\tOR: 0.414141\tOF1: 0.569444\n",
      "Train Epoch: 3 [512/4307 (12%)]\tLoss: 0.471007 \tOP: 0.921053\tOR: 0.372340\tOF1: 0.530303\n",
      "Train Epoch: 3 [576/4307 (13%)]\tLoss: 0.499236 \tOP: 0.909091\tOR: 0.267857\tOF1: 0.413793\n",
      "Train Epoch: 3 [640/4307 (15%)]\tLoss: 0.442806 \tOP: 0.921053\tOR: 0.350000\tOF1: 0.507246\n",
      "Train Epoch: 3 [704/4307 (16%)]\tLoss: 0.492024 \tOP: 0.894737\tOR: 0.326923\tOF1: 0.478873\n",
      "Train Epoch: 3 [768/4307 (18%)]\tLoss: 0.465870 \tOP: 0.972222\tOR: 0.350000\tOF1: 0.514706\n",
      "Train Epoch: 3 [832/4307 (19%)]\tLoss: 0.463144 \tOP: 0.945946\tOR: 0.339806\tOF1: 0.500000\n",
      "Train Epoch: 3 [896/4307 (21%)]\tLoss: 0.500909 \tOP: 0.894737\tOR: 0.311927\tOF1: 0.462585\n",
      "Train Epoch: 3 [960/4307 (22%)]\tLoss: 0.492751 \tOP: 0.795455\tOR: 0.393258\tOF1: 0.526316\n",
      "Train Epoch: 3 [1024/4307 (24%)]\tLoss: 0.476294 \tOP: 0.900000\tOR: 0.378947\tOF1: 0.533333\n",
      "Train Epoch: 3 [1088/4307 (25%)]\tLoss: 0.476931 \tOP: 0.875000\tOR: 0.388889\tOF1: 0.538462\n",
      "Train Epoch: 3 [1152/4307 (26%)]\tLoss: 0.450912 \tOP: 0.891304\tOR: 0.398058\tOF1: 0.550336\n",
      "Train Epoch: 3 [1216/4307 (28%)]\tLoss: 0.464021 \tOP: 0.894737\tOR: 0.365591\tOF1: 0.519084\n",
      "Train Epoch: 3 [1280/4307 (29%)]\tLoss: 0.467139 \tOP: 0.921053\tOR: 0.343137\tOF1: 0.500000\n",
      "Train Epoch: 3 [1344/4307 (31%)]\tLoss: 0.526636 \tOP: 0.813953\tOR: 0.343137\tOF1: 0.482759\n",
      "Train Epoch: 3 [1408/4307 (32%)]\tLoss: 0.454406 \tOP: 0.955556\tOR: 0.421569\tOF1: 0.585034\n",
      "Train Epoch: 3 [1472/4307 (34%)]\tLoss: 0.457498 \tOP: 0.976744\tOR: 0.424242\tOF1: 0.591549\n",
      "Train Epoch: 3 [1536/4307 (35%)]\tLoss: 0.459759 \tOP: 0.885714\tOR: 0.329787\tOF1: 0.480620\n",
      "Train Epoch: 3 [1600/4307 (37%)]\tLoss: 0.460629 \tOP: 0.941176\tOR: 0.310680\tOF1: 0.467153\n",
      "Train Epoch: 3 [1664/4307 (38%)]\tLoss: 0.473128 \tOP: 0.975000\tOR: 0.371429\tOF1: 0.537931\n",
      "Train Epoch: 3 [1728/4307 (40%)]\tLoss: 0.440734 \tOP: 0.860465\tOR: 0.362745\tOF1: 0.510345\n",
      "Train Epoch: 3 [1792/4307 (41%)]\tLoss: 0.510940 \tOP: 0.923077\tOR: 0.356436\tOF1: 0.514286\n",
      "Train Epoch: 3 [1856/4307 (43%)]\tLoss: 0.503274 \tOP: 0.869565\tOR: 0.408163\tOF1: 0.555556\n",
      "Train Epoch: 3 [1920/4307 (44%)]\tLoss: 0.446801 \tOP: 0.897436\tOR: 0.376344\tOF1: 0.530303\n",
      "Train Epoch: 3 [1984/4307 (46%)]\tLoss: 0.442696 \tOP: 0.972222\tOR: 0.380435\tOF1: 0.546875\n",
      "Train Epoch: 3 [2048/4307 (47%)]\tLoss: 0.497714 \tOP: 0.925000\tOR: 0.355769\tOF1: 0.513889\n",
      "Train Epoch: 3 [2112/4307 (49%)]\tLoss: 0.488656 \tOP: 0.880952\tOR: 0.389474\tOF1: 0.540146\n",
      "Train Epoch: 3 [2176/4307 (50%)]\tLoss: 0.482974 \tOP: 0.933333\tOR: 0.456522\tOF1: 0.613139\n",
      "Train Epoch: 3 [2240/4307 (51%)]\tLoss: 0.515869 \tOP: 0.863636\tOR: 0.391753\tOF1: 0.539007\n",
      "Train Epoch: 3 [2304/4307 (53%)]\tLoss: 0.452953 \tOP: 0.902439\tOR: 0.349057\tOF1: 0.503401\n",
      "Train Epoch: 3 [2368/4307 (54%)]\tLoss: 0.453266 \tOP: 0.972222\tOR: 0.346535\tOF1: 0.510949\n",
      "Train Epoch: 3 [2432/4307 (56%)]\tLoss: 0.436677 \tOP: 0.971429\tOR: 0.336634\tOF1: 0.500000\n",
      "Train Epoch: 3 [2496/4307 (57%)]\tLoss: 0.413755 \tOP: 0.948718\tOR: 0.406593\tOF1: 0.569231\n",
      "Train Epoch: 3 [2560/4307 (59%)]\tLoss: 0.440492 \tOP: 0.857143\tOR: 0.382979\tOF1: 0.529412\n",
      "Train Epoch: 3 [2624/4307 (60%)]\tLoss: 0.466573 \tOP: 0.911765\tOR: 0.313131\tOF1: 0.466165\n",
      "Train Epoch: 3 [2688/4307 (62%)]\tLoss: 0.502811 \tOP: 0.923077\tOR: 0.356436\tOF1: 0.514286\n",
      "Train Epoch: 3 [2752/4307 (63%)]\tLoss: 0.485961 \tOP: 0.868421\tOR: 0.323529\tOF1: 0.471429\n",
      "Train Epoch: 3 [2816/4307 (65%)]\tLoss: 0.478332 \tOP: 0.976744\tOR: 0.378378\tOF1: 0.545455\n",
      "Train Epoch: 3 [2880/4307 (66%)]\tLoss: 0.468890 \tOP: 0.886364\tOR: 0.378641\tOF1: 0.530612\n",
      "Train Epoch: 3 [2944/4307 (68%)]\tLoss: 0.510258 \tOP: 0.944444\tOR: 0.311927\tOF1: 0.468966\n",
      "Train Epoch: 3 [3008/4307 (69%)]\tLoss: 0.479447 \tOP: 0.825000\tOR: 0.347368\tOF1: 0.488889\n",
      "Train Epoch: 3 [3072/4307 (71%)]\tLoss: 0.440107 \tOP: 0.941176\tOR: 0.461538\tOF1: 0.619355\n",
      "Train Epoch: 3 [3136/4307 (72%)]\tLoss: 0.483240 \tOP: 0.860465\tOR: 0.389474\tOF1: 0.536232\n",
      "Train Epoch: 3 [3200/4307 (74%)]\tLoss: 0.464950 \tOP: 0.829268\tOR: 0.369565\tOF1: 0.511278\n",
      "Train Epoch: 3 [3264/4307 (75%)]\tLoss: 0.452928 \tOP: 0.926829\tOR: 0.380000\tOF1: 0.539007\n",
      "Train Epoch: 3 [3328/4307 (76%)]\tLoss: 0.467812 \tOP: 0.972222\tOR: 0.364583\tOF1: 0.530303\n",
      "Train Epoch: 3 [3392/4307 (78%)]\tLoss: 0.473488 \tOP: 0.875000\tOR: 0.446809\tOF1: 0.591549\n",
      "Train Epoch: 3 [3456/4307 (79%)]\tLoss: 0.524957 \tOP: 0.891304\tOR: 0.401961\tOF1: 0.554054\n",
      "Train Epoch: 3 [3520/4307 (81%)]\tLoss: 0.513500 \tOP: 0.916667\tOR: 0.326733\tOF1: 0.481752\n",
      "Train Epoch: 3 [3584/4307 (82%)]\tLoss: 0.427260 \tOP: 0.880952\tOR: 0.397849\tOF1: 0.548148\n",
      "Train Epoch: 3 [3648/4307 (84%)]\tLoss: 0.450278 \tOP: 0.921053\tOR: 0.346535\tOF1: 0.503597\n",
      "Train Epoch: 3 [3712/4307 (85%)]\tLoss: 0.500220 \tOP: 0.897436\tOR: 0.339806\tOF1: 0.492958\n",
      "Train Epoch: 3 [3776/4307 (87%)]\tLoss: 0.452330 \tOP: 0.950000\tOR: 0.380000\tOF1: 0.542857\n",
      "Train Epoch: 3 [3840/4307 (88%)]\tLoss: 0.451648 \tOP: 0.977778\tOR: 0.448980\tOF1: 0.615385\n",
      "Train Epoch: 3 [3904/4307 (90%)]\tLoss: 0.489538 \tOP: 0.871795\tOR: 0.330097\tOF1: 0.478873\n",
      "Train Epoch: 3 [3968/4307 (91%)]\tLoss: 0.406702 \tOP: 0.977778\tOR: 0.458333\tOF1: 0.624113\n",
      "Train Epoch: 3 [4032/4307 (93%)]\tLoss: 0.488865 \tOP: 0.840000\tOR: 0.415842\tOF1: 0.556291\n",
      "Train Epoch: 3 [4096/4307 (94%)]\tLoss: 0.529145 \tOP: 0.888889\tOR: 0.353982\tOF1: 0.506329\n",
      "Train Epoch: 3 [4160/4307 (96%)]\tLoss: 0.469557 \tOP: 0.900000\tOR: 0.367347\tOF1: 0.521739\n",
      "Train Epoch: 3 [4224/4307 (97%)]\tLoss: 0.490284 \tOP: 0.906977\tOR: 0.382353\tOF1: 0.537931\n",
      "Train Epoch: 3 [1273/4307 (99%)]\tLoss: 0.444014 \tOP: 0.666667\tOR: 0.320000\tOF1: 0.432432\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5196 \n",
      "OP: 0.666392\n",
      "OR: 0.432319\n",
      "OF1: 0.523603\n",
      "\n",
      "Train Epoch: 4 [0/4307 (0%)]\tLoss: 0.430854 \tOP: 0.886792\tOR: 0.479592\tOF1: 0.622517\n",
      "Train Epoch: 4 [64/4307 (1%)]\tLoss: 0.457218 \tOP: 0.921053\tOR: 0.346535\tOF1: 0.503597\n",
      "Train Epoch: 4 [128/4307 (3%)]\tLoss: 0.467439 \tOP: 0.867925\tOR: 0.464646\tOF1: 0.605263\n",
      "Train Epoch: 4 [192/4307 (4%)]\tLoss: 0.440118 \tOP: 0.928571\tOR: 0.423913\tOF1: 0.582090\n",
      "Train Epoch: 4 [256/4307 (6%)]\tLoss: 0.430015 \tOP: 0.897959\tOR: 0.431373\tOF1: 0.582781\n",
      "Train Epoch: 4 [320/4307 (7%)]\tLoss: 0.403045 \tOP: 0.940000\tOR: 0.489583\tOF1: 0.643836\n",
      "Train Epoch: 4 [384/4307 (9%)]\tLoss: 0.468300 \tOP: 0.972973\tOR: 0.371134\tOF1: 0.537313\n",
      "Train Epoch: 4 [448/4307 (10%)]\tLoss: 0.435402 \tOP: 0.976744\tOR: 0.385321\tOF1: 0.552632\n",
      "Train Epoch: 4 [512/4307 (12%)]\tLoss: 0.437329 \tOP: 0.926829\tOR: 0.372549\tOF1: 0.531469\n",
      "Train Epoch: 4 [576/4307 (13%)]\tLoss: 0.476534 \tOP: 0.972222\tOR: 0.333333\tOF1: 0.496454\n",
      "Train Epoch: 4 [640/4307 (15%)]\tLoss: 0.420629 \tOP: 0.976744\tOR: 0.428571\tOF1: 0.595745\n",
      "Train Epoch: 4 [704/4307 (16%)]\tLoss: 0.428163 \tOP: 0.921569\tOR: 0.474747\tOF1: 0.626667\n",
      "Train Epoch: 4 [768/4307 (18%)]\tLoss: 0.464936 \tOP: 0.948718\tOR: 0.349057\tOF1: 0.510345\n",
      "Train Epoch: 4 [832/4307 (19%)]\tLoss: 0.473501 \tOP: 0.868421\tOR: 0.358696\tOF1: 0.507692\n",
      "Train Epoch: 4 [896/4307 (21%)]\tLoss: 0.441652 \tOP: 0.909091\tOR: 0.408163\tOF1: 0.563380\n",
      "Train Epoch: 4 [960/4307 (22%)]\tLoss: 0.458402 \tOP: 0.926829\tOR: 0.404255\tOF1: 0.562963\n",
      "Train Epoch: 4 [1024/4307 (24%)]\tLoss: 0.458468 \tOP: 0.937500\tOR: 0.309278\tOF1: 0.465116\n",
      "Train Epoch: 4 [1088/4307 (25%)]\tLoss: 0.457328 \tOP: 0.875000\tOR: 0.368421\tOF1: 0.518519\n",
      "Train Epoch: 4 [1152/4307 (26%)]\tLoss: 0.462381 \tOP: 1.000000\tOR: 0.443396\tOF1: 0.614379\n",
      "Train Epoch: 4 [1216/4307 (28%)]\tLoss: 0.448399 \tOP: 0.921053\tOR: 0.360825\tOF1: 0.518519\n",
      "Train Epoch: 4 [1280/4307 (29%)]\tLoss: 0.429568 \tOP: 0.975610\tOR: 0.380952\tOF1: 0.547945\n",
      "Train Epoch: 4 [1344/4307 (31%)]\tLoss: 0.407153 \tOP: 1.000000\tOR: 0.436893\tOF1: 0.608108\n",
      "Train Epoch: 4 [1408/4307 (32%)]\tLoss: 0.462874 \tOP: 0.923077\tOR: 0.346154\tOF1: 0.503497\n",
      "Train Epoch: 4 [1472/4307 (34%)]\tLoss: 0.455555 \tOP: 0.872340\tOR: 0.405941\tOF1: 0.554054\n",
      "Train Epoch: 4 [1536/4307 (35%)]\tLoss: 0.396136 \tOP: 1.000000\tOR: 0.417476\tOF1: 0.589041\n",
      "Train Epoch: 4 [1600/4307 (37%)]\tLoss: 0.424165 \tOP: 0.940000\tOR: 0.423423\tOF1: 0.583851\n",
      "Train Epoch: 4 [1664/4307 (38%)]\tLoss: 0.453230 \tOP: 0.975000\tOR: 0.382353\tOF1: 0.549296\n",
      "Train Epoch: 4 [1728/4307 (40%)]\tLoss: 0.433918 \tOP: 0.950000\tOR: 0.368932\tOF1: 0.531469\n",
      "Train Epoch: 4 [1792/4307 (41%)]\tLoss: 0.450404 \tOP: 0.947368\tOR: 0.352941\tOF1: 0.514286\n",
      "Train Epoch: 4 [1856/4307 (43%)]\tLoss: 0.393032 \tOP: 0.943396\tOR: 0.480769\tOF1: 0.636943\n",
      "Train Epoch: 4 [1920/4307 (44%)]\tLoss: 0.445046 \tOP: 0.945946\tOR: 0.343137\tOF1: 0.503597\n",
      "Train Epoch: 4 [1984/4307 (46%)]\tLoss: 0.452789 \tOP: 0.863636\tOR: 0.404255\tOF1: 0.550725\n",
      "Train Epoch: 4 [2048/4307 (47%)]\tLoss: 0.459941 \tOP: 0.888889\tOR: 0.388350\tOF1: 0.540541\n",
      "Train Epoch: 4 [2112/4307 (49%)]\tLoss: 0.434246 \tOP: 0.925000\tOR: 0.381443\tOF1: 0.540146\n",
      "Train Epoch: 4 [2176/4307 (50%)]\tLoss: 0.406901 \tOP: 1.000000\tOR: 0.458333\tOF1: 0.628571\n",
      "Train Epoch: 4 [2240/4307 (51%)]\tLoss: 0.473855 \tOP: 0.974359\tOR: 0.380000\tOF1: 0.546763\n",
      "Train Epoch: 4 [2304/4307 (53%)]\tLoss: 0.445927 \tOP: 0.911765\tOR: 0.295238\tOF1: 0.446043\n",
      "Train Epoch: 4 [2368/4307 (54%)]\tLoss: 0.443398 \tOP: 1.000000\tOR: 0.428571\tOF1: 0.600000\n",
      "Train Epoch: 4 [2432/4307 (56%)]\tLoss: 0.409332 \tOP: 0.978723\tOR: 0.500000\tOF1: 0.661871\n",
      "Train Epoch: 4 [2496/4307 (57%)]\tLoss: 0.472867 \tOP: 0.780488\tOR: 0.326531\tOF1: 0.460432\n",
      "Train Epoch: 4 [2560/4307 (59%)]\tLoss: 0.435932 \tOP: 0.959184\tOR: 0.470000\tOF1: 0.630872\n",
      "Train Epoch: 4 [2624/4307 (60%)]\tLoss: 0.492130 \tOP: 0.891892\tOR: 0.333333\tOF1: 0.485294\n",
      "Train Epoch: 4 [2688/4307 (62%)]\tLoss: 0.468373 \tOP: 0.875000\tOR: 0.411765\tOF1: 0.560000\n",
      "Train Epoch: 4 [2752/4307 (63%)]\tLoss: 0.439880 \tOP: 0.895833\tOR: 0.443299\tOF1: 0.593103\n",
      "Train Epoch: 4 [2816/4307 (65%)]\tLoss: 0.446812 \tOP: 0.952381\tOR: 0.373832\tOF1: 0.536913\n",
      "Train Epoch: 4 [2880/4307 (66%)]\tLoss: 0.447579 \tOP: 0.959184\tOR: 0.447619\tOF1: 0.610390\n",
      "Train Epoch: 4 [2944/4307 (68%)]\tLoss: 0.458827 \tOP: 0.930233\tOR: 0.449438\tOF1: 0.606061\n",
      "Train Epoch: 4 [3008/4307 (69%)]\tLoss: 0.440833 \tOP: 0.886364\tOR: 0.386139\tOF1: 0.537931\n",
      "Train Epoch: 4 [3072/4307 (71%)]\tLoss: 0.421237 \tOP: 0.956522\tOR: 0.411215\tOF1: 0.575163\n",
      "Train Epoch: 4 [3136/4307 (72%)]\tLoss: 0.432960 \tOP: 0.891304\tOR: 0.410000\tOF1: 0.561644\n",
      "Train Epoch: 4 [3200/4307 (74%)]\tLoss: 0.478001 \tOP: 0.925000\tOR: 0.389474\tOF1: 0.548148\n",
      "Train Epoch: 4 [3264/4307 (75%)]\tLoss: 0.408390 \tOP: 1.000000\tOR: 0.500000\tOF1: 0.666667\n",
      "Train Epoch: 4 [3328/4307 (76%)]\tLoss: 0.444145 \tOP: 0.931818\tOR: 0.405941\tOF1: 0.565517\n",
      "Train Epoch: 4 [3392/4307 (78%)]\tLoss: 0.442569 \tOP: 0.934783\tOR: 0.413462\tOF1: 0.573333\n",
      "Train Epoch: 4 [3456/4307 (79%)]\tLoss: 0.407226 \tOP: 0.976744\tOR: 0.411765\tOF1: 0.579310\n",
      "Train Epoch: 4 [3520/4307 (81%)]\tLoss: 0.478048 \tOP: 0.875000\tOR: 0.384615\tOF1: 0.534351\n",
      "Train Epoch: 4 [3584/4307 (82%)]\tLoss: 0.402733 \tOP: 0.958333\tOR: 0.479167\tOF1: 0.638889\n",
      "Train Epoch: 4 [3648/4307 (84%)]\tLoss: 0.474360 \tOP: 0.904762\tOR: 0.376238\tOF1: 0.531469\n",
      "Train Epoch: 4 [3712/4307 (85%)]\tLoss: 0.399901 \tOP: 0.940000\tOR: 0.484536\tOF1: 0.639456\n",
      "Train Epoch: 4 [3776/4307 (87%)]\tLoss: 0.467935 \tOP: 0.974359\tOR: 0.380000\tOF1: 0.546763\n",
      "Train Epoch: 4 [3840/4307 (88%)]\tLoss: 0.422225 \tOP: 0.978261\tOR: 0.428571\tOF1: 0.596026\n",
      "Train Epoch: 4 [3904/4307 (90%)]\tLoss: 0.429051 \tOP: 0.952381\tOR: 0.392157\tOF1: 0.555556\n",
      "Train Epoch: 4 [3968/4307 (91%)]\tLoss: 0.490865 \tOP: 0.814815\tOR: 0.440000\tOF1: 0.571429\n",
      "Train Epoch: 4 [4032/4307 (93%)]\tLoss: 0.431273 \tOP: 0.955556\tOR: 0.452632\tOF1: 0.614286\n",
      "Train Epoch: 4 [4096/4307 (94%)]\tLoss: 0.464021 \tOP: 0.909091\tOR: 0.412371\tOF1: 0.567376\n",
      "Train Epoch: 4 [4160/4307 (96%)]\tLoss: 0.429373 \tOP: 0.954545\tOR: 0.442105\tOF1: 0.604317\n",
      "Train Epoch: 4 [4224/4307 (97%)]\tLoss: 0.435551 \tOP: 1.000000\tOR: 0.474227\tOF1: 0.643357\n",
      "Train Epoch: 4 [1273/4307 (99%)]\tLoss: 0.460012 \tOP: 0.882353\tOR: 0.454545\tOF1: 0.600000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5271 \n",
      "OP: 0.665381\n",
      "OR: 0.454915\n",
      "OF1: 0.539408\n",
      "\n",
      "Train Epoch: 5 [0/4307 (0%)]\tLoss: 0.415052 \tOP: 0.956522\tOR: 0.407407\tOF1: 0.571429\n",
      "Train Epoch: 5 [64/4307 (1%)]\tLoss: 0.397096 \tOP: 0.930233\tOR: 0.384615\tOF1: 0.544218\n",
      "Train Epoch: 5 [128/4307 (3%)]\tLoss: 0.404976 \tOP: 0.916667\tOR: 0.463158\tOF1: 0.615385\n",
      "Train Epoch: 5 [192/4307 (4%)]\tLoss: 0.437334 \tOP: 0.976190\tOR: 0.418367\tOF1: 0.585714\n",
      "Train Epoch: 5 [256/4307 (6%)]\tLoss: 0.387758 \tOP: 1.000000\tOR: 0.441176\tOF1: 0.612245\n",
      "Train Epoch: 5 [320/4307 (7%)]\tLoss: 0.418571 \tOP: 0.977778\tOR: 0.419048\tOF1: 0.586667\n",
      "Train Epoch: 5 [384/4307 (9%)]\tLoss: 0.449061 \tOP: 0.921569\tOR: 0.451923\tOF1: 0.606452\n",
      "Train Epoch: 5 [448/4307 (10%)]\tLoss: 0.450928 \tOP: 0.921053\tOR: 0.372340\tOF1: 0.530303\n",
      "Train Epoch: 5 [512/4307 (12%)]\tLoss: 0.401270 \tOP: 0.953488\tOR: 0.436170\tOF1: 0.598540\n",
      "Train Epoch: 5 [576/4307 (13%)]\tLoss: 0.395596 \tOP: 0.954545\tOR: 0.432990\tOF1: 0.595745\n",
      "Train Epoch: 5 [640/4307 (15%)]\tLoss: 0.458080 \tOP: 0.926829\tOR: 0.404255\tOF1: 0.562963\n",
      "Train Epoch: 5 [704/4307 (16%)]\tLoss: 0.381815 \tOP: 0.977778\tOR: 0.488889\tOF1: 0.651852\n",
      "Train Epoch: 5 [768/4307 (18%)]\tLoss: 0.424645 \tOP: 1.000000\tOR: 0.366337\tOF1: 0.536232\n",
      "Train Epoch: 5 [832/4307 (19%)]\tLoss: 0.448275 \tOP: 0.930233\tOR: 0.412371\tOF1: 0.571429\n",
      "Train Epoch: 5 [896/4307 (21%)]\tLoss: 0.420817 \tOP: 0.924528\tOR: 0.490000\tOF1: 0.640523\n",
      "Train Epoch: 5 [960/4307 (22%)]\tLoss: 0.434601 \tOP: 0.895833\tOR: 0.472527\tOF1: 0.618705\n",
      "Train Epoch: 5 [1024/4307 (24%)]\tLoss: 0.402229 \tOP: 0.980000\tOR: 0.490000\tOF1: 0.653333\n",
      "Train Epoch: 5 [1088/4307 (25%)]\tLoss: 0.424877 \tOP: 0.916667\tOR: 0.358696\tOF1: 0.515625\n",
      "Train Epoch: 5 [1152/4307 (26%)]\tLoss: 0.392002 \tOP: 0.978723\tOR: 0.450980\tOF1: 0.617450\n",
      "Train Epoch: 5 [1216/4307 (28%)]\tLoss: 0.434373 \tOP: 0.953488\tOR: 0.398058\tOF1: 0.561644\n",
      "Train Epoch: 5 [1280/4307 (29%)]\tLoss: 0.385336 \tOP: 1.000000\tOR: 0.518182\tOF1: 0.682635\n",
      "Train Epoch: 5 [1344/4307 (31%)]\tLoss: 0.383605 \tOP: 0.976744\tOR: 0.461538\tOF1: 0.626866\n",
      "Train Epoch: 5 [1408/4307 (32%)]\tLoss: 0.409392 \tOP: 0.942308\tOR: 0.490000\tOF1: 0.644737\n",
      "Train Epoch: 5 [1472/4307 (34%)]\tLoss: 0.423866 \tOP: 0.940000\tOR: 0.489583\tOF1: 0.643836\n",
      "Train Epoch: 5 [1536/4307 (35%)]\tLoss: 0.421230 \tOP: 0.976190\tOR: 0.422680\tOF1: 0.589928\n",
      "Train Epoch: 5 [1600/4307 (37%)]\tLoss: 0.409570 \tOP: 0.952381\tOR: 0.416667\tOF1: 0.579710\n",
      "Train Epoch: 5 [1664/4307 (38%)]\tLoss: 0.435120 \tOP: 0.977778\tOR: 0.440000\tOF1: 0.606897\n",
      "Train Epoch: 5 [1728/4307 (40%)]\tLoss: 0.408098 \tOP: 0.938776\tOR: 0.433962\tOF1: 0.593548\n",
      "Train Epoch: 5 [1792/4307 (41%)]\tLoss: 0.376115 \tOP: 0.965517\tOR: 0.504505\tOF1: 0.662722\n",
      "Train Epoch: 5 [1856/4307 (43%)]\tLoss: 0.408978 \tOP: 0.909091\tOR: 0.430108\tOF1: 0.583942\n",
      "Train Epoch: 5 [1920/4307 (44%)]\tLoss: 0.408077 \tOP: 0.979592\tOR: 0.521739\tOF1: 0.680851\n",
      "Train Epoch: 5 [1984/4307 (46%)]\tLoss: 0.426798 \tOP: 0.942857\tOR: 0.333333\tOF1: 0.492537\n",
      "Train Epoch: 5 [2048/4307 (47%)]\tLoss: 0.418642 \tOP: 0.941176\tOR: 0.470588\tOF1: 0.627451\n",
      "Train Epoch: 5 [2112/4307 (49%)]\tLoss: 0.409710 \tOP: 0.888889\tOR: 0.412371\tOF1: 0.563380\n",
      "Train Epoch: 5 [2176/4307 (50%)]\tLoss: 0.409810 \tOP: 0.940000\tOR: 0.479592\tOF1: 0.635135\n",
      "Train Epoch: 5 [2240/4307 (51%)]\tLoss: 0.439402 \tOP: 1.000000\tOR: 0.458716\tOF1: 0.628931\n",
      "Train Epoch: 5 [2304/4307 (53%)]\tLoss: 0.431120 \tOP: 0.936170\tOR: 0.431373\tOF1: 0.590604\n",
      "Train Epoch: 5 [2368/4307 (54%)]\tLoss: 0.420184 \tOP: 0.977778\tOR: 0.419048\tOF1: 0.586667\n",
      "Train Epoch: 5 [2432/4307 (56%)]\tLoss: 0.397541 \tOP: 0.977778\tOR: 0.448980\tOF1: 0.615385\n",
      "Train Epoch: 5 [2496/4307 (57%)]\tLoss: 0.441965 \tOP: 0.962264\tOR: 0.500000\tOF1: 0.658065\n",
      "Train Epoch: 5 [2560/4307 (59%)]\tLoss: 0.396236 \tOP: 1.000000\tOR: 0.521739\tOF1: 0.685714\n",
      "Train Epoch: 5 [2624/4307 (60%)]\tLoss: 0.441490 \tOP: 0.906977\tOR: 0.397959\tOF1: 0.553191\n",
      "Train Epoch: 5 [2688/4307 (62%)]\tLoss: 0.455318 \tOP: 0.904762\tOR: 0.358491\tOF1: 0.513514\n",
      "Train Epoch: 5 [2752/4307 (63%)]\tLoss: 0.415402 \tOP: 0.950000\tOR: 0.380000\tOF1: 0.542857\n",
      "Train Epoch: 5 [2816/4307 (65%)]\tLoss: 0.433134 \tOP: 0.911111\tOR: 0.394231\tOF1: 0.550336\n",
      "Train Epoch: 5 [2880/4307 (66%)]\tLoss: 0.397608 \tOP: 0.961538\tOR: 0.490196\tOF1: 0.649351\n",
      "Train Epoch: 5 [2944/4307 (68%)]\tLoss: 0.435999 \tOP: 0.975000\tOR: 0.375000\tOF1: 0.541667\n",
      "Train Epoch: 5 [3008/4307 (69%)]\tLoss: 0.393783 \tOP: 1.000000\tOR: 0.407767\tOF1: 0.579310\n",
      "Train Epoch: 5 [3072/4307 (71%)]\tLoss: 0.439055 \tOP: 0.931818\tOR: 0.405941\tOF1: 0.565517\n",
      "Train Epoch: 5 [3136/4307 (72%)]\tLoss: 0.431725 \tOP: 0.869565\tOR: 0.449438\tOF1: 0.592593\n",
      "Train Epoch: 5 [3200/4307 (74%)]\tLoss: 0.352662 \tOP: 1.000000\tOR: 0.471698\tOF1: 0.641026\n",
      "Train Epoch: 5 [3264/4307 (75%)]\tLoss: 0.424877 \tOP: 0.905660\tOR: 0.489796\tOF1: 0.635762\n",
      "Train Epoch: 5 [3328/4307 (76%)]\tLoss: 0.440654 \tOP: 0.976190\tOR: 0.427083\tOF1: 0.594203\n",
      "Train Epoch: 5 [3392/4307 (78%)]\tLoss: 0.410417 \tOP: 0.956522\tOR: 0.427184\tOF1: 0.590604\n",
      "Train Epoch: 5 [3456/4307 (79%)]\tLoss: 0.409409 \tOP: 0.888889\tOR: 0.408163\tOF1: 0.559441\n",
      "Train Epoch: 5 [3520/4307 (81%)]\tLoss: 0.440215 \tOP: 0.930233\tOR: 0.408163\tOF1: 0.567376\n",
      "Train Epoch: 5 [3584/4307 (82%)]\tLoss: 0.437483 \tOP: 0.926829\tOR: 0.365385\tOF1: 0.524138\n",
      "Train Epoch: 5 [3648/4307 (84%)]\tLoss: 0.430081 \tOP: 0.931818\tOR: 0.414141\tOF1: 0.573427\n",
      "Train Epoch: 5 [3712/4307 (85%)]\tLoss: 0.424803 \tOP: 0.920000\tOR: 0.429907\tOF1: 0.585987\n",
      "Train Epoch: 5 [3776/4307 (87%)]\tLoss: 0.433194 \tOP: 0.976744\tOR: 0.424242\tOF1: 0.591549\n",
      "Train Epoch: 5 [3840/4307 (88%)]\tLoss: 0.437259 \tOP: 0.933333\tOR: 0.388889\tOF1: 0.549020\n",
      "Train Epoch: 5 [3904/4307 (90%)]\tLoss: 0.423087 \tOP: 0.954545\tOR: 0.428571\tOF1: 0.591549\n",
      "Train Epoch: 5 [3968/4307 (91%)]\tLoss: 0.395533 \tOP: 0.916667\tOR: 0.427184\tOF1: 0.582781\n",
      "Train Epoch: 5 [4032/4307 (93%)]\tLoss: 0.399029 \tOP: 0.933333\tOR: 0.446809\tOF1: 0.604317\n",
      "Train Epoch: 5 [4096/4307 (94%)]\tLoss: 0.431208 \tOP: 0.950000\tOR: 0.383838\tOF1: 0.546763\n",
      "Train Epoch: 5 [4160/4307 (96%)]\tLoss: 0.420236 \tOP: 0.977273\tOR: 0.409524\tOF1: 0.577181\n",
      "Train Epoch: 5 [4224/4307 (97%)]\tLoss: 0.446931 \tOP: 0.972973\tOR: 0.356436\tOF1: 0.521739\n",
      "Train Epoch: 5 [1273/4307 (99%)]\tLoss: 0.399163 \tOP: 0.928571\tOR: 0.464286\tOF1: 0.619048\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5113 \n",
      "OP: 0.695124\n",
      "OR: 0.459330\n",
      "OF1: 0.552124\n",
      "\n",
      "Train Epoch: 6 [0/4307 (0%)]\tLoss: 0.383826 \tOP: 0.978723\tOR: 0.479167\tOF1: 0.643357\n",
      "Train Epoch: 6 [64/4307 (1%)]\tLoss: 0.405385 \tOP: 0.957447\tOR: 0.436893\tOF1: 0.600000\n",
      "Train Epoch: 6 [128/4307 (3%)]\tLoss: 0.420359 \tOP: 0.978261\tOR: 0.416667\tOF1: 0.584416\n",
      "Train Epoch: 6 [192/4307 (4%)]\tLoss: 0.426672 \tOP: 0.911111\tOR: 0.410000\tOF1: 0.565517\n",
      "Train Epoch: 6 [256/4307 (6%)]\tLoss: 0.400977 \tOP: 0.977778\tOR: 0.458333\tOF1: 0.624113\n",
      "Train Epoch: 6 [320/4307 (7%)]\tLoss: 0.436223 \tOP: 0.934783\tOR: 0.409524\tOF1: 0.569536\n",
      "Train Epoch: 6 [384/4307 (9%)]\tLoss: 0.405089 \tOP: 0.980392\tOR: 0.495050\tOF1: 0.657895\n",
      "Train Epoch: 6 [448/4307 (10%)]\tLoss: 0.426919 \tOP: 1.000000\tOR: 0.441176\tOF1: 0.612245\n",
      "Train Epoch: 6 [512/4307 (12%)]\tLoss: 0.407202 \tOP: 0.976190\tOR: 0.410000\tOF1: 0.577465\n",
      "Train Epoch: 6 [576/4307 (13%)]\tLoss: 0.420170 \tOP: 0.930233\tOR: 0.416667\tOF1: 0.575540\n",
      "Train Epoch: 6 [640/4307 (15%)]\tLoss: 0.398926 \tOP: 0.976190\tOR: 0.410000\tOF1: 0.577465\n",
      "Train Epoch: 6 [704/4307 (16%)]\tLoss: 0.401067 \tOP: 0.951220\tOR: 0.371429\tOF1: 0.534247\n",
      "Train Epoch: 6 [768/4307 (18%)]\tLoss: 0.379305 \tOP: 0.955556\tOR: 0.438776\tOF1: 0.601399\n",
      "Train Epoch: 6 [832/4307 (19%)]\tLoss: 0.445037 \tOP: 0.913043\tOR: 0.392523\tOF1: 0.549020\n",
      "Train Epoch: 6 [896/4307 (21%)]\tLoss: 0.445609 \tOP: 0.973684\tOR: 0.370000\tOF1: 0.536232\n",
      "Train Epoch: 6 [960/4307 (22%)]\tLoss: 0.412634 \tOP: 0.959184\tOR: 0.474747\tOF1: 0.635135\n",
      "Train Epoch: 6 [1024/4307 (24%)]\tLoss: 0.399305 \tOP: 0.979167\tOR: 0.439252\tOF1: 0.606452\n",
      "Train Epoch: 6 [1088/4307 (25%)]\tLoss: 0.391847 \tOP: 0.975610\tOR: 0.400000\tOF1: 0.567376\n",
      "Train Epoch: 6 [1152/4307 (26%)]\tLoss: 0.430535 \tOP: 0.953488\tOR: 0.386792\tOF1: 0.550336\n",
      "Train Epoch: 6 [1216/4307 (28%)]\tLoss: 0.398662 \tOP: 0.938776\tOR: 0.479167\tOF1: 0.634483\n",
      "Train Epoch: 6 [1280/4307 (29%)]\tLoss: 0.404619 \tOP: 0.955556\tOR: 0.462366\tOF1: 0.623188\n",
      "Train Epoch: 6 [1344/4307 (31%)]\tLoss: 0.389608 \tOP: 1.000000\tOR: 0.484848\tOF1: 0.653061\n",
      "Train Epoch: 6 [1408/4307 (32%)]\tLoss: 0.418617 \tOP: 0.933333\tOR: 0.407767\tOF1: 0.567568\n",
      "Train Epoch: 6 [1472/4307 (34%)]\tLoss: 0.407063 \tOP: 0.963636\tOR: 0.535354\tOF1: 0.688312\n",
      "Train Epoch: 6 [1536/4307 (35%)]\tLoss: 0.401683 \tOP: 1.000000\tOR: 0.443396\tOF1: 0.614379\n",
      "Train Epoch: 6 [1600/4307 (37%)]\tLoss: 0.369151 \tOP: 0.982143\tOR: 0.528846\tOF1: 0.687500\n",
      "Train Epoch: 6 [1664/4307 (38%)]\tLoss: 0.387174 \tOP: 1.000000\tOR: 0.520000\tOF1: 0.684211\n",
      "Train Epoch: 6 [1728/4307 (40%)]\tLoss: 0.386078 \tOP: 0.960784\tOR: 0.471154\tOF1: 0.632258\n",
      "Train Epoch: 6 [1792/4307 (41%)]\tLoss: 0.432142 \tOP: 0.916667\tOR: 0.366667\tOF1: 0.523810\n",
      "Train Epoch: 6 [1856/4307 (43%)]\tLoss: 0.425938 \tOP: 0.931818\tOR: 0.427083\tOF1: 0.585714\n",
      "Train Epoch: 6 [1920/4307 (44%)]\tLoss: 0.378101 \tOP: 0.981132\tOR: 0.504854\tOF1: 0.666667\n",
      "Train Epoch: 6 [1984/4307 (46%)]\tLoss: 0.386536 \tOP: 1.000000\tOR: 0.480000\tOF1: 0.648649\n",
      "Train Epoch: 6 [2048/4307 (47%)]\tLoss: 0.386557 \tOP: 0.931818\tOR: 0.440860\tOF1: 0.598540\n",
      "Train Epoch: 6 [2112/4307 (49%)]\tLoss: 0.419952 \tOP: 0.960000\tOR: 0.505263\tOF1: 0.662069\n",
      "Train Epoch: 6 [2176/4307 (50%)]\tLoss: 0.445750 \tOP: 0.906977\tOR: 0.414894\tOF1: 0.569343\n",
      "Train Epoch: 6 [2240/4307 (51%)]\tLoss: 0.426027 \tOP: 0.923077\tOR: 0.484848\tOF1: 0.635762\n",
      "Train Epoch: 6 [2304/4307 (53%)]\tLoss: 0.399180 \tOP: 0.975610\tOR: 0.439560\tOF1: 0.606061\n",
      "Train Epoch: 6 [2368/4307 (54%)]\tLoss: 0.408996 \tOP: 0.980000\tOR: 0.471154\tOF1: 0.636364\n",
      "Train Epoch: 6 [2432/4307 (56%)]\tLoss: 0.459627 \tOP: 0.875000\tOR: 0.407767\tOF1: 0.556291\n",
      "Train Epoch: 6 [2496/4307 (57%)]\tLoss: 0.376245 \tOP: 1.000000\tOR: 0.514563\tOF1: 0.679487\n",
      "Train Epoch: 6 [2560/4307 (59%)]\tLoss: 0.413832 \tOP: 0.977273\tOR: 0.417476\tOF1: 0.585034\n",
      "Train Epoch: 6 [2624/4307 (60%)]\tLoss: 0.431385 \tOP: 0.950000\tOR: 0.400000\tOF1: 0.562963\n",
      "Train Epoch: 6 [2688/4307 (62%)]\tLoss: 0.413071 \tOP: 0.977778\tOR: 0.444444\tOF1: 0.611111\n",
      "Train Epoch: 6 [2752/4307 (63%)]\tLoss: 0.410412 \tOP: 1.000000\tOR: 0.393258\tOF1: 0.564516\n",
      "Train Epoch: 6 [2816/4307 (65%)]\tLoss: 0.457282 \tOP: 0.962264\tOR: 0.536842\tOF1: 0.689189\n",
      "Train Epoch: 6 [2880/4307 (66%)]\tLoss: 0.336911 \tOP: 0.981481\tOR: 0.557895\tOF1: 0.711409\n",
      "Train Epoch: 6 [2944/4307 (68%)]\tLoss: 0.387522 \tOP: 0.960000\tOR: 0.461538\tOF1: 0.623377\n",
      "Train Epoch: 6 [3008/4307 (69%)]\tLoss: 0.394019 \tOP: 0.916667\tOR: 0.572917\tOF1: 0.705128\n",
      "Train Epoch: 6 [3072/4307 (71%)]\tLoss: 0.415735 \tOP: 0.937500\tOR: 0.416667\tOF1: 0.576923\n",
      "Train Epoch: 6 [3136/4307 (72%)]\tLoss: 0.412284 \tOP: 0.972973\tOR: 0.367347\tOF1: 0.533333\n",
      "Train Epoch: 6 [3200/4307 (74%)]\tLoss: 0.374829 \tOP: 0.979167\tOR: 0.479592\tOF1: 0.643836\n",
      "Train Epoch: 6 [3264/4307 (75%)]\tLoss: 0.362970 \tOP: 0.979167\tOR: 0.494737\tOF1: 0.657343\n",
      "Train Epoch: 6 [3328/4307 (76%)]\tLoss: 0.440109 \tOP: 0.976190\tOR: 0.366071\tOF1: 0.532468\n",
      "Train Epoch: 6 [3392/4307 (78%)]\tLoss: 0.432192 \tOP: 0.980392\tOR: 0.446429\tOF1: 0.613497\n",
      "Train Epoch: 6 [3456/4307 (79%)]\tLoss: 0.378240 \tOP: 0.977778\tOR: 0.444444\tOF1: 0.611111\n",
      "Train Epoch: 6 [3520/4307 (81%)]\tLoss: 0.437342 \tOP: 0.950000\tOR: 0.365385\tOF1: 0.527778\n",
      "Train Epoch: 6 [3584/4307 (82%)]\tLoss: 0.405232 \tOP: 0.958333\tOR: 0.425926\tOF1: 0.589744\n",
      "Train Epoch: 6 [3648/4307 (84%)]\tLoss: 0.427943 \tOP: 0.979167\tOR: 0.456311\tOF1: 0.622517\n",
      "Train Epoch: 6 [3712/4307 (85%)]\tLoss: 0.419602 \tOP: 0.933333\tOR: 0.428571\tOF1: 0.587413\n",
      "Train Epoch: 6 [3776/4307 (87%)]\tLoss: 0.414862 \tOP: 0.958333\tOR: 0.464646\tOF1: 0.625850\n",
      "Train Epoch: 6 [3840/4307 (88%)]\tLoss: 0.411282 \tOP: 0.973684\tOR: 0.385417\tOF1: 0.552239\n",
      "Train Epoch: 6 [3904/4307 (90%)]\tLoss: 0.394866 \tOP: 0.952381\tOR: 0.459770\tOF1: 0.620155\n",
      "Train Epoch: 6 [3968/4307 (91%)]\tLoss: 0.383172 \tOP: 1.000000\tOR: 0.419048\tOF1: 0.590604\n",
      "Train Epoch: 6 [4032/4307 (93%)]\tLoss: 0.389030 \tOP: 0.975610\tOR: 0.430108\tOF1: 0.597015\n",
      "Train Epoch: 6 [4096/4307 (94%)]\tLoss: 0.385870 \tOP: 1.000000\tOR: 0.473118\tOF1: 0.642336\n",
      "Train Epoch: 6 [4160/4307 (96%)]\tLoss: 0.345297 \tOP: 0.983333\tOR: 0.614583\tOF1: 0.756410\n",
      "Train Epoch: 6 [4224/4307 (97%)]\tLoss: 0.390773 \tOP: 0.924528\tOR: 0.475728\tOF1: 0.628205\n",
      "Train Epoch: 6 [1273/4307 (99%)]\tLoss: 0.530223 \tOP: 0.818182\tOR: 0.290323\tOF1: 0.428571\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5286 \n",
      "OP: 0.683451\n",
      "OR: 0.525101\n",
      "OF1: 0.592826\n",
      "\n",
      "Train Epoch: 7 [0/4307 (0%)]\tLoss: 0.380468 \tOP: 0.955556\tOR: 0.425743\tOF1: 0.589041\n",
      "Train Epoch: 7 [64/4307 (1%)]\tLoss: 0.385352 \tOP: 0.977778\tOR: 0.488889\tOF1: 0.651852\n",
      "Train Epoch: 7 [128/4307 (3%)]\tLoss: 0.414282 \tOP: 0.947368\tOR: 0.378947\tOF1: 0.541353\n",
      "Train Epoch: 7 [192/4307 (4%)]\tLoss: 0.411941 \tOP: 0.974359\tOR: 0.361905\tOF1: 0.527778\n",
      "Train Epoch: 7 [256/4307 (6%)]\tLoss: 0.386637 \tOP: 0.977273\tOR: 0.417476\tOF1: 0.585034\n",
      "Train Epoch: 7 [320/4307 (7%)]\tLoss: 0.423106 \tOP: 0.909091\tOR: 0.425532\tOF1: 0.579710\n",
      "Train Epoch: 7 [384/4307 (9%)]\tLoss: 0.384423 \tOP: 0.953488\tOR: 0.418367\tOF1: 0.581560\n",
      "Train Epoch: 7 [448/4307 (10%)]\tLoss: 0.383621 \tOP: 0.941176\tOR: 0.475248\tOF1: 0.631579\n",
      "Train Epoch: 7 [512/4307 (12%)]\tLoss: 0.409192 \tOP: 0.921053\tOR: 0.368421\tOF1: 0.526316\n",
      "Train Epoch: 7 [576/4307 (13%)]\tLoss: 0.368127 \tOP: 0.976190\tOR: 0.455556\tOF1: 0.621212\n",
      "Train Epoch: 7 [640/4307 (15%)]\tLoss: 0.359059 \tOP: 1.000000\tOR: 0.555556\tOF1: 0.714286\n",
      "Train Epoch: 7 [704/4307 (16%)]\tLoss: 0.362131 \tOP: 1.000000\tOR: 0.414141\tOF1: 0.585714\n",
      "Train Epoch: 7 [768/4307 (18%)]\tLoss: 0.408338 \tOP: 0.954545\tOR: 0.424242\tOF1: 0.587413\n",
      "Train Epoch: 7 [832/4307 (19%)]\tLoss: 0.401423 \tOP: 0.971429\tOR: 0.340000\tOF1: 0.503704\n",
      "Train Epoch: 7 [896/4307 (21%)]\tLoss: 0.413560 \tOP: 0.953488\tOR: 0.369369\tOF1: 0.532468\n",
      "Train Epoch: 7 [960/4307 (22%)]\tLoss: 0.416521 \tOP: 0.977273\tOR: 0.425743\tOF1: 0.593103\n",
      "Train Epoch: 7 [1024/4307 (24%)]\tLoss: 0.374783 \tOP: 0.980769\tOR: 0.531250\tOF1: 0.689189\n",
      "Train Epoch: 7 [1088/4307 (25%)]\tLoss: 0.384928 \tOP: 0.960000\tOR: 0.484848\tOF1: 0.644295\n",
      "Train Epoch: 7 [1152/4307 (26%)]\tLoss: 0.375245 \tOP: 0.962264\tOR: 0.490385\tOF1: 0.649682\n",
      "Train Epoch: 7 [1216/4307 (28%)]\tLoss: 0.413801 \tOP: 0.980000\tOR: 0.466667\tOF1: 0.632258\n",
      "Train Epoch: 7 [1280/4307 (29%)]\tLoss: 0.398928 \tOP: 0.936170\tOR: 0.463158\tOF1: 0.619718\n",
      "Train Epoch: 7 [1344/4307 (31%)]\tLoss: 0.386984 \tOP: 1.000000\tOR: 0.523810\tOF1: 0.687500\n",
      "Train Epoch: 7 [1408/4307 (32%)]\tLoss: 0.447932 \tOP: 0.888889\tOR: 0.326531\tOF1: 0.477612\n",
      "Train Epoch: 7 [1472/4307 (34%)]\tLoss: 0.393803 \tOP: 0.978723\tOR: 0.464646\tOF1: 0.630137\n",
      "Train Epoch: 7 [1536/4307 (35%)]\tLoss: 0.372306 \tOP: 1.000000\tOR: 0.490909\tOF1: 0.658537\n",
      "Train Epoch: 7 [1600/4307 (37%)]\tLoss: 0.343138 \tOP: 1.000000\tOR: 0.500000\tOF1: 0.666667\n",
      "Train Epoch: 7 [1664/4307 (38%)]\tLoss: 0.396949 \tOP: 0.952381\tOR: 0.416667\tOF1: 0.579710\n",
      "Train Epoch: 7 [1728/4307 (40%)]\tLoss: 0.396299 \tOP: 0.977778\tOR: 0.440000\tOF1: 0.606897\n",
      "Train Epoch: 7 [1792/4307 (41%)]\tLoss: 0.379368 \tOP: 1.000000\tOR: 0.524272\tOF1: 0.687898\n",
      "Train Epoch: 7 [1856/4307 (43%)]\tLoss: 0.427060 \tOP: 0.923077\tOR: 0.387097\tOF1: 0.545455\n",
      "Train Epoch: 7 [1920/4307 (44%)]\tLoss: 0.371903 \tOP: 0.954545\tOR: 0.400000\tOF1: 0.563758\n",
      "Train Epoch: 7 [1984/4307 (46%)]\tLoss: 0.398300 \tOP: 0.979167\tOR: 0.465347\tOF1: 0.630872\n",
      "Train Epoch: 7 [2048/4307 (47%)]\tLoss: 0.380960 \tOP: 0.976190\tOR: 0.427083\tOF1: 0.594203\n",
      "Train Epoch: 7 [2112/4307 (49%)]\tLoss: 0.367531 \tOP: 0.980392\tOR: 0.520833\tOF1: 0.680272\n",
      "Train Epoch: 7 [2176/4307 (50%)]\tLoss: 0.387870 \tOP: 0.959184\tOR: 0.451923\tOF1: 0.614379\n",
      "Train Epoch: 7 [2240/4307 (51%)]\tLoss: 0.395591 \tOP: 1.000000\tOR: 0.484848\tOF1: 0.653061\n",
      "Train Epoch: 7 [2304/4307 (53%)]\tLoss: 0.393106 \tOP: 0.981481\tOR: 0.514563\tOF1: 0.675159\n",
      "Train Epoch: 7 [2368/4307 (54%)]\tLoss: 0.382654 \tOP: 1.000000\tOR: 0.434783\tOF1: 0.606061\n",
      "Train Epoch: 7 [2432/4307 (56%)]\tLoss: 0.408322 \tOP: 0.960000\tOR: 0.461538\tOF1: 0.623377\n",
      "Train Epoch: 7 [2496/4307 (57%)]\tLoss: 0.372368 \tOP: 0.980392\tOR: 0.495050\tOF1: 0.657895\n",
      "Train Epoch: 7 [2560/4307 (59%)]\tLoss: 0.411479 \tOP: 0.940000\tOR: 0.465347\tOF1: 0.622517\n",
      "Train Epoch: 7 [2624/4307 (60%)]\tLoss: 0.419609 \tOP: 1.000000\tOR: 0.427273\tOF1: 0.598726\n",
      "Train Epoch: 7 [2688/4307 (62%)]\tLoss: 0.374260 \tOP: 0.980392\tOR: 0.520833\tOF1: 0.680272\n",
      "Train Epoch: 7 [2752/4307 (63%)]\tLoss: 0.393657 \tOP: 0.961538\tOR: 0.490196\tOF1: 0.649351\n",
      "Train Epoch: 7 [2816/4307 (65%)]\tLoss: 0.381416 \tOP: 1.000000\tOR: 0.463636\tOF1: 0.633540\n",
      "Train Epoch: 7 [2880/4307 (66%)]\tLoss: 0.404077 \tOP: 0.980392\tOR: 0.480769\tOF1: 0.645161\n",
      "Train Epoch: 7 [2944/4307 (68%)]\tLoss: 0.379224 \tOP: 0.976744\tOR: 0.432990\tOF1: 0.600000\n",
      "Train Epoch: 7 [3008/4307 (69%)]\tLoss: 0.411524 \tOP: 0.960784\tOR: 0.471154\tOF1: 0.632258\n",
      "Train Epoch: 7 [3072/4307 (71%)]\tLoss: 0.433782 \tOP: 0.933333\tOR: 0.424242\tOF1: 0.583333\n",
      "Train Epoch: 7 [3136/4307 (72%)]\tLoss: 0.416322 \tOP: 0.943396\tOR: 0.526316\tOF1: 0.675676\n",
      "Train Epoch: 7 [3200/4307 (74%)]\tLoss: 0.398239 \tOP: 1.000000\tOR: 0.407767\tOF1: 0.579310\n",
      "Train Epoch: 7 [3264/4307 (75%)]\tLoss: 0.357534 \tOP: 0.960784\tOR: 0.500000\tOF1: 0.657718\n",
      "Train Epoch: 7 [3328/4307 (76%)]\tLoss: 0.367210 \tOP: 1.000000\tOR: 0.490385\tOF1: 0.658065\n",
      "Train Epoch: 7 [3392/4307 (78%)]\tLoss: 0.390978 \tOP: 0.960784\tOR: 0.490000\tOF1: 0.649007\n",
      "Train Epoch: 7 [3456/4307 (79%)]\tLoss: 0.385099 \tOP: 0.930233\tOR: 0.449438\tOF1: 0.606061\n",
      "Train Epoch: 7 [3520/4307 (81%)]\tLoss: 0.426632 \tOP: 0.923077\tOR: 0.457143\tOF1: 0.611465\n",
      "Train Epoch: 7 [3584/4307 (82%)]\tLoss: 0.374814 \tOP: 0.909091\tOR: 0.490196\tOF1: 0.636943\n",
      "Train Epoch: 7 [3648/4307 (84%)]\tLoss: 0.413417 \tOP: 0.870370\tOR: 0.505376\tOF1: 0.639456\n",
      "Train Epoch: 7 [3712/4307 (85%)]\tLoss: 0.360710 \tOP: 0.956522\tOR: 0.453608\tOF1: 0.615385\n",
      "Train Epoch: 7 [3776/4307 (87%)]\tLoss: 0.389264 \tOP: 1.000000\tOR: 0.468750\tOF1: 0.638298\n",
      "Train Epoch: 7 [3840/4307 (88%)]\tLoss: 0.363161 \tOP: 0.968254\tOR: 0.544643\tOF1: 0.697143\n",
      "Train Epoch: 7 [3904/4307 (90%)]\tLoss: 0.388726 \tOP: 1.000000\tOR: 0.345794\tOF1: 0.513889\n",
      "Train Epoch: 7 [3968/4307 (91%)]\tLoss: 0.370262 \tOP: 0.980392\tOR: 0.526316\tOF1: 0.684932\n",
      "Train Epoch: 7 [4032/4307 (93%)]\tLoss: 0.408271 \tOP: 0.951220\tOR: 0.393939\tOF1: 0.557143\n",
      "Train Epoch: 7 [4096/4307 (94%)]\tLoss: 0.396309 \tOP: 0.921569\tOR: 0.522222\tOF1: 0.666667\n",
      "Train Epoch: 7 [4160/4307 (96%)]\tLoss: 0.371124 \tOP: 0.981481\tOR: 0.535354\tOF1: 0.692810\n",
      "Train Epoch: 7 [4224/4307 (97%)]\tLoss: 0.369968 \tOP: 1.000000\tOR: 0.473118\tOF1: 0.642336\n",
      "Train Epoch: 7 [1273/4307 (99%)]\tLoss: 0.362854 \tOP: 1.000000\tOR: 0.454545\tOF1: 0.625000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5266 \n",
      "OP: 0.730731\n",
      "OR: 0.398057\n",
      "OF1: 0.514107\n",
      "\n",
      "Train Epoch: 8 [0/4307 (0%)]\tLoss: 0.386096 \tOP: 0.961538\tOR: 0.490196\tOF1: 0.649351\n",
      "Train Epoch: 8 [64/4307 (1%)]\tLoss: 0.386131 \tOP: 0.936170\tOR: 0.453608\tOF1: 0.611111\n",
      "Train Epoch: 8 [128/4307 (3%)]\tLoss: 0.408590 \tOP: 0.977778\tOR: 0.427184\tOF1: 0.594595\n",
      "Train Epoch: 8 [192/4307 (4%)]\tLoss: 0.368347 \tOP: 1.000000\tOR: 0.552381\tOF1: 0.711656\n",
      "Train Epoch: 8 [256/4307 (6%)]\tLoss: 0.339826 \tOP: 1.000000\tOR: 0.495146\tOF1: 0.662338\n",
      "Train Epoch: 8 [320/4307 (7%)]\tLoss: 0.409517 \tOP: 0.972222\tOR: 0.380435\tOF1: 0.546875\n",
      "Train Epoch: 8 [384/4307 (9%)]\tLoss: 0.379422 \tOP: 1.000000\tOR: 0.438095\tOF1: 0.609272\n",
      "Train Epoch: 8 [448/4307 (10%)]\tLoss: 0.398402 \tOP: 0.937500\tOR: 0.454545\tOF1: 0.612245\n",
      "Train Epoch: 8 [512/4307 (12%)]\tLoss: 0.358829 \tOP: 0.982456\tOR: 0.577320\tOF1: 0.727273\n",
      "Train Epoch: 8 [576/4307 (13%)]\tLoss: 0.370669 \tOP: 0.977273\tOR: 0.452632\tOF1: 0.618705\n",
      "Train Epoch: 8 [640/4307 (15%)]\tLoss: 0.405760 \tOP: 0.980769\tOR: 0.510000\tOF1: 0.671053\n",
      "Train Epoch: 8 [704/4307 (16%)]\tLoss: 0.375998 \tOP: 1.000000\tOR: 0.466019\tOF1: 0.635762\n",
      "Train Epoch: 8 [768/4307 (18%)]\tLoss: 0.385325 \tOP: 0.957447\tOR: 0.468750\tOF1: 0.629371\n",
      "Train Epoch: 8 [832/4307 (19%)]\tLoss: 0.400932 \tOP: 0.975610\tOR: 0.425532\tOF1: 0.592593\n",
      "Train Epoch: 8 [896/4307 (21%)]\tLoss: 0.371881 \tOP: 1.000000\tOR: 0.454545\tOF1: 0.625000\n",
      "Train Epoch: 8 [960/4307 (22%)]\tLoss: 0.388251 \tOP: 0.957447\tOR: 0.478723\tOF1: 0.638298\n",
      "Train Epoch: 8 [1024/4307 (24%)]\tLoss: 0.401468 \tOP: 0.945455\tOR: 0.514851\tOF1: 0.666667\n",
      "Train Epoch: 8 [1088/4307 (25%)]\tLoss: 0.357829 \tOP: 1.000000\tOR: 0.472222\tOF1: 0.641509\n",
      "Train Epoch: 8 [1152/4307 (26%)]\tLoss: 0.383858 \tOP: 0.983051\tOR: 0.542056\tOF1: 0.698795\n",
      "Train Epoch: 8 [1216/4307 (28%)]\tLoss: 0.367190 \tOP: 1.000000\tOR: 0.510417\tOF1: 0.675862\n",
      "Train Epoch: 8 [1280/4307 (29%)]\tLoss: 0.368801 \tOP: 1.000000\tOR: 0.523810\tOF1: 0.687500\n",
      "Train Epoch: 8 [1344/4307 (31%)]\tLoss: 0.409695 \tOP: 0.980392\tOR: 0.515464\tOF1: 0.675676\n",
      "Train Epoch: 8 [1408/4307 (32%)]\tLoss: 0.385900 \tOP: 1.000000\tOR: 0.483516\tOF1: 0.651852\n",
      "Train Epoch: 8 [1472/4307 (34%)]\tLoss: 0.369213 \tOP: 1.000000\tOR: 0.490385\tOF1: 0.658065\n",
      "Train Epoch: 8 [1536/4307 (35%)]\tLoss: 0.429810 \tOP: 0.975610\tOR: 0.400000\tOF1: 0.567376\n",
      "Train Epoch: 8 [1600/4307 (37%)]\tLoss: 0.392240 \tOP: 0.947368\tOR: 0.378947\tOF1: 0.541353\n",
      "Train Epoch: 8 [1664/4307 (38%)]\tLoss: 0.368617 \tOP: 0.979592\tOR: 0.480000\tOF1: 0.644295\n",
      "Train Epoch: 8 [1728/4307 (40%)]\tLoss: 0.393631 \tOP: 0.958333\tOR: 0.494624\tOF1: 0.652482\n",
      "Train Epoch: 8 [1792/4307 (41%)]\tLoss: 0.370895 \tOP: 1.000000\tOR: 0.480769\tOF1: 0.649351\n",
      "Train Epoch: 8 [1856/4307 (43%)]\tLoss: 0.356613 \tOP: 0.981132\tOR: 0.520000\tOF1: 0.679739\n",
      "Train Epoch: 8 [1920/4307 (44%)]\tLoss: 0.358949 \tOP: 0.979167\tOR: 0.474747\tOF1: 0.639456\n",
      "Train Epoch: 8 [1984/4307 (46%)]\tLoss: 0.402173 \tOP: 0.977778\tOR: 0.444444\tOF1: 0.611111\n",
      "Train Epoch: 8 [2048/4307 (47%)]\tLoss: 0.385121 \tOP: 1.000000\tOR: 0.468750\tOF1: 0.638298\n",
      "Train Epoch: 8 [2112/4307 (49%)]\tLoss: 0.344322 \tOP: 0.980769\tOR: 0.495146\tOF1: 0.658065\n",
      "Train Epoch: 8 [2176/4307 (50%)]\tLoss: 0.347005 \tOP: 0.978261\tOR: 0.454545\tOF1: 0.620690\n",
      "Train Epoch: 8 [2240/4307 (51%)]\tLoss: 0.422838 \tOP: 0.978261\tOR: 0.441176\tOF1: 0.608108\n",
      "Train Epoch: 8 [2304/4307 (53%)]\tLoss: 0.338234 \tOP: 0.960784\tOR: 0.490000\tOF1: 0.649007\n",
      "Train Epoch: 8 [2368/4307 (54%)]\tLoss: 0.378637 \tOP: 0.959184\tOR: 0.460784\tOF1: 0.622517\n",
      "Train Epoch: 8 [2432/4307 (56%)]\tLoss: 0.386348 \tOP: 0.977778\tOR: 0.440000\tOF1: 0.606897\n",
      "Train Epoch: 8 [2496/4307 (57%)]\tLoss: 0.377319 \tOP: 0.949153\tOR: 0.543689\tOF1: 0.691358\n",
      "Train Epoch: 8 [2560/4307 (59%)]\tLoss: 0.359391 \tOP: 0.944444\tOR: 0.495146\tOF1: 0.649682\n",
      "Train Epoch: 8 [2624/4307 (60%)]\tLoss: 0.392117 \tOP: 0.978723\tOR: 0.442308\tOF1: 0.609272\n",
      "Train Epoch: 8 [2688/4307 (62%)]\tLoss: 0.385399 \tOP: 0.977778\tOR: 0.453608\tOF1: 0.619718\n",
      "Train Epoch: 8 [2752/4307 (63%)]\tLoss: 0.347097 \tOP: 0.981132\tOR: 0.490566\tOF1: 0.654088\n",
      "Train Epoch: 8 [2816/4307 (65%)]\tLoss: 0.383210 \tOP: 0.979167\tOR: 0.500000\tOF1: 0.661972\n",
      "Train Epoch: 8 [2880/4307 (66%)]\tLoss: 0.416871 \tOP: 0.918367\tOR: 0.445545\tOF1: 0.600000\n",
      "Train Epoch: 8 [2944/4307 (68%)]\tLoss: 0.374639 \tOP: 0.978261\tOR: 0.459184\tOF1: 0.625000\n",
      "Train Epoch: 8 [3008/4307 (69%)]\tLoss: 0.368082 \tOP: 1.000000\tOR: 0.490000\tOF1: 0.657718\n",
      "Train Epoch: 8 [3072/4307 (71%)]\tLoss: 0.386949 \tOP: 1.000000\tOR: 0.474747\tOF1: 0.643836\n",
      "Train Epoch: 8 [3136/4307 (72%)]\tLoss: 0.376443 \tOP: 0.978723\tOR: 0.474227\tOF1: 0.638889\n",
      "Train Epoch: 8 [3200/4307 (74%)]\tLoss: 0.375526 \tOP: 0.976744\tOR: 0.442105\tOF1: 0.608696\n",
      "Train Epoch: 8 [3264/4307 (75%)]\tLoss: 0.338783 \tOP: 0.981132\tOR: 0.520000\tOF1: 0.679739\n",
      "Train Epoch: 8 [3328/4307 (76%)]\tLoss: 0.395337 \tOP: 1.000000\tOR: 0.423423\tOF1: 0.594937\n",
      "Train Epoch: 8 [3392/4307 (78%)]\tLoss: 0.362522 \tOP: 1.000000\tOR: 0.435644\tOF1: 0.606897\n",
      "Train Epoch: 8 [3456/4307 (79%)]\tLoss: 0.383570 \tOP: 0.977273\tOR: 0.438776\tOF1: 0.605634\n",
      "Train Epoch: 8 [3520/4307 (81%)]\tLoss: 0.390988 \tOP: 0.977778\tOR: 0.427184\tOF1: 0.594595\n",
      "Train Epoch: 8 [3584/4307 (82%)]\tLoss: 0.367917 \tOP: 1.000000\tOR: 0.430000\tOF1: 0.601399\n",
      "Train Epoch: 8 [3648/4307 (84%)]\tLoss: 0.411691 \tOP: 0.962963\tOR: 0.514851\tOF1: 0.670968\n",
      "Train Epoch: 8 [3712/4307 (85%)]\tLoss: 0.388275 \tOP: 1.000000\tOR: 0.468750\tOF1: 0.638298\n",
      "Train Epoch: 8 [3776/4307 (87%)]\tLoss: 0.350686 \tOP: 1.000000\tOR: 0.480000\tOF1: 0.648649\n",
      "Train Epoch: 8 [3840/4307 (88%)]\tLoss: 0.376637 \tOP: 0.976744\tOR: 0.482759\tOF1: 0.646154\n",
      "Train Epoch: 8 [3904/4307 (90%)]\tLoss: 0.355998 \tOP: 1.000000\tOR: 0.489796\tOF1: 0.657534\n",
      "Train Epoch: 8 [3968/4307 (91%)]\tLoss: 0.391987 \tOP: 0.959184\tOR: 0.439252\tOF1: 0.602564\n",
      "Train Epoch: 8 [4032/4307 (93%)]\tLoss: 0.387788 \tOP: 0.953488\tOR: 0.436170\tOF1: 0.598540\n",
      "Train Epoch: 8 [4096/4307 (94%)]\tLoss: 0.400262 \tOP: 0.962264\tOR: 0.459459\tOF1: 0.621951\n",
      "Train Epoch: 8 [4160/4307 (96%)]\tLoss: 0.375015 \tOP: 0.981132\tOR: 0.536082\tOF1: 0.693333\n",
      "Train Epoch: 8 [4224/4307 (97%)]\tLoss: 0.408088 \tOP: 0.956522\tOR: 0.423077\tOF1: 0.586667\n",
      "Train Epoch: 8 [1273/4307 (99%)]\tLoss: 0.422859 \tOP: 0.900000\tOR: 0.321429\tOF1: 0.473684\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5755 \n",
      "OP: 0.631408\n",
      "OR: 0.553700\n",
      "OF1: 0.589596\n",
      "\n",
      "Train Epoch: 9 [0/4307 (0%)]\tLoss: 0.381273 \tOP: 1.000000\tOR: 0.532609\tOF1: 0.695035\n",
      "Train Epoch: 9 [64/4307 (1%)]\tLoss: 0.401267 \tOP: 1.000000\tOR: 0.450980\tOF1: 0.621622\n",
      "Train Epoch: 9 [128/4307 (3%)]\tLoss: 0.409852 \tOP: 1.000000\tOR: 0.390000\tOF1: 0.561151\n",
      "Train Epoch: 9 [192/4307 (4%)]\tLoss: 0.420623 \tOP: 0.959184\tOR: 0.456311\tOF1: 0.618421\n",
      "Train Epoch: 9 [256/4307 (6%)]\tLoss: 0.348367 \tOP: 1.000000\tOR: 0.505155\tOF1: 0.671233\n",
      "Train Epoch: 9 [320/4307 (7%)]\tLoss: 0.347339 \tOP: 1.000000\tOR: 0.452632\tOF1: 0.623188\n",
      "Train Epoch: 9 [384/4307 (9%)]\tLoss: 0.343305 \tOP: 0.960000\tOR: 0.484848\tOF1: 0.644295\n",
      "Train Epoch: 9 [448/4307 (10%)]\tLoss: 0.403347 \tOP: 0.951220\tOR: 0.402062\tOF1: 0.565217\n",
      "Train Epoch: 9 [512/4307 (12%)]\tLoss: 0.346382 \tOP: 1.000000\tOR: 0.466019\tOF1: 0.635762\n",
      "Train Epoch: 9 [576/4307 (13%)]\tLoss: 0.407891 \tOP: 0.955556\tOR: 0.421569\tOF1: 0.585034\n",
      "Train Epoch: 9 [640/4307 (15%)]\tLoss: 0.376421 \tOP: 1.000000\tOR: 0.504950\tOF1: 0.671053\n",
      "Train Epoch: 9 [704/4307 (16%)]\tLoss: 0.356172 \tOP: 1.000000\tOR: 0.422680\tOF1: 0.594203\n",
      "Train Epoch: 9 [768/4307 (18%)]\tLoss: 0.368052 \tOP: 0.976744\tOR: 0.456522\tOF1: 0.622222\n",
      "Train Epoch: 9 [832/4307 (19%)]\tLoss: 0.366719 \tOP: 1.000000\tOR: 0.571429\tOF1: 0.727273\n",
      "Train Epoch: 9 [896/4307 (21%)]\tLoss: 0.384418 \tOP: 0.978261\tOR: 0.483871\tOF1: 0.647482\n",
      "Train Epoch: 9 [960/4307 (22%)]\tLoss: 0.408816 \tOP: 0.977778\tOR: 0.431373\tOF1: 0.598639\n",
      "Train Epoch: 9 [1024/4307 (24%)]\tLoss: 0.336932 \tOP: 1.000000\tOR: 0.461538\tOF1: 0.631579\n",
      "Train Epoch: 9 [1088/4307 (25%)]\tLoss: 0.349782 \tOP: 0.977273\tOR: 0.457447\tOF1: 0.623188\n",
      "Train Epoch: 9 [1152/4307 (26%)]\tLoss: 0.403055 \tOP: 1.000000\tOR: 0.608247\tOF1: 0.756410\n",
      "Train Epoch: 9 [1216/4307 (28%)]\tLoss: 0.365213 \tOP: 1.000000\tOR: 0.462264\tOF1: 0.632258\n",
      "Train Epoch: 9 [1280/4307 (29%)]\tLoss: 0.360473 \tOP: 0.982143\tOR: 0.528846\tOF1: 0.687500\n",
      "Train Epoch: 9 [1344/4307 (31%)]\tLoss: 0.376055 \tOP: 0.978261\tOR: 0.432692\tOF1: 0.600000\n",
      "Train Epoch: 9 [1408/4307 (32%)]\tLoss: 0.364943 \tOP: 1.000000\tOR: 0.500000\tOF1: 0.666667\n",
      "Train Epoch: 9 [1472/4307 (34%)]\tLoss: 0.362433 \tOP: 1.000000\tOR: 0.451923\tOF1: 0.622517\n",
      "Train Epoch: 9 [1536/4307 (35%)]\tLoss: 0.397279 \tOP: 0.980000\tOR: 0.475728\tOF1: 0.640523\n",
      "Train Epoch: 9 [1600/4307 (37%)]\tLoss: 0.360636 \tOP: 0.977778\tOR: 0.423077\tOF1: 0.590604\n",
      "Train Epoch: 9 [1664/4307 (38%)]\tLoss: 0.347178 \tOP: 1.000000\tOR: 0.524390\tOF1: 0.688000\n",
      "Train Epoch: 9 [1728/4307 (40%)]\tLoss: 0.399812 \tOP: 1.000000\tOR: 0.458716\tOF1: 0.628931\n",
      "Train Epoch: 9 [1792/4307 (41%)]\tLoss: 0.393224 \tOP: 1.000000\tOR: 0.491071\tOF1: 0.658683\n",
      "Train Epoch: 9 [1856/4307 (43%)]\tLoss: 0.387491 \tOP: 1.000000\tOR: 0.443396\tOF1: 0.614379\n",
      "Train Epoch: 9 [1920/4307 (44%)]\tLoss: 0.382948 \tOP: 1.000000\tOR: 0.468750\tOF1: 0.638298\n",
      "Train Epoch: 9 [1984/4307 (46%)]\tLoss: 0.398292 \tOP: 0.945946\tOR: 0.364583\tOF1: 0.526316\n",
      "Train Epoch: 9 [2048/4307 (47%)]\tLoss: 0.382327 \tOP: 0.958333\tOR: 0.442308\tOF1: 0.605263\n",
      "Train Epoch: 9 [2112/4307 (49%)]\tLoss: 0.360401 \tOP: 1.000000\tOR: 0.535354\tOF1: 0.697368\n",
      "Train Epoch: 9 [2176/4307 (50%)]\tLoss: 0.347804 \tOP: 0.981481\tOR: 0.546392\tOF1: 0.701987\n",
      "Train Epoch: 9 [2240/4307 (51%)]\tLoss: 0.343695 \tOP: 1.000000\tOR: 0.560000\tOF1: 0.717949\n",
      "Train Epoch: 9 [2304/4307 (53%)]\tLoss: 0.375584 \tOP: 0.982759\tOR: 0.570000\tOF1: 0.721519\n",
      "Train Epoch: 9 [2368/4307 (54%)]\tLoss: 0.353334 \tOP: 0.980392\tOR: 0.476190\tOF1: 0.641026\n",
      "Train Epoch: 9 [2432/4307 (56%)]\tLoss: 0.387925 \tOP: 1.000000\tOR: 0.411215\tOF1: 0.582781\n",
      "Train Epoch: 9 [2496/4307 (57%)]\tLoss: 0.347429 \tOP: 1.000000\tOR: 0.400000\tOF1: 0.571429\n",
      "Train Epoch: 9 [2560/4307 (59%)]\tLoss: 0.402226 \tOP: 0.955556\tOR: 0.462366\tOF1: 0.623188\n",
      "Train Epoch: 9 [2624/4307 (60%)]\tLoss: 0.399714 \tOP: 1.000000\tOR: 0.403670\tOF1: 0.575163\n",
      "Train Epoch: 9 [2688/4307 (62%)]\tLoss: 0.315803 \tOP: 1.000000\tOR: 0.628571\tOF1: 0.771930\n",
      "Train Epoch: 9 [2752/4307 (63%)]\tLoss: 0.360899 \tOP: 1.000000\tOR: 0.500000\tOF1: 0.666667\n",
      "Train Epoch: 9 [2816/4307 (65%)]\tLoss: 0.350174 \tOP: 0.962963\tOR: 0.577778\tOF1: 0.722222\n",
      "Train Epoch: 9 [2880/4307 (66%)]\tLoss: 0.353759 \tOP: 0.983051\tOR: 0.517857\tOF1: 0.678363\n",
      "Train Epoch: 9 [2944/4307 (68%)]\tLoss: 0.365749 \tOP: 1.000000\tOR: 0.531915\tOF1: 0.694444\n",
      "Train Epoch: 9 [3008/4307 (69%)]\tLoss: 0.315783 \tOP: 1.000000\tOR: 0.537634\tOF1: 0.699301\n",
      "Train Epoch: 9 [3072/4307 (71%)]\tLoss: 0.345185 \tOP: 0.981132\tOR: 0.559140\tOF1: 0.712329\n",
      "Train Epoch: 9 [3136/4307 (72%)]\tLoss: 0.364206 \tOP: 0.981818\tOR: 0.519231\tOF1: 0.679245\n",
      "Train Epoch: 9 [3200/4307 (74%)]\tLoss: 0.361613 \tOP: 0.977273\tOR: 0.409524\tOF1: 0.577181\n",
      "Train Epoch: 9 [3264/4307 (75%)]\tLoss: 0.368617 \tOP: 0.980769\tOR: 0.515152\tOF1: 0.675497\n",
      "Train Epoch: 9 [3328/4307 (76%)]\tLoss: 0.401570 \tOP: 0.955556\tOR: 0.447917\tOF1: 0.609929\n",
      "Train Epoch: 9 [3392/4307 (78%)]\tLoss: 0.389461 \tOP: 0.929825\tOR: 0.546392\tOF1: 0.688312\n",
      "Train Epoch: 9 [3456/4307 (79%)]\tLoss: 0.377523 \tOP: 0.978261\tOR: 0.463918\tOF1: 0.629371\n",
      "Train Epoch: 9 [3520/4307 (81%)]\tLoss: 0.374596 \tOP: 1.000000\tOR: 0.412371\tOF1: 0.583942\n",
      "Train Epoch: 9 [3584/4307 (82%)]\tLoss: 0.389684 \tOP: 1.000000\tOR: 0.429825\tOF1: 0.601227\n",
      "Train Epoch: 9 [3648/4307 (84%)]\tLoss: 0.374930 \tOP: 1.000000\tOR: 0.460000\tOF1: 0.630137\n",
      "Train Epoch: 9 [3712/4307 (85%)]\tLoss: 0.378272 \tOP: 1.000000\tOR: 0.510204\tOF1: 0.675676\n",
      "Train Epoch: 9 [3776/4307 (87%)]\tLoss: 0.358073 \tOP: 1.000000\tOR: 0.455446\tOF1: 0.625850\n",
      "Train Epoch: 9 [3840/4307 (88%)]\tLoss: 0.356826 \tOP: 1.000000\tOR: 0.443396\tOF1: 0.614379\n",
      "Train Epoch: 9 [3904/4307 (90%)]\tLoss: 0.364759 \tOP: 1.000000\tOR: 0.552083\tOF1: 0.711409\n",
      "Train Epoch: 9 [3968/4307 (91%)]\tLoss: 0.393449 \tOP: 0.952381\tOR: 0.444444\tOF1: 0.606061\n",
      "Train Epoch: 9 [4032/4307 (93%)]\tLoss: 0.377628 \tOP: 0.960784\tOR: 0.532609\tOF1: 0.685315\n",
      "Train Epoch: 9 [4096/4307 (94%)]\tLoss: 0.349129 \tOP: 0.982143\tOR: 0.544554\tOF1: 0.700637\n",
      "Train Epoch: 9 [4160/4307 (96%)]\tLoss: 0.401615 \tOP: 1.000000\tOR: 0.436893\tOF1: 0.608108\n",
      "Train Epoch: 9 [4224/4307 (97%)]\tLoss: 0.342047 \tOP: 1.000000\tOR: 0.553398\tOF1: 0.712500\n",
      "Train Epoch: 9 [1273/4307 (99%)]\tLoss: 0.323631 \tOP: 1.000000\tOR: 0.500000\tOF1: 0.666667\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5599 \n",
      "OP: 0.660516\n",
      "OR: 0.484416\n",
      "OF1: 0.557747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model3.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model3(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        loss_lst_train3.append(loss.data.item())\n",
    "        OF1_lst_train3.append(OF1)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model3.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    OP_final = 0\n",
    "    OR_final = 0\n",
    "    OF1_final = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model3(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        OP_final += OP\n",
    "        OR_final += OR\n",
    "        OF1_final += OF1\n",
    "        \n",
    "    loss_lst_test3.append(test_loss.data.item()/i)\n",
    "    OF1_lst_test3.append(OF1_final/i)\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP_final/i, OR_final/i, OF1_final/i))\n",
    "\n",
    "loss_lst_train3 = []\n",
    "OF1_lst_train3 = []\n",
    "\n",
    "loss_lst_test3 = []\n",
    "OF1_lst_test3 = []    \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7,7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256*7*7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, len(classes)),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = CustomCNN()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model4.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-447663f93c68>:28: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4307 (0%)]\tLoss: 0.693548 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [64/4307 (1%)]\tLoss: 0.673958 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [128/4307 (3%)]\tLoss: 0.651217 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [192/4307 (4%)]\tLoss: 0.614288 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [256/4307 (6%)]\tLoss: 0.575148 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [320/4307 (7%)]\tLoss: 0.549834 \tOP: 0.606061\tOR: 0.198020\tOF1: 0.298507\n",
      "Train Epoch: 0 [384/4307 (9%)]\tLoss: 0.547184 \tOP: 0.606061\tOR: 0.396040\tOF1: 0.479042\n",
      "Train Epoch: 0 [448/4307 (10%)]\tLoss: 0.515406 \tOP: 0.632353\tOR: 0.462366\tOF1: 0.534161\n",
      "Train Epoch: 0 [512/4307 (12%)]\tLoss: 0.577373 \tOP: 0.544118\tOR: 0.389474\tOF1: 0.453988\n",
      "Train Epoch: 0 [576/4307 (13%)]\tLoss: 0.504035 \tOP: 0.655738\tOR: 0.430108\tOF1: 0.519481\n",
      "Train Epoch: 0 [640/4307 (15%)]\tLoss: 0.575263 \tOP: 0.517857\tOR: 0.284314\tOF1: 0.367089\n",
      "Train Epoch: 0 [704/4307 (16%)]\tLoss: 0.562592 \tOP: 0.357143\tOR: 0.101010\tOF1: 0.157480\n",
      "Train Epoch: 0 [768/4307 (18%)]\tLoss: 0.556728 \tOP: 0.166667\tOR: 0.009901\tOF1: 0.018692\n",
      "Train Epoch: 0 [832/4307 (19%)]\tLoss: 0.580609 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [896/4307 (21%)]\tLoss: 0.600277 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [960/4307 (22%)]\tLoss: 0.550424 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [1024/4307 (24%)]\tLoss: 0.553584 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [1088/4307 (25%)]\tLoss: 0.575691 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [1152/4307 (26%)]\tLoss: 0.579093 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [1216/4307 (28%)]\tLoss: 0.584008 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [1280/4307 (29%)]\tLoss: 0.561484 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [1344/4307 (31%)]\tLoss: 0.566006 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [1408/4307 (32%)]\tLoss: 0.573587 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [1472/4307 (34%)]\tLoss: 0.569815 \tOP: 0.333333\tOR: 0.019417\tOF1: 0.036697\n",
      "Train Epoch: 0 [1536/4307 (35%)]\tLoss: 0.552821 \tOP: 0.125000\tOR: 0.010000\tOF1: 0.018519\n",
      "Train Epoch: 0 [1600/4307 (37%)]\tLoss: 0.545445 \tOP: 0.125000\tOR: 0.010101\tOF1: 0.018692\n",
      "Train Epoch: 0 [1664/4307 (38%)]\tLoss: 0.544607 \tOP: 0.312500\tOR: 0.052632\tOF1: 0.090090\n",
      "Train Epoch: 0 [1728/4307 (40%)]\tLoss: 0.566915 \tOP: 0.400000\tOR: 0.102041\tOF1: 0.162602\n",
      "Train Epoch: 0 [1792/4307 (41%)]\tLoss: 0.583185 \tOP: 0.371429\tOR: 0.131313\tOF1: 0.194030\n",
      "Train Epoch: 0 [1856/4307 (43%)]\tLoss: 0.529163 \tOP: 0.451613\tOR: 0.153846\tOF1: 0.229508\n",
      "Train Epoch: 0 [1920/4307 (44%)]\tLoss: 0.569808 \tOP: 0.576923\tOR: 0.140187\tOF1: 0.225564\n",
      "Train Epoch: 0 [1984/4307 (46%)]\tLoss: 0.554940 \tOP: 0.416667\tOR: 0.100000\tOF1: 0.161290\n",
      "Train Epoch: 0 [2048/4307 (47%)]\tLoss: 0.561694 \tOP: 0.458333\tOR: 0.110000\tOF1: 0.177419\n",
      "Train Epoch: 0 [2112/4307 (49%)]\tLoss: 0.562425 \tOP: 0.454545\tOR: 0.098039\tOF1: 0.161290\n",
      "Train Epoch: 0 [2176/4307 (50%)]\tLoss: 0.567349 \tOP: 0.277778\tOR: 0.047619\tOF1: 0.081301\n",
      "Train Epoch: 0 [2240/4307 (51%)]\tLoss: 0.544698 \tOP: 0.533333\tOR: 0.074766\tOF1: 0.131148\n",
      "Train Epoch: 0 [2304/4307 (53%)]\tLoss: 0.552475 \tOP: 0.600000\tOR: 0.117647\tOF1: 0.196721\n",
      "Train Epoch: 0 [2368/4307 (54%)]\tLoss: 0.569462 \tOP: 0.450000\tOR: 0.087379\tOF1: 0.146341\n",
      "Train Epoch: 0 [2432/4307 (56%)]\tLoss: 0.562020 \tOP: 0.500000\tOR: 0.051546\tOF1: 0.093458\n",
      "Train Epoch: 0 [2496/4307 (57%)]\tLoss: 0.546518 \tOP: 0.461538\tOR: 0.060606\tOF1: 0.107143\n",
      "Train Epoch: 0 [2560/4307 (59%)]\tLoss: 0.522675 \tOP: 0.421053\tOR: 0.082474\tOF1: 0.137931\n",
      "Train Epoch: 0 [2624/4307 (60%)]\tLoss: 0.551200 \tOP: 0.500000\tOR: 0.128713\tOF1: 0.204724\n",
      "Train Epoch: 0 [2688/4307 (62%)]\tLoss: 0.560102 \tOP: 0.468750\tOR: 0.163043\tOF1: 0.241935\n",
      "Train Epoch: 0 [2752/4307 (63%)]\tLoss: 0.523860 \tOP: 0.545455\tOR: 0.183673\tOF1: 0.274809\n",
      "Train Epoch: 0 [2816/4307 (65%)]\tLoss: 0.597059 \tOP: 0.545455\tOR: 0.166667\tOF1: 0.255319\n",
      "Train Epoch: 0 [2880/4307 (66%)]\tLoss: 0.546224 \tOP: 0.681818\tOR: 0.147059\tOF1: 0.241935\n",
      "Train Epoch: 0 [2944/4307 (68%)]\tLoss: 0.532452 \tOP: 0.600000\tOR: 0.176471\tOF1: 0.272727\n",
      "Train Epoch: 0 [3008/4307 (69%)]\tLoss: 0.499194 \tOP: 0.680000\tOR: 0.168317\tOF1: 0.269841\n",
      "Train Epoch: 0 [3072/4307 (71%)]\tLoss: 0.523312 \tOP: 0.541667\tOR: 0.131313\tOF1: 0.211382\n",
      "Train Epoch: 0 [3136/4307 (72%)]\tLoss: 0.579521 \tOP: 0.586957\tOR: 0.254717\tOF1: 0.355263\n",
      "Train Epoch: 0 [3200/4307 (74%)]\tLoss: 0.534267 \tOP: 0.622642\tOR: 0.333333\tOF1: 0.434211\n",
      "Train Epoch: 0 [3264/4307 (75%)]\tLoss: 0.531324 \tOP: 0.614035\tOR: 0.343137\tOF1: 0.440252\n",
      "Train Epoch: 0 [3328/4307 (76%)]\tLoss: 0.523743 \tOP: 0.676923\tOR: 0.448980\tOF1: 0.539877\n",
      "Train Epoch: 0 [3392/4307 (78%)]\tLoss: 0.523229 \tOP: 0.641791\tOR: 0.430000\tOF1: 0.514970\n",
      "Train Epoch: 0 [3456/4307 (79%)]\tLoss: 0.567231 \tOP: 0.492537\tOR: 0.333333\tOF1: 0.397590\n",
      "Train Epoch: 0 [3520/4307 (81%)]\tLoss: 0.526543 \tOP: 0.602941\tOR: 0.414141\tOF1: 0.491018\n",
      "Train Epoch: 0 [3584/4307 (82%)]\tLoss: 0.490636 \tOP: 0.691176\tOR: 0.489583\tOF1: 0.573171\n",
      "Train Epoch: 0 [3648/4307 (84%)]\tLoss: 0.596741 \tOP: 0.470588\tOR: 0.307692\tOF1: 0.372093\n",
      "Train Epoch: 0 [3712/4307 (85%)]\tLoss: 0.557899 \tOP: 0.523810\tOR: 0.336735\tOF1: 0.409938\n",
      "Train Epoch: 0 [3776/4307 (87%)]\tLoss: 0.561704 \tOP: 0.614035\tOR: 0.339806\tOF1: 0.437500\n",
      "Train Epoch: 0 [3840/4307 (88%)]\tLoss: 0.535770 \tOP: 0.571429\tOR: 0.311111\tOF1: 0.402878\n",
      "Train Epoch: 0 [3904/4307 (90%)]\tLoss: 0.572433 \tOP: 0.428571\tOR: 0.116505\tOF1: 0.183206\n",
      "Train Epoch: 0 [3968/4307 (91%)]\tLoss: 0.515459 \tOP: 0.555556\tOR: 0.105263\tOF1: 0.176991\n",
      "Train Epoch: 0 [4032/4307 (93%)]\tLoss: 0.550765 \tOP: 0.363636\tOR: 0.038835\tOF1: 0.070175\n",
      "Train Epoch: 0 [4096/4307 (94%)]\tLoss: 0.552633 \tOP: 0.125000\tOR: 0.009804\tOF1: 0.018182\n",
      "Train Epoch: 0 [4160/4307 (96%)]\tLoss: 0.526910 \tOP: 0.583333\tOR: 0.069307\tOF1: 0.123894\n",
      "Train Epoch: 0 [4224/4307 (97%)]\tLoss: 0.554353 \tOP: 0.500000\tOR: 0.074468\tOF1: 0.129630\n",
      "Train Epoch: 0 [1273/4307 (99%)]\tLoss: 0.576452 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-33e25302e374>:41: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5366 \n",
      "OP: 0.481925\n",
      "OR: 0.062725\n",
      "OF1: 0.110734\n",
      "\n",
      "Train Epoch: 1 [0/4307 (0%)]\tLoss: 0.519318 \tOP: 0.600000\tOR: 0.087379\tOF1: 0.152542\n",
      "Train Epoch: 1 [64/4307 (1%)]\tLoss: 0.549761 \tOP: 0.470588\tOR: 0.079208\tOF1: 0.135593\n",
      "Train Epoch: 1 [128/4307 (3%)]\tLoss: 0.570125 \tOP: 0.368421\tOR: 0.068627\tOF1: 0.115702\n",
      "Train Epoch: 1 [192/4307 (4%)]\tLoss: 0.552091 \tOP: 0.363636\tOR: 0.080000\tOF1: 0.131148\n",
      "Train Epoch: 1 [256/4307 (6%)]\tLoss: 0.536003 \tOP: 0.363636\tOR: 0.086022\tOF1: 0.139130\n",
      "Train Epoch: 1 [320/4307 (7%)]\tLoss: 0.518619 \tOP: 0.733333\tOR: 0.106796\tOF1: 0.186441\n",
      "Train Epoch: 1 [384/4307 (9%)]\tLoss: 0.554349 \tOP: 0.461538\tOR: 0.058252\tOF1: 0.103448\n",
      "Train Epoch: 1 [448/4307 (10%)]\tLoss: 0.514676 \tOP: 0.562500\tOR: 0.092784\tOF1: 0.159292\n",
      "Train Epoch: 1 [512/4307 (12%)]\tLoss: 0.556097 \tOP: 0.733333\tOR: 0.213592\tOF1: 0.330827\n",
      "Train Epoch: 1 [576/4307 (13%)]\tLoss: 0.558576 \tOP: 0.526316\tOR: 0.096154\tOF1: 0.162602\n",
      "Train Epoch: 1 [640/4307 (15%)]\tLoss: 0.542550 \tOP: 0.600000\tOR: 0.153061\tOF1: 0.243902\n",
      "Train Epoch: 1 [704/4307 (16%)]\tLoss: 0.504782 \tOP: 0.627907\tOR: 0.296703\tOF1: 0.402985\n",
      "Train Epoch: 1 [768/4307 (18%)]\tLoss: 0.532021 \tOP: 0.500000\tOR: 0.252632\tOF1: 0.335664\n",
      "Train Epoch: 1 [832/4307 (19%)]\tLoss: 0.549730 \tOP: 0.521739\tOR: 0.247423\tOF1: 0.335664\n",
      "Train Epoch: 1 [896/4307 (21%)]\tLoss: 0.566118 \tOP: 0.456522\tOR: 0.210000\tOF1: 0.287671\n",
      "Train Epoch: 1 [960/4307 (22%)]\tLoss: 0.559332 \tOP: 0.543478\tOR: 0.252525\tOF1: 0.344828\n",
      "Train Epoch: 1 [1024/4307 (24%)]\tLoss: 0.543328 \tOP: 0.583333\tOR: 0.203883\tOF1: 0.302158\n",
      "Train Epoch: 1 [1088/4307 (25%)]\tLoss: 0.544382 \tOP: 0.600000\tOR: 0.180000\tOF1: 0.276923\n",
      "Train Epoch: 1 [1152/4307 (26%)]\tLoss: 0.570914 \tOP: 0.761905\tOR: 0.145455\tOF1: 0.244275\n",
      "Train Epoch: 1 [1216/4307 (28%)]\tLoss: 0.537492 \tOP: 0.642857\tOR: 0.083333\tOF1: 0.147541\n",
      "Train Epoch: 1 [1280/4307 (29%)]\tLoss: 0.514663 \tOP: 0.708333\tOR: 0.165049\tOF1: 0.267717\n",
      "Train Epoch: 1 [1344/4307 (31%)]\tLoss: 0.518709 \tOP: 0.583333\tOR: 0.145833\tOF1: 0.233333\n",
      "Train Epoch: 1 [1408/4307 (32%)]\tLoss: 0.544506 \tOP: 0.576923\tOR: 0.144231\tOF1: 0.230769\n",
      "Train Epoch: 1 [1472/4307 (34%)]\tLoss: 0.565413 \tOP: 0.321429\tOR: 0.094737\tOF1: 0.146341\n",
      "Train Epoch: 1 [1536/4307 (35%)]\tLoss: 0.566056 \tOP: 0.368421\tOR: 0.070707\tOF1: 0.118644\n",
      "Train Epoch: 1 [1600/4307 (37%)]\tLoss: 0.562724 \tOP: 0.566667\tOR: 0.184783\tOF1: 0.278689\n",
      "Train Epoch: 1 [1664/4307 (38%)]\tLoss: 0.559661 \tOP: 0.653846\tOR: 0.163462\tOF1: 0.261538\n",
      "Train Epoch: 1 [1728/4307 (40%)]\tLoss: 0.559041 \tOP: 0.620690\tOR: 0.173077\tOF1: 0.270677\n",
      "Train Epoch: 1 [1792/4307 (41%)]\tLoss: 0.520494 \tOP: 0.521739\tOR: 0.136364\tOF1: 0.216216\n",
      "Train Epoch: 1 [1856/4307 (43%)]\tLoss: 0.523376 \tOP: 0.500000\tOR: 0.135417\tOF1: 0.213115\n",
      "Train Epoch: 1 [1920/4307 (44%)]\tLoss: 0.503078 \tOP: 0.590909\tOR: 0.134021\tOF1: 0.218487\n",
      "Train Epoch: 1 [1984/4307 (46%)]\tLoss: 0.549926 \tOP: 0.444444\tOR: 0.122449\tOF1: 0.192000\n",
      "Train Epoch: 1 [2048/4307 (47%)]\tLoss: 0.530124 \tOP: 0.500000\tOR: 0.118812\tOF1: 0.192000\n",
      "Train Epoch: 1 [2112/4307 (49%)]\tLoss: 0.513590 \tOP: 0.615385\tOR: 0.164948\tOF1: 0.260163\n",
      "Train Epoch: 1 [2176/4307 (50%)]\tLoss: 0.542200 \tOP: 0.583333\tOR: 0.138614\tOF1: 0.224000\n",
      "Train Epoch: 1 [2240/4307 (51%)]\tLoss: 0.514771 \tOP: 0.482759\tOR: 0.145833\tOF1: 0.224000\n",
      "Train Epoch: 1 [2304/4307 (53%)]\tLoss: 0.540219 \tOP: 0.676471\tOR: 0.227723\tOF1: 0.340741\n",
      "Train Epoch: 1 [2368/4307 (54%)]\tLoss: 0.548460 \tOP: 0.552632\tOR: 0.218750\tOF1: 0.313433\n",
      "Train Epoch: 1 [2432/4307 (56%)]\tLoss: 0.501404 \tOP: 0.487805\tOR: 0.240964\tOF1: 0.322581\n",
      "Train Epoch: 1 [2496/4307 (57%)]\tLoss: 0.554274 \tOP: 0.424242\tOR: 0.138614\tOF1: 0.208955\n",
      "Train Epoch: 1 [2560/4307 (59%)]\tLoss: 0.531389 \tOP: 0.541667\tOR: 0.136842\tOF1: 0.218487\n",
      "Train Epoch: 1 [2624/4307 (60%)]\tLoss: 0.573854 \tOP: 0.375000\tOR: 0.061224\tOF1: 0.105263\n",
      "Train Epoch: 1 [2688/4307 (62%)]\tLoss: 0.524432 \tOP: 0.600000\tOR: 0.156250\tOF1: 0.247934\n",
      "Train Epoch: 1 [2752/4307 (63%)]\tLoss: 0.534693 \tOP: 0.588235\tOR: 0.097087\tOF1: 0.166667\n",
      "Train Epoch: 1 [2816/4307 (65%)]\tLoss: 0.546996 \tOP: 0.562500\tOR: 0.085714\tOF1: 0.148760\n",
      "Train Epoch: 1 [2880/4307 (66%)]\tLoss: 0.507624 \tOP: 0.777778\tOR: 0.142857\tOF1: 0.241379\n",
      "Train Epoch: 1 [2944/4307 (68%)]\tLoss: 0.527674 \tOP: 0.538462\tOR: 0.070000\tOF1: 0.123894\n",
      "Train Epoch: 1 [3008/4307 (69%)]\tLoss: 0.535130 \tOP: 0.523810\tOR: 0.104762\tOF1: 0.174603\n",
      "Train Epoch: 1 [3072/4307 (71%)]\tLoss: 0.553252 \tOP: 0.608696\tOR: 0.133333\tOF1: 0.218750\n",
      "Train Epoch: 1 [3136/4307 (72%)]\tLoss: 0.500895 \tOP: 0.722222\tOR: 0.260000\tOF1: 0.382353\n",
      "Train Epoch: 1 [3200/4307 (74%)]\tLoss: 0.542234 \tOP: 0.581395\tOR: 0.247525\tOF1: 0.347222\n",
      "Train Epoch: 1 [3264/4307 (75%)]\tLoss: 0.533127 \tOP: 0.608696\tOR: 0.277228\tOF1: 0.380952\n",
      "Train Epoch: 1 [3328/4307 (76%)]\tLoss: 0.494429 \tOP: 0.653061\tOR: 0.323232\tOF1: 0.432432\n",
      "Train Epoch: 1 [3392/4307 (78%)]\tLoss: 0.541700 \tOP: 0.576923\tOR: 0.300000\tOF1: 0.394737\n",
      "Train Epoch: 1 [3456/4307 (79%)]\tLoss: 0.517963 \tOP: 0.684211\tOR: 0.382353\tOF1: 0.490566\n",
      "Train Epoch: 1 [3520/4307 (81%)]\tLoss: 0.469067 \tOP: 0.759259\tOR: 0.401961\tOF1: 0.525641\n",
      "Train Epoch: 1 [3584/4307 (82%)]\tLoss: 0.540743 \tOP: 0.641791\tOR: 0.413462\tOF1: 0.502924\n",
      "Train Epoch: 1 [3648/4307 (84%)]\tLoss: 0.530147 \tOP: 0.585714\tOR: 0.410000\tOF1: 0.482353\n",
      "Train Epoch: 1 [3712/4307 (85%)]\tLoss: 0.530554 \tOP: 0.621622\tOR: 0.438095\tOF1: 0.513966\n",
      "Train Epoch: 1 [3776/4307 (87%)]\tLoss: 0.556513 \tOP: 0.614286\tOR: 0.425743\tOF1: 0.502924\n",
      "Train Epoch: 1 [3840/4307 (88%)]\tLoss: 0.537860 \tOP: 0.661290\tOR: 0.401961\tOF1: 0.500000\n",
      "Train Epoch: 1 [3904/4307 (90%)]\tLoss: 0.589258 \tOP: 0.550000\tOR: 0.317308\tOF1: 0.402439\n",
      "Train Epoch: 1 [3968/4307 (91%)]\tLoss: 0.560052 \tOP: 0.596491\tOR: 0.330097\tOF1: 0.425000\n",
      "Train Epoch: 1 [4032/4307 (93%)]\tLoss: 0.560475 \tOP: 0.520000\tOR: 0.273684\tOF1: 0.358621\n",
      "Train Epoch: 1 [4096/4307 (94%)]\tLoss: 0.550192 \tOP: 0.659574\tOR: 0.298077\tOF1: 0.410596\n",
      "Train Epoch: 1 [4160/4307 (96%)]\tLoss: 0.531526 \tOP: 0.615385\tOR: 0.252632\tOF1: 0.358209\n",
      "Train Epoch: 1 [4224/4307 (97%)]\tLoss: 0.554971 \tOP: 0.600000\tOR: 0.262136\tOF1: 0.364865\n",
      "Train Epoch: 1 [1273/4307 (99%)]\tLoss: 0.516386 \tOP: 0.588235\tOR: 0.333333\tOF1: 0.425532\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5343 \n",
      "OP: 0.616526\n",
      "OR: 0.284338\n",
      "OF1: 0.388354\n",
      "\n",
      "Train Epoch: 2 [0/4307 (0%)]\tLoss: 0.526964 \tOP: 0.659091\tOR: 0.295918\tOF1: 0.408451\n",
      "Train Epoch: 2 [64/4307 (1%)]\tLoss: 0.551966 \tOP: 0.604167\tOR: 0.278846\tOF1: 0.381579\n",
      "Train Epoch: 2 [128/4307 (3%)]\tLoss: 0.488586 \tOP: 0.583333\tOR: 0.294737\tOF1: 0.391608\n",
      "Train Epoch: 2 [192/4307 (4%)]\tLoss: 0.571519 \tOP: 0.520833\tOR: 0.247525\tOF1: 0.335570\n",
      "Train Epoch: 2 [256/4307 (6%)]\tLoss: 0.547312 \tOP: 0.685714\tOR: 0.228571\tOF1: 0.342857\n",
      "Train Epoch: 2 [320/4307 (7%)]\tLoss: 0.554046 \tOP: 0.696970\tOR: 0.221154\tOF1: 0.335766\n",
      "Train Epoch: 2 [384/4307 (9%)]\tLoss: 0.501935 \tOP: 0.655172\tOR: 0.191919\tOF1: 0.296875\n",
      "Train Epoch: 2 [448/4307 (10%)]\tLoss: 0.533806 \tOP: 0.633333\tOR: 0.193878\tOF1: 0.296875\n",
      "Train Epoch: 2 [512/4307 (12%)]\tLoss: 0.536952 \tOP: 0.580645\tOR: 0.189474\tOF1: 0.285714\n",
      "Train Epoch: 2 [576/4307 (13%)]\tLoss: 0.549527 \tOP: 0.513514\tOR: 0.186275\tOF1: 0.273381\n",
      "Train Epoch: 2 [640/4307 (15%)]\tLoss: 0.501462 \tOP: 0.615385\tOR: 0.168421\tOF1: 0.264463\n",
      "Train Epoch: 2 [704/4307 (16%)]\tLoss: 0.537703 \tOP: 0.518519\tOR: 0.147368\tOF1: 0.229508\n",
      "Train Epoch: 2 [768/4307 (18%)]\tLoss: 0.552521 \tOP: 0.566667\tOR: 0.168317\tOF1: 0.259542\n",
      "Train Epoch: 2 [832/4307 (19%)]\tLoss: 0.579691 \tOP: 0.583333\tOR: 0.200000\tOF1: 0.297872\n",
      "Train Epoch: 2 [896/4307 (21%)]\tLoss: 0.514193 \tOP: 0.631579\tOR: 0.255319\tOF1: 0.363636\n",
      "Train Epoch: 2 [960/4307 (22%)]\tLoss: 0.556659 \tOP: 0.589744\tOR: 0.232323\tOF1: 0.333333\n",
      "Train Epoch: 2 [1024/4307 (24%)]\tLoss: 0.523687 \tOP: 0.527778\tOR: 0.202128\tOF1: 0.292308\n",
      "Train Epoch: 2 [1088/4307 (25%)]\tLoss: 0.525380 \tOP: 0.590909\tOR: 0.282609\tOF1: 0.382353\n",
      "Train Epoch: 2 [1152/4307 (26%)]\tLoss: 0.585808 \tOP: 0.655172\tOR: 0.339286\tOF1: 0.447059\n",
      "Train Epoch: 2 [1216/4307 (28%)]\tLoss: 0.538953 \tOP: 0.480769\tOR: 0.277778\tOF1: 0.352113\n",
      "Train Epoch: 2 [1280/4307 (29%)]\tLoss: 0.509622 \tOP: 0.511111\tOR: 0.255556\tOF1: 0.340741\n",
      "Train Epoch: 2 [1344/4307 (31%)]\tLoss: 0.541358 \tOP: 0.512195\tOR: 0.221053\tOF1: 0.308824\n",
      "Train Epoch: 2 [1408/4307 (32%)]\tLoss: 0.513059 \tOP: 0.781250\tOR: 0.245098\tOF1: 0.373134\n",
      "Train Epoch: 2 [1472/4307 (34%)]\tLoss: 0.586566 \tOP: 0.468750\tOR: 0.140187\tOF1: 0.215827\n",
      "Train Epoch: 2 [1536/4307 (35%)]\tLoss: 0.610460 \tOP: 0.423077\tOR: 0.110000\tOF1: 0.174603\n",
      "Train Epoch: 2 [1600/4307 (37%)]\tLoss: 0.565789 \tOP: 0.473684\tOR: 0.092784\tOF1: 0.155172\n",
      "Train Epoch: 2 [1664/4307 (38%)]\tLoss: 0.552413 \tOP: 0.272727\tOR: 0.030000\tOF1: 0.054054\n",
      "Train Epoch: 2 [1728/4307 (40%)]\tLoss: 0.543887 \tOP: 0.250000\tOR: 0.020000\tOF1: 0.037037\n",
      "Train Epoch: 2 [1792/4307 (41%)]\tLoss: 0.548126 \tOP: 0.375000\tOR: 0.030928\tOF1: 0.057143\n",
      "Train Epoch: 2 [1856/4307 (43%)]\tLoss: 0.513512 \tOP: 0.285714\tOR: 0.020408\tOF1: 0.038095\n",
      "Train Epoch: 2 [1920/4307 (44%)]\tLoss: 0.511979 \tOP: 0.500000\tOR: 0.058252\tOF1: 0.104348\n",
      "Train Epoch: 2 [1984/4307 (46%)]\tLoss: 0.547120 \tOP: 0.733333\tOR: 0.106796\tOF1: 0.186441\n",
      "Train Epoch: 2 [2048/4307 (47%)]\tLoss: 0.555052 \tOP: 0.473684\tOR: 0.088235\tOF1: 0.148760\n",
      "Train Epoch: 2 [2112/4307 (49%)]\tLoss: 0.508662 \tOP: 0.481481\tOR: 0.138298\tOF1: 0.214876\n",
      "Train Epoch: 2 [2176/4307 (50%)]\tLoss: 0.534889 \tOP: 0.625000\tOR: 0.263158\tOF1: 0.370370\n",
      "Train Epoch: 2 [2240/4307 (51%)]\tLoss: 0.563541 \tOP: 0.574468\tOR: 0.272727\tOF1: 0.369863\n",
      "Train Epoch: 2 [2304/4307 (53%)]\tLoss: 0.544172 \tOP: 0.644444\tOR: 0.284314\tOF1: 0.394558\n",
      "Train Epoch: 2 [2368/4307 (54%)]\tLoss: 0.512772 \tOP: 0.657895\tOR: 0.252525\tOF1: 0.364964\n",
      "Train Epoch: 2 [2432/4307 (56%)]\tLoss: 0.528562 \tOP: 0.617647\tOR: 0.200000\tOF1: 0.302158\n",
      "Train Epoch: 2 [2496/4307 (57%)]\tLoss: 0.520445 \tOP: 0.583333\tOR: 0.221053\tOF1: 0.320611\n",
      "Train Epoch: 2 [2560/4307 (59%)]\tLoss: 0.548492 \tOP: 0.600000\tOR: 0.205882\tOF1: 0.306569\n",
      "Train Epoch: 2 [2624/4307 (60%)]\tLoss: 0.540483 \tOP: 0.611111\tOR: 0.220000\tOF1: 0.323529\n",
      "Train Epoch: 2 [2688/4307 (62%)]\tLoss: 0.570832 \tOP: 0.545455\tOR: 0.244898\tOF1: 0.338028\n",
      "Train Epoch: 2 [2752/4307 (63%)]\tLoss: 0.527078 \tOP: 0.655172\tOR: 0.184466\tOF1: 0.287879\n",
      "Train Epoch: 2 [2816/4307 (65%)]\tLoss: 0.554706 \tOP: 0.606061\tOR: 0.202020\tOF1: 0.303030\n",
      "Train Epoch: 2 [2880/4307 (66%)]\tLoss: 0.557990 \tOP: 0.481481\tOR: 0.128713\tOF1: 0.203125\n",
      "Train Epoch: 2 [2944/4307 (68%)]\tLoss: 0.497081 \tOP: 0.666667\tOR: 0.215054\tOF1: 0.325203\n",
      "Train Epoch: 2 [3008/4307 (69%)]\tLoss: 0.509346 \tOP: 0.700000\tOR: 0.133333\tOF1: 0.224000\n",
      "Train Epoch: 2 [3072/4307 (71%)]\tLoss: 0.557567 \tOP: 0.633333\tOR: 0.184466\tOF1: 0.285714\n",
      "Train Epoch: 2 [3136/4307 (72%)]\tLoss: 0.570494 \tOP: 0.617647\tOR: 0.194444\tOF1: 0.295775\n",
      "Train Epoch: 2 [3200/4307 (74%)]\tLoss: 0.568701 \tOP: 0.594595\tOR: 0.201835\tOF1: 0.301370\n",
      "Train Epoch: 2 [3264/4307 (75%)]\tLoss: 0.545809 \tOP: 0.666667\tOR: 0.178218\tOF1: 0.281250\n",
      "Train Epoch: 2 [3328/4307 (76%)]\tLoss: 0.547742 \tOP: 0.566667\tOR: 0.160377\tOF1: 0.250000\n",
      "Train Epoch: 2 [3392/4307 (78%)]\tLoss: 0.530489 \tOP: 0.689655\tOR: 0.188679\tOF1: 0.296296\n",
      "Train Epoch: 2 [3456/4307 (79%)]\tLoss: 0.552578 \tOP: 0.555556\tOR: 0.186916\tOF1: 0.279720\n",
      "Train Epoch: 2 [3520/4307 (81%)]\tLoss: 0.514662 \tOP: 0.697674\tOR: 0.322581\tOF1: 0.441176\n",
      "Train Epoch: 2 [3584/4307 (82%)]\tLoss: 0.522468 \tOP: 0.657895\tOR: 0.242718\tOF1: 0.354610\n",
      "Train Epoch: 2 [3648/4307 (84%)]\tLoss: 0.535081 \tOP: 0.720930\tOR: 0.298077\tOF1: 0.421769\n",
      "Train Epoch: 2 [3712/4307 (85%)]\tLoss: 0.529295 \tOP: 0.627907\tOR: 0.257143\tOF1: 0.364865\n",
      "Train Epoch: 2 [3776/4307 (87%)]\tLoss: 0.526843 \tOP: 0.557692\tOR: 0.311828\tOF1: 0.400000\n",
      "Train Epoch: 2 [3840/4307 (88%)]\tLoss: 0.522701 \tOP: 0.593220\tOR: 0.380435\tOF1: 0.463576\n",
      "Train Epoch: 2 [3904/4307 (90%)]\tLoss: 0.519572 \tOP: 0.649123\tOR: 0.355769\tOF1: 0.459627\n",
      "Train Epoch: 2 [3968/4307 (91%)]\tLoss: 0.533683 \tOP: 0.629032\tOR: 0.367925\tOF1: 0.464286\n",
      "Train Epoch: 2 [4032/4307 (93%)]\tLoss: 0.540304 \tOP: 0.627119\tOR: 0.370000\tOF1: 0.465409\n",
      "Train Epoch: 2 [4096/4307 (94%)]\tLoss: 0.494117 \tOP: 0.625000\tOR: 0.326087\tOF1: 0.428571\n",
      "Train Epoch: 2 [4160/4307 (96%)]\tLoss: 0.502862 \tOP: 0.638298\tOR: 0.303030\tOF1: 0.410959\n",
      "Train Epoch: 2 [4224/4307 (97%)]\tLoss: 0.512221 \tOP: 0.717391\tOR: 0.340206\tOF1: 0.461538\n",
      "Train Epoch: 2 [1273/4307 (99%)]\tLoss: 0.523513 \tOP: 0.555556\tOR: 0.357143\tOF1: 0.434783\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5297 \n",
      "OP: 0.612949\n",
      "OR: 0.250094\n",
      "OF1: 0.354130\n",
      "\n",
      "Train Epoch: 3 [0/4307 (0%)]\tLoss: 0.580756 \tOP: 0.657895\tOR: 0.225225\tOF1: 0.335570\n",
      "Train Epoch: 3 [64/4307 (1%)]\tLoss: 0.510786 \tOP: 0.687500\tOR: 0.224490\tOF1: 0.338462\n",
      "Train Epoch: 3 [128/4307 (3%)]\tLoss: 0.537907 \tOP: 0.483871\tOR: 0.151515\tOF1: 0.230769\n",
      "Train Epoch: 3 [192/4307 (4%)]\tLoss: 0.526693 \tOP: 0.694444\tOR: 0.231481\tOF1: 0.347222\n",
      "Train Epoch: 3 [256/4307 (6%)]\tLoss: 0.483415 \tOP: 0.736842\tOR: 0.264151\tOF1: 0.388889\n",
      "Train Epoch: 3 [320/4307 (7%)]\tLoss: 0.504731 \tOP: 0.730769\tOR: 0.197917\tOF1: 0.311475\n",
      "Train Epoch: 3 [384/4307 (9%)]\tLoss: 0.564347 \tOP: 0.589744\tOR: 0.216981\tOF1: 0.317241\n",
      "Train Epoch: 3 [448/4307 (10%)]\tLoss: 0.568886 \tOP: 0.611111\tOR: 0.300000\tOF1: 0.402439\n",
      "Train Epoch: 3 [512/4307 (12%)]\tLoss: 0.530291 \tOP: 0.608696\tOR: 0.291667\tOF1: 0.394366\n",
      "Train Epoch: 3 [576/4307 (13%)]\tLoss: 0.512924 \tOP: 0.604167\tOR: 0.308511\tOF1: 0.408451\n",
      "Train Epoch: 3 [640/4307 (15%)]\tLoss: 0.520032 \tOP: 0.627907\tOR: 0.275510\tOF1: 0.382979\n",
      "Train Epoch: 3 [704/4307 (16%)]\tLoss: 0.559218 \tOP: 0.512821\tOR: 0.200000\tOF1: 0.287770\n",
      "Train Epoch: 3 [768/4307 (18%)]\tLoss: 0.535243 \tOP: 0.588235\tOR: 0.183486\tOF1: 0.279720\n",
      "Train Epoch: 3 [832/4307 (19%)]\tLoss: 0.542109 \tOP: 0.677419\tOR: 0.201923\tOF1: 0.311111\n",
      "Train Epoch: 3 [896/4307 (21%)]\tLoss: 0.491520 \tOP: 0.678571\tOR: 0.202128\tOF1: 0.311475\n",
      "Train Epoch: 3 [960/4307 (22%)]\tLoss: 0.503411 \tOP: 0.750000\tOR: 0.200000\tOF1: 0.315789\n",
      "Train Epoch: 3 [1024/4307 (24%)]\tLoss: 0.511758 \tOP: 0.694444\tOR: 0.247525\tOF1: 0.364964\n",
      "Train Epoch: 3 [1088/4307 (25%)]\tLoss: 0.538640 \tOP: 0.783784\tOR: 0.266055\tOF1: 0.397260\n",
      "Train Epoch: 3 [1152/4307 (26%)]\tLoss: 0.514735 \tOP: 0.630435\tOR: 0.295918\tOF1: 0.402778\n",
      "Train Epoch: 3 [1216/4307 (28%)]\tLoss: 0.565980 \tOP: 0.625000\tOR: 0.272727\tOF1: 0.379747\n",
      "Train Epoch: 3 [1280/4307 (29%)]\tLoss: 0.535664 \tOP: 0.612245\tOR: 0.315789\tOF1: 0.416667\n",
      "Train Epoch: 3 [1344/4307 (31%)]\tLoss: 0.488609 \tOP: 0.655172\tOR: 0.404255\tOF1: 0.500000\n",
      "Train Epoch: 3 [1408/4307 (32%)]\tLoss: 0.608678 \tOP: 0.500000\tOR: 0.221154\tOF1: 0.306667\n",
      "Train Epoch: 3 [1472/4307 (34%)]\tLoss: 0.541483 \tOP: 0.613636\tOR: 0.281250\tOF1: 0.385714\n",
      "Train Epoch: 3 [1536/4307 (35%)]\tLoss: 0.543665 \tOP: 0.571429\tOR: 0.202020\tOF1: 0.298507\n",
      "Train Epoch: 3 [1600/4307 (37%)]\tLoss: 0.543385 \tOP: 0.589744\tOR: 0.223301\tOF1: 0.323944\n",
      "Train Epoch: 3 [1664/4307 (38%)]\tLoss: 0.525424 \tOP: 0.575758\tOR: 0.200000\tOF1: 0.296875\n",
      "Train Epoch: 3 [1728/4307 (40%)]\tLoss: 0.544667 \tOP: 0.518519\tOR: 0.148936\tOF1: 0.231405\n",
      "Train Epoch: 3 [1792/4307 (41%)]\tLoss: 0.562624 \tOP: 0.708333\tOR: 0.151786\tOF1: 0.250000\n",
      "Train Epoch: 3 [1856/4307 (43%)]\tLoss: 0.525072 \tOP: 0.538462\tOR: 0.140000\tOF1: 0.222222\n",
      "Train Epoch: 3 [1920/4307 (44%)]\tLoss: 0.536197 \tOP: 0.500000\tOR: 0.097087\tOF1: 0.162602\n",
      "Train Epoch: 3 [1984/4307 (46%)]\tLoss: 0.445472 \tOP: 0.781250\tOR: 0.268817\tOF1: 0.400000\n",
      "Train Epoch: 3 [2048/4307 (47%)]\tLoss: 0.536729 \tOP: 0.676471\tOR: 0.232323\tOF1: 0.345865\n",
      "Train Epoch: 3 [2112/4307 (49%)]\tLoss: 0.537083 \tOP: 0.666667\tOR: 0.262626\tOF1: 0.376812\n",
      "Train Epoch: 3 [2176/4307 (50%)]\tLoss: 0.550586 \tOP: 0.692308\tOR: 0.346154\tOF1: 0.461538\n",
      "Train Epoch: 3 [2240/4307 (51%)]\tLoss: 0.509677 \tOP: 0.653846\tOR: 0.357895\tOF1: 0.462585\n",
      "Train Epoch: 3 [2304/4307 (53%)]\tLoss: 0.521115 \tOP: 0.633333\tOR: 0.372549\tOF1: 0.469136\n",
      "Train Epoch: 3 [2368/4307 (54%)]\tLoss: 0.527307 \tOP: 0.607143\tOR: 0.346939\tOF1: 0.441558\n",
      "Train Epoch: 3 [2432/4307 (56%)]\tLoss: 0.524844 \tOP: 0.640000\tOR: 0.313725\tOF1: 0.421053\n",
      "Train Epoch: 3 [2496/4307 (57%)]\tLoss: 0.559217 \tOP: 0.540541\tOR: 0.196078\tOF1: 0.287770\n",
      "Train Epoch: 3 [2560/4307 (59%)]\tLoss: 0.541269 \tOP: 0.580645\tOR: 0.202247\tOF1: 0.300000\n",
      "Train Epoch: 3 [2624/4307 (60%)]\tLoss: 0.531390 \tOP: 0.640000\tOR: 0.152381\tOF1: 0.246154\n",
      "Train Epoch: 3 [2688/4307 (62%)]\tLoss: 0.500091 \tOP: 0.666667\tOR: 0.206897\tOF1: 0.315789\n",
      "Train Epoch: 3 [2752/4307 (63%)]\tLoss: 0.539881 \tOP: 0.700000\tOR: 0.214286\tOF1: 0.328125\n",
      "Train Epoch: 3 [2816/4307 (65%)]\tLoss: 0.571836 \tOP: 0.380952\tOR: 0.078431\tOF1: 0.130081\n",
      "Train Epoch: 3 [2880/4307 (66%)]\tLoss: 0.522045 \tOP: 0.791667\tOR: 0.184466\tOF1: 0.299213\n",
      "Train Epoch: 3 [2944/4307 (68%)]\tLoss: 0.537733 \tOP: 0.520000\tOR: 0.131313\tOF1: 0.209677\n",
      "Train Epoch: 3 [3008/4307 (69%)]\tLoss: 0.534946 \tOP: 0.545455\tOR: 0.123711\tOF1: 0.201681\n",
      "Train Epoch: 3 [3072/4307 (71%)]\tLoss: 0.555403 \tOP: 0.555556\tOR: 0.142857\tOF1: 0.227273\n",
      "Train Epoch: 3 [3136/4307 (72%)]\tLoss: 0.576608 \tOP: 0.607143\tOR: 0.161905\tOF1: 0.255639\n",
      "Train Epoch: 3 [3200/4307 (74%)]\tLoss: 0.511606 \tOP: 0.605263\tOR: 0.242105\tOF1: 0.345865\n",
      "Train Epoch: 3 [3264/4307 (75%)]\tLoss: 0.532623 \tOP: 0.588235\tOR: 0.212766\tOF1: 0.312500\n",
      "Train Epoch: 3 [3328/4307 (76%)]\tLoss: 0.535559 \tOP: 0.615385\tOR: 0.240000\tOF1: 0.345324\n",
      "Train Epoch: 3 [3392/4307 (78%)]\tLoss: 0.536442 \tOP: 0.600000\tOR: 0.203883\tOF1: 0.304348\n",
      "Train Epoch: 3 [3456/4307 (79%)]\tLoss: 0.537697 \tOP: 0.560976\tOR: 0.230000\tOF1: 0.326241\n",
      "Train Epoch: 3 [3520/4307 (81%)]\tLoss: 0.521174 \tOP: 0.552632\tOR: 0.233333\tOF1: 0.328125\n",
      "Train Epoch: 3 [3584/4307 (82%)]\tLoss: 0.555991 \tOP: 0.613636\tOR: 0.257143\tOF1: 0.362416\n",
      "Train Epoch: 3 [3648/4307 (84%)]\tLoss: 0.495898 \tOP: 0.535714\tOR: 0.161290\tOF1: 0.247934\n",
      "Train Epoch: 3 [3712/4307 (85%)]\tLoss: 0.538392 \tOP: 0.500000\tOR: 0.109890\tOF1: 0.180180\n",
      "Train Epoch: 3 [3776/4307 (87%)]\tLoss: 0.567916 \tOP: 0.571429\tOR: 0.116505\tOF1: 0.193548\n",
      "Train Epoch: 3 [3840/4307 (88%)]\tLoss: 0.489731 \tOP: 0.576923\tOR: 0.168539\tOF1: 0.260870\n",
      "Train Epoch: 3 [3904/4307 (90%)]\tLoss: 0.550377 \tOP: 0.608696\tOR: 0.135922\tOF1: 0.222222\n",
      "Train Epoch: 3 [3968/4307 (91%)]\tLoss: 0.510045 \tOP: 0.652174\tOR: 0.151515\tOF1: 0.245902\n",
      "Train Epoch: 3 [4032/4307 (93%)]\tLoss: 0.519234 \tOP: 0.692308\tOR: 0.089109\tOF1: 0.157895\n",
      "Train Epoch: 3 [4096/4307 (94%)]\tLoss: 0.564595 \tOP: 0.562500\tOR: 0.083333\tOF1: 0.145161\n",
      "Train Epoch: 3 [4160/4307 (96%)]\tLoss: 0.499737 \tOP: 0.476190\tOR: 0.111111\tOF1: 0.180180\n",
      "Train Epoch: 3 [4224/4307 (97%)]\tLoss: 0.520848 \tOP: 0.600000\tOR: 0.126316\tOF1: 0.208696\n",
      "Train Epoch: 3 [1273/4307 (99%)]\tLoss: 0.498566 \tOP: 0.538462\tOR: 0.250000\tOF1: 0.341463\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5263 \n",
      "OP: 0.629819\n",
      "OR: 0.184187\n",
      "OF1: 0.283614\n",
      "\n",
      "Train Epoch: 4 [0/4307 (0%)]\tLoss: 0.518367 \tOP: 0.656250\tOR: 0.221053\tOF1: 0.330709\n",
      "Train Epoch: 4 [64/4307 (1%)]\tLoss: 0.556585 \tOP: 0.685714\tOR: 0.233010\tOF1: 0.347826\n",
      "Train Epoch: 4 [128/4307 (3%)]\tLoss: 0.515940 \tOP: 0.682927\tOR: 0.269231\tOF1: 0.386207\n",
      "Train Epoch: 4 [192/4307 (4%)]\tLoss: 0.574611 \tOP: 0.583333\tOR: 0.266667\tOF1: 0.366013\n",
      "Train Epoch: 4 [256/4307 (6%)]\tLoss: 0.516697 \tOP: 0.531915\tOR: 0.265957\tOF1: 0.354610\n",
      "Train Epoch: 4 [320/4307 (7%)]\tLoss: 0.557530 \tOP: 0.695652\tOR: 0.293578\tOF1: 0.412903\n",
      "Train Epoch: 4 [384/4307 (9%)]\tLoss: 0.501566 \tOP: 0.755102\tOR: 0.352381\tOF1: 0.480519\n",
      "Train Epoch: 4 [448/4307 (10%)]\tLoss: 0.512555 \tOP: 0.707317\tOR: 0.290000\tOF1: 0.411348\n",
      "Train Epoch: 4 [512/4307 (12%)]\tLoss: 0.510816 \tOP: 0.700000\tOR: 0.350000\tOF1: 0.466667\n",
      "Train Epoch: 4 [576/4307 (13%)]\tLoss: 0.526878 \tOP: 0.673469\tOR: 0.320388\tOF1: 0.434211\n",
      "Train Epoch: 4 [640/4307 (15%)]\tLoss: 0.503723 \tOP: 0.711111\tOR: 0.323232\tOF1: 0.444444\n",
      "Train Epoch: 4 [704/4307 (16%)]\tLoss: 0.509025 \tOP: 0.614035\tOR: 0.372340\tOF1: 0.463576\n",
      "Train Epoch: 4 [768/4307 (18%)]\tLoss: 0.531047 \tOP: 0.684211\tOR: 0.364486\tOF1: 0.475610\n",
      "Train Epoch: 4 [832/4307 (19%)]\tLoss: 0.473288 \tOP: 0.716981\tOR: 0.408602\tOF1: 0.520548\n",
      "Train Epoch: 4 [896/4307 (21%)]\tLoss: 0.535834 \tOP: 0.510638\tOR: 0.250000\tOF1: 0.335664\n",
      "Train Epoch: 4 [960/4307 (22%)]\tLoss: 0.539523 \tOP: 0.698113\tOR: 0.336364\tOF1: 0.453988\n",
      "Train Epoch: 4 [1024/4307 (24%)]\tLoss: 0.500162 \tOP: 0.630435\tOR: 0.292929\tOF1: 0.400000\n",
      "Train Epoch: 4 [1088/4307 (25%)]\tLoss: 0.588402 \tOP: 0.540000\tOR: 0.259615\tOF1: 0.350649\n",
      "Train Epoch: 4 [1152/4307 (26%)]\tLoss: 0.556534 \tOP: 0.600000\tOR: 0.228571\tOF1: 0.331034\n",
      "Train Epoch: 4 [1216/4307 (28%)]\tLoss: 0.522016 \tOP: 0.666667\tOR: 0.231579\tOF1: 0.343750\n",
      "Train Epoch: 4 [1280/4307 (29%)]\tLoss: 0.537054 \tOP: 0.609756\tOR: 0.242718\tOF1: 0.347222\n",
      "Train Epoch: 4 [1344/4307 (31%)]\tLoss: 0.545481 \tOP: 0.634146\tOR: 0.268041\tOF1: 0.376812\n",
      "Train Epoch: 4 [1408/4307 (32%)]\tLoss: 0.564194 \tOP: 0.558824\tOR: 0.186275\tOF1: 0.279412\n",
      "Train Epoch: 4 [1472/4307 (34%)]\tLoss: 0.538402 \tOP: 0.655172\tOR: 0.184466\tOF1: 0.287879\n",
      "Train Epoch: 4 [1536/4307 (35%)]\tLoss: 0.538355 \tOP: 0.589744\tOR: 0.232323\tOF1: 0.333333\n",
      "Train Epoch: 4 [1600/4307 (37%)]\tLoss: 0.522752 \tOP: 0.727273\tOR: 0.152381\tOF1: 0.251969\n",
      "Train Epoch: 4 [1664/4307 (38%)]\tLoss: 0.537162 \tOP: 0.677419\tOR: 0.216495\tOF1: 0.328125\n",
      "Train Epoch: 4 [1728/4307 (40%)]\tLoss: 0.497716 \tOP: 0.642857\tOR: 0.195652\tOF1: 0.300000\n",
      "Train Epoch: 4 [1792/4307 (41%)]\tLoss: 0.528190 \tOP: 0.655172\tOR: 0.202128\tOF1: 0.308943\n",
      "Train Epoch: 4 [1856/4307 (43%)]\tLoss: 0.513302 \tOP: 0.656250\tOR: 0.216495\tOF1: 0.325581\n",
      "Train Epoch: 4 [1920/4307 (44%)]\tLoss: 0.466054 \tOP: 0.684211\tOR: 0.276596\tOF1: 0.393939\n",
      "Train Epoch: 4 [1984/4307 (46%)]\tLoss: 0.566938 \tOP: 0.702128\tOR: 0.302752\tOF1: 0.423077\n",
      "Train Epoch: 4 [2048/4307 (47%)]\tLoss: 0.530008 \tOP: 0.521739\tOR: 0.275862\tOF1: 0.360902\n",
      "Train Epoch: 4 [2112/4307 (49%)]\tLoss: 0.519230 \tOP: 0.613636\tOR: 0.296703\tOF1: 0.400000\n",
      "Train Epoch: 4 [2176/4307 (50%)]\tLoss: 0.519251 \tOP: 0.604651\tOR: 0.260000\tOF1: 0.363636\n",
      "Train Epoch: 4 [2240/4307 (51%)]\tLoss: 0.533247 \tOP: 0.702703\tOR: 0.247619\tOF1: 0.366197\n",
      "Train Epoch: 4 [2304/4307 (53%)]\tLoss: 0.527183 \tOP: 0.625000\tOR: 0.268817\tOF1: 0.375940\n",
      "Train Epoch: 4 [2368/4307 (54%)]\tLoss: 0.548031 \tOP: 0.794118\tOR: 0.247706\tOF1: 0.377622\n",
      "Train Epoch: 4 [2432/4307 (56%)]\tLoss: 0.541240 \tOP: 0.680000\tOR: 0.155963\tOF1: 0.253731\n",
      "Train Epoch: 4 [2496/4307 (57%)]\tLoss: 0.490970 \tOP: 0.750000\tOR: 0.255319\tOF1: 0.380952\n",
      "Train Epoch: 4 [2560/4307 (59%)]\tLoss: 0.487394 \tOP: 0.678571\tOR: 0.200000\tOF1: 0.308943\n",
      "Train Epoch: 4 [2624/4307 (60%)]\tLoss: 0.495074 \tOP: 0.800000\tOR: 0.196078\tOF1: 0.314961\n",
      "Train Epoch: 4 [2688/4307 (62%)]\tLoss: 0.554585 \tOP: 0.567568\tOR: 0.207921\tOF1: 0.304348\n",
      "Train Epoch: 4 [2752/4307 (63%)]\tLoss: 0.530678 \tOP: 0.714286\tOR: 0.283019\tOF1: 0.405405\n",
      "Train Epoch: 4 [2816/4307 (65%)]\tLoss: 0.524716 \tOP: 0.583333\tOR: 0.214286\tOF1: 0.313433\n",
      "Train Epoch: 4 [2880/4307 (66%)]\tLoss: 0.514030 \tOP: 0.675676\tOR: 0.257732\tOF1: 0.373134\n",
      "Train Epoch: 4 [2944/4307 (68%)]\tLoss: 0.571617 \tOP: 0.711111\tOR: 0.290909\tOF1: 0.412903\n",
      "Train Epoch: 4 [3008/4307 (69%)]\tLoss: 0.577048 \tOP: 0.608696\tOR: 0.261682\tOF1: 0.366013\n",
      "Train Epoch: 4 [3072/4307 (71%)]\tLoss: 0.551099 \tOP: 0.520833\tOR: 0.250000\tOF1: 0.337838\n",
      "Train Epoch: 4 [3136/4307 (72%)]\tLoss: 0.549382 \tOP: 0.588235\tOR: 0.300000\tOF1: 0.397351\n",
      "Train Epoch: 4 [3200/4307 (74%)]\tLoss: 0.518956 \tOP: 0.648148\tOR: 0.372340\tOF1: 0.472973\n",
      "Train Epoch: 4 [3264/4307 (75%)]\tLoss: 0.551517 \tOP: 0.595238\tOR: 0.255102\tOF1: 0.357143\n",
      "Train Epoch: 4 [3328/4307 (76%)]\tLoss: 0.518485 \tOP: 0.738095\tOR: 0.292453\tOF1: 0.418919\n",
      "Train Epoch: 4 [3392/4307 (78%)]\tLoss: 0.501833 \tOP: 0.651163\tOR: 0.301075\tOF1: 0.411765\n",
      "Train Epoch: 4 [3456/4307 (79%)]\tLoss: 0.516972 \tOP: 0.604651\tOR: 0.273684\tOF1: 0.376812\n",
      "Train Epoch: 4 [3520/4307 (81%)]\tLoss: 0.510996 \tOP: 0.666667\tOR: 0.315789\tOF1: 0.428571\n",
      "Train Epoch: 4 [3584/4307 (82%)]\tLoss: 0.514822 \tOP: 0.642857\tOR: 0.259615\tOF1: 0.369863\n",
      "Train Epoch: 4 [3648/4307 (84%)]\tLoss: 0.527125 \tOP: 0.636364\tOR: 0.288660\tOF1: 0.397163\n",
      "Train Epoch: 4 [3712/4307 (85%)]\tLoss: 0.522459 \tOP: 0.657895\tOR: 0.255102\tOF1: 0.367647\n",
      "Train Epoch: 4 [3776/4307 (87%)]\tLoss: 0.535259 \tOP: 0.688889\tOR: 0.319588\tOF1: 0.436620\n",
      "Train Epoch: 4 [3840/4307 (88%)]\tLoss: 0.488476 \tOP: 0.688889\tOR: 0.322917\tOF1: 0.439716\n",
      "Train Epoch: 4 [3904/4307 (90%)]\tLoss: 0.516954 \tOP: 0.604167\tOR: 0.295918\tOF1: 0.397260\n",
      "Train Epoch: 4 [3968/4307 (91%)]\tLoss: 0.472555 \tOP: 0.761905\tOR: 0.344086\tOF1: 0.474074\n",
      "Train Epoch: 4 [4032/4307 (93%)]\tLoss: 0.582563 \tOP: 0.575000\tOR: 0.214953\tOF1: 0.312925\n",
      "Train Epoch: 4 [4096/4307 (94%)]\tLoss: 0.522100 \tOP: 0.666667\tOR: 0.164948\tOF1: 0.264463\n",
      "Train Epoch: 4 [4160/4307 (96%)]\tLoss: 0.525160 \tOP: 0.640000\tOR: 0.160000\tOF1: 0.256000\n",
      "Train Epoch: 4 [4224/4307 (97%)]\tLoss: 0.542838 \tOP: 0.741935\tOR: 0.223301\tOF1: 0.343284\n",
      "Train Epoch: 4 [1273/4307 (99%)]\tLoss: 0.533388 \tOP: 0.375000\tOR: 0.096774\tOF1: 0.153846\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5273 \n",
      "OP: 0.631103\n",
      "OR: 0.168328\n",
      "OF1: 0.264725\n",
      "\n",
      "Train Epoch: 5 [0/4307 (0%)]\tLoss: 0.578668 \tOP: 0.555556\tOR: 0.138889\tOF1: 0.222222\n",
      "Train Epoch: 5 [64/4307 (1%)]\tLoss: 0.544685 \tOP: 0.677419\tOR: 0.187500\tOF1: 0.293706\n",
      "Train Epoch: 5 [128/4307 (3%)]\tLoss: 0.504442 \tOP: 0.657143\tOR: 0.237113\tOF1: 0.348485\n",
      "Train Epoch: 5 [192/4307 (4%)]\tLoss: 0.525651 \tOP: 0.636364\tOR: 0.274510\tOF1: 0.383562\n",
      "Train Epoch: 5 [256/4307 (6%)]\tLoss: 0.496437 \tOP: 0.767442\tOR: 0.326733\tOF1: 0.458333\n",
      "Train Epoch: 5 [320/4307 (7%)]\tLoss: 0.524219 \tOP: 0.557377\tOR: 0.343434\tOF1: 0.425000\n",
      "Train Epoch: 5 [384/4307 (9%)]\tLoss: 0.539401 \tOP: 0.586207\tOR: 0.343434\tOF1: 0.433121\n",
      "Train Epoch: 5 [448/4307 (10%)]\tLoss: 0.512498 \tOP: 0.732143\tOR: 0.386792\tOF1: 0.506173\n",
      "Train Epoch: 5 [512/4307 (12%)]\tLoss: 0.532452 \tOP: 0.612245\tOR: 0.303030\tOF1: 0.405405\n",
      "Train Epoch: 5 [576/4307 (13%)]\tLoss: 0.562986 \tOP: 0.596154\tOR: 0.289720\tOF1: 0.389937\n",
      "Train Epoch: 5 [640/4307 (15%)]\tLoss: 0.566259 \tOP: 0.627451\tOR: 0.310680\tOF1: 0.415584\n",
      "Train Epoch: 5 [704/4307 (16%)]\tLoss: 0.550552 \tOP: 0.542857\tOR: 0.190000\tOF1: 0.281481\n",
      "Train Epoch: 5 [768/4307 (18%)]\tLoss: 0.524271 \tOP: 0.682927\tOR: 0.266667\tOF1: 0.383562\n",
      "Train Epoch: 5 [832/4307 (19%)]\tLoss: 0.530821 \tOP: 0.666667\tOR: 0.192308\tOF1: 0.298507\n",
      "Train Epoch: 5 [896/4307 (21%)]\tLoss: 0.559544 \tOP: 0.566667\tOR: 0.163462\tOF1: 0.253731\n",
      "Train Epoch: 5 [960/4307 (22%)]\tLoss: 0.490308 \tOP: 0.743590\tOR: 0.308511\tOF1: 0.436090\n",
      "Train Epoch: 5 [1024/4307 (24%)]\tLoss: 0.540066 \tOP: 0.593750\tOR: 0.177570\tOF1: 0.273381\n",
      "Train Epoch: 5 [1088/4307 (25%)]\tLoss: 0.521458 \tOP: 0.611111\tOR: 0.211538\tOF1: 0.314286\n",
      "Train Epoch: 5 [1152/4307 (26%)]\tLoss: 0.517483 \tOP: 0.628571\tOR: 0.220000\tOF1: 0.325926\n",
      "Train Epoch: 5 [1216/4307 (28%)]\tLoss: 0.534516 \tOP: 0.657895\tOR: 0.240385\tOF1: 0.352113\n",
      "Train Epoch: 5 [1280/4307 (29%)]\tLoss: 0.571822 \tOP: 0.517857\tOR: 0.281553\tOF1: 0.364780\n",
      "Train Epoch: 5 [1344/4307 (31%)]\tLoss: 0.479109 \tOP: 0.659574\tOR: 0.333333\tOF1: 0.442857\n",
      "Train Epoch: 5 [1408/4307 (32%)]\tLoss: 0.493606 \tOP: 0.709091\tOR: 0.406250\tOF1: 0.516556\n",
      "Train Epoch: 5 [1472/4307 (34%)]\tLoss: 0.521501 \tOP: 0.660377\tOR: 0.357143\tOF1: 0.463576\n",
      "Train Epoch: 5 [1536/4307 (35%)]\tLoss: 0.532106 \tOP: 0.631579\tOR: 0.367347\tOF1: 0.464516\n",
      "Train Epoch: 5 [1600/4307 (37%)]\tLoss: 0.483645 \tOP: 0.648148\tOR: 0.353535\tOF1: 0.457516\n",
      "Train Epoch: 5 [1664/4307 (38%)]\tLoss: 0.476649 \tOP: 0.729167\tOR: 0.372340\tOF1: 0.492958\n",
      "Train Epoch: 5 [1728/4307 (40%)]\tLoss: 0.484533 \tOP: 0.750000\tOR: 0.410526\tOF1: 0.530612\n",
      "Train Epoch: 5 [1792/4307 (41%)]\tLoss: 0.539145 \tOP: 0.673469\tOR: 0.326733\tOF1: 0.440000\n",
      "Train Epoch: 5 [1856/4307 (43%)]\tLoss: 0.478535 \tOP: 0.714286\tOR: 0.376344\tOF1: 0.492958\n",
      "Train Epoch: 5 [1920/4307 (44%)]\tLoss: 0.506210 \tOP: 0.634146\tOR: 0.254902\tOF1: 0.363636\n",
      "Train Epoch: 5 [1984/4307 (46%)]\tLoss: 0.513735 \tOP: 0.736842\tOR: 0.294737\tOF1: 0.421053\n",
      "Train Epoch: 5 [2048/4307 (47%)]\tLoss: 0.603270 \tOP: 0.513514\tOR: 0.184466\tOF1: 0.271429\n",
      "Train Epoch: 5 [2112/4307 (49%)]\tLoss: 0.499898 \tOP: 0.600000\tOR: 0.255319\tOF1: 0.358209\n",
      "Train Epoch: 5 [2176/4307 (50%)]\tLoss: 0.511439 \tOP: 0.675676\tOR: 0.255102\tOF1: 0.370370\n",
      "Train Epoch: 5 [2240/4307 (51%)]\tLoss: 0.483314 \tOP: 0.733333\tOR: 0.217822\tOF1: 0.335878\n",
      "Train Epoch: 5 [2304/4307 (53%)]\tLoss: 0.504995 \tOP: 0.666667\tOR: 0.204545\tOF1: 0.313043\n",
      "Train Epoch: 5 [2368/4307 (54%)]\tLoss: 0.507562 \tOP: 0.647059\tOR: 0.226804\tOF1: 0.335878\n",
      "Train Epoch: 5 [2432/4307 (56%)]\tLoss: 0.577529 \tOP: 0.676471\tOR: 0.221154\tOF1: 0.333333\n",
      "Train Epoch: 5 [2496/4307 (57%)]\tLoss: 0.466553 \tOP: 0.736842\tOR: 0.301075\tOF1: 0.427481\n",
      "Train Epoch: 5 [2560/4307 (59%)]\tLoss: 0.530896 \tOP: 0.612903\tOR: 0.190000\tOF1: 0.290076\n",
      "Train Epoch: 5 [2624/4307 (60%)]\tLoss: 0.504621 \tOP: 0.738095\tOR: 0.303922\tOF1: 0.430556\n",
      "Train Epoch: 5 [2688/4307 (62%)]\tLoss: 0.560423 \tOP: 0.575758\tOR: 0.182692\tOF1: 0.277372\n",
      "Train Epoch: 5 [2752/4307 (63%)]\tLoss: 0.557409 \tOP: 0.625000\tOR: 0.233645\tOF1: 0.340136\n",
      "Train Epoch: 5 [2816/4307 (65%)]\tLoss: 0.518436 \tOP: 0.657895\tOR: 0.233645\tOF1: 0.344828\n",
      "Train Epoch: 5 [2880/4307 (66%)]\tLoss: 0.488863 \tOP: 0.725000\tOR: 0.302083\tOF1: 0.426471\n",
      "Train Epoch: 5 [2944/4307 (68%)]\tLoss: 0.456722 \tOP: 0.700000\tOR: 0.307692\tOF1: 0.427481\n",
      "Train Epoch: 5 [3008/4307 (69%)]\tLoss: 0.520263 \tOP: 0.581395\tOR: 0.263158\tOF1: 0.362319\n",
      "Train Epoch: 5 [3072/4307 (71%)]\tLoss: 0.495213 \tOP: 0.729167\tOR: 0.360825\tOF1: 0.482759\n",
      "Train Epoch: 5 [3136/4307 (72%)]\tLoss: 0.488024 \tOP: 0.706897\tOR: 0.431579\tOF1: 0.535948\n",
      "Train Epoch: 5 [3200/4307 (74%)]\tLoss: 0.571555 \tOP: 0.560000\tOR: 0.280000\tOF1: 0.373333\n",
      "Train Epoch: 5 [3264/4307 (75%)]\tLoss: 0.552730 \tOP: 0.521739\tOR: 0.244898\tOF1: 0.333333\n",
      "Train Epoch: 5 [3328/4307 (76%)]\tLoss: 0.547940 \tOP: 0.543478\tOR: 0.242718\tOF1: 0.335570\n",
      "Train Epoch: 5 [3392/4307 (78%)]\tLoss: 0.539446 \tOP: 0.571429\tOR: 0.170213\tOF1: 0.262295\n",
      "Train Epoch: 5 [3456/4307 (79%)]\tLoss: 0.553733 \tOP: 0.571429\tOR: 0.117647\tOF1: 0.195122\n",
      "Train Epoch: 5 [3520/4307 (81%)]\tLoss: 0.540295 \tOP: 0.650000\tOR: 0.127451\tOF1: 0.213115\n",
      "Train Epoch: 5 [3584/4307 (82%)]\tLoss: 0.538728 \tOP: 0.600000\tOR: 0.094737\tOF1: 0.163636\n",
      "Train Epoch: 5 [3648/4307 (84%)]\tLoss: 0.556373 \tOP: 0.600000\tOR: 0.086538\tOF1: 0.151261\n",
      "Train Epoch: 5 [3712/4307 (85%)]\tLoss: 0.542711 \tOP: 0.666667\tOR: 0.120000\tOF1: 0.203390\n",
      "Train Epoch: 5 [3776/4307 (87%)]\tLoss: 0.525048 \tOP: 0.750000\tOR: 0.159574\tOF1: 0.263158\n",
      "Train Epoch: 5 [3840/4307 (88%)]\tLoss: 0.540055 \tOP: 0.565217\tOR: 0.128713\tOF1: 0.209677\n",
      "Train Epoch: 5 [3904/4307 (90%)]\tLoss: 0.478069 \tOP: 0.617647\tOR: 0.235955\tOF1: 0.341463\n",
      "Train Epoch: 5 [3968/4307 (91%)]\tLoss: 0.533111 \tOP: 0.656250\tOR: 0.198113\tOF1: 0.304348\n",
      "Train Epoch: 5 [4032/4307 (93%)]\tLoss: 0.506524 \tOP: 0.675676\tOR: 0.242718\tOF1: 0.357143\n",
      "Train Epoch: 5 [4096/4307 (94%)]\tLoss: 0.553007 \tOP: 0.647059\tOR: 0.207547\tOF1: 0.314286\n",
      "Train Epoch: 5 [4160/4307 (96%)]\tLoss: 0.547584 \tOP: 0.642857\tOR: 0.259615\tOF1: 0.369863\n",
      "Train Epoch: 5 [4224/4307 (97%)]\tLoss: 0.523717 \tOP: 0.659574\tOR: 0.313131\tOF1: 0.424658\n",
      "Train Epoch: 5 [1273/4307 (99%)]\tLoss: 0.429641 \tOP: 0.647059\tOR: 0.458333\tOF1: 0.536585\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5270 \n",
      "OP: 0.643441\n",
      "OR: 0.344079\n",
      "OF1: 0.447991\n",
      "\n",
      "Train Epoch: 6 [0/4307 (0%)]\tLoss: 0.512287 \tOP: 0.631579\tOR: 0.367347\tOF1: 0.464516\n",
      "Train Epoch: 6 [64/4307 (1%)]\tLoss: 0.499423 \tOP: 0.653846\tOR: 0.357895\tOF1: 0.462585\n",
      "Train Epoch: 6 [128/4307 (3%)]\tLoss: 0.567805 \tOP: 0.727273\tOR: 0.347826\tOF1: 0.470588\n",
      "Train Epoch: 6 [192/4307 (4%)]\tLoss: 0.550963 \tOP: 0.588235\tOR: 0.288462\tOF1: 0.387097\n",
      "Train Epoch: 6 [256/4307 (6%)]\tLoss: 0.539514 \tOP: 0.588235\tOR: 0.319149\tOF1: 0.413793\n",
      "Train Epoch: 6 [320/4307 (7%)]\tLoss: 0.492639 \tOP: 0.680000\tOR: 0.343434\tOF1: 0.456376\n",
      "Train Epoch: 6 [384/4307 (9%)]\tLoss: 0.498106 \tOP: 0.692308\tOR: 0.360000\tOF1: 0.473684\n",
      "Train Epoch: 6 [448/4307 (10%)]\tLoss: 0.491371 \tOP: 0.744186\tOR: 0.326531\tOF1: 0.453901\n",
      "Train Epoch: 6 [512/4307 (12%)]\tLoss: 0.522285 \tOP: 0.550000\tOR: 0.213592\tOF1: 0.307692\n",
      "Train Epoch: 6 [576/4307 (13%)]\tLoss: 0.542214 \tOP: 0.609756\tOR: 0.257732\tOF1: 0.362319\n",
      "Train Epoch: 6 [640/4307 (15%)]\tLoss: 0.509840 \tOP: 0.722222\tOR: 0.252427\tOF1: 0.374101\n",
      "Train Epoch: 6 [704/4307 (16%)]\tLoss: 0.508580 \tOP: 0.725000\tOR: 0.281553\tOF1: 0.405594\n",
      "Train Epoch: 6 [768/4307 (18%)]\tLoss: 0.563107 \tOP: 0.652174\tOR: 0.275229\tOF1: 0.387097\n",
      "Train Epoch: 6 [832/4307 (19%)]\tLoss: 0.506445 \tOP: 0.666667\tOR: 0.255319\tOF1: 0.369231\n",
      "Train Epoch: 6 [896/4307 (21%)]\tLoss: 0.511240 \tOP: 0.619048\tOR: 0.260000\tOF1: 0.366197\n",
      "Train Epoch: 6 [960/4307 (22%)]\tLoss: 0.532912 \tOP: 0.687500\tOR: 0.323529\tOF1: 0.440000\n",
      "Train Epoch: 6 [1024/4307 (24%)]\tLoss: 0.501626 \tOP: 0.660000\tOR: 0.343750\tOF1: 0.452055\n",
      "Train Epoch: 6 [1088/4307 (25%)]\tLoss: 0.533731 \tOP: 0.590909\tOR: 0.268041\tOF1: 0.368794\n",
      "Train Epoch: 6 [1152/4307 (26%)]\tLoss: 0.560627 \tOP: 0.695652\tOR: 0.285714\tOF1: 0.405063\n",
      "Train Epoch: 6 [1216/4307 (28%)]\tLoss: 0.500600 \tOP: 0.627907\tOR: 0.310345\tOF1: 0.415385\n",
      "Train Epoch: 6 [1280/4307 (29%)]\tLoss: 0.545609 \tOP: 0.675000\tOR: 0.262136\tOF1: 0.377622\n",
      "Train Epoch: 6 [1344/4307 (31%)]\tLoss: 0.550713 \tOP: 0.651163\tOR: 0.264151\tOF1: 0.375839\n",
      "Train Epoch: 6 [1408/4307 (32%)]\tLoss: 0.521041 \tOP: 0.756757\tOR: 0.264151\tOF1: 0.391608\n",
      "Train Epoch: 6 [1472/4307 (34%)]\tLoss: 0.533707 \tOP: 0.694444\tOR: 0.250000\tOF1: 0.367647\n",
      "Train Epoch: 6 [1536/4307 (35%)]\tLoss: 0.528959 \tOP: 0.735294\tOR: 0.255102\tOF1: 0.378788\n",
      "Train Epoch: 6 [1600/4307 (37%)]\tLoss: 0.532390 \tOP: 0.589744\tOR: 0.237113\tOF1: 0.338235\n",
      "Train Epoch: 6 [1664/4307 (38%)]\tLoss: 0.535414 \tOP: 0.631579\tOR: 0.237624\tOF1: 0.345324\n",
      "Train Epoch: 6 [1728/4307 (40%)]\tLoss: 0.493463 \tOP: 0.687500\tOR: 0.234043\tOF1: 0.349206\n",
      "Train Epoch: 6 [1792/4307 (41%)]\tLoss: 0.496339 \tOP: 0.702703\tOR: 0.247619\tOF1: 0.366197\n",
      "Train Epoch: 6 [1856/4307 (43%)]\tLoss: 0.491372 \tOP: 0.673913\tOR: 0.306931\tOF1: 0.421769\n",
      "Train Epoch: 6 [1920/4307 (44%)]\tLoss: 0.503044 \tOP: 0.735849\tOR: 0.361111\tOF1: 0.484472\n",
      "Train Epoch: 6 [1984/4307 (46%)]\tLoss: 0.519641 \tOP: 0.690909\tOR: 0.372549\tOF1: 0.484076\n",
      "Train Epoch: 6 [2048/4307 (47%)]\tLoss: 0.510612 \tOP: 0.677419\tOR: 0.420000\tOF1: 0.518519\n",
      "Train Epoch: 6 [2112/4307 (49%)]\tLoss: 0.473882 \tOP: 0.633333\tOR: 0.422222\tOF1: 0.506667\n",
      "Train Epoch: 6 [2176/4307 (50%)]\tLoss: 0.599126 \tOP: 0.573770\tOR: 0.339806\tOF1: 0.426829\n",
      "Train Epoch: 6 [2240/4307 (51%)]\tLoss: 0.496368 \tOP: 0.703704\tOR: 0.391753\tOF1: 0.503311\n",
      "Train Epoch: 6 [2304/4307 (53%)]\tLoss: 0.470834 \tOP: 0.795918\tOR: 0.410526\tOF1: 0.541667\n",
      "Train Epoch: 6 [2368/4307 (54%)]\tLoss: 0.489231 \tOP: 0.750000\tOR: 0.356436\tOF1: 0.483221\n",
      "Train Epoch: 6 [2432/4307 (56%)]\tLoss: 0.493222 \tOP: 0.613636\tOR: 0.303371\tOF1: 0.406015\n",
      "Train Epoch: 6 [2496/4307 (57%)]\tLoss: 0.530027 \tOP: 0.619048\tOR: 0.252427\tOF1: 0.358621\n",
      "Train Epoch: 6 [2560/4307 (59%)]\tLoss: 0.523602 \tOP: 0.651163\tOR: 0.291667\tOF1: 0.402878\n",
      "Train Epoch: 6 [2624/4307 (60%)]\tLoss: 0.510167 \tOP: 0.729730\tOR: 0.270000\tOF1: 0.394161\n",
      "Train Epoch: 6 [2688/4307 (62%)]\tLoss: 0.516256 \tOP: 0.609756\tOR: 0.263158\tOF1: 0.367647\n",
      "Train Epoch: 6 [2752/4307 (63%)]\tLoss: 0.524814 \tOP: 0.648649\tOR: 0.237624\tOF1: 0.347826\n",
      "Train Epoch: 6 [2816/4307 (65%)]\tLoss: 0.539021 \tOP: 0.666667\tOR: 0.260000\tOF1: 0.374101\n",
      "Train Epoch: 6 [2880/4307 (66%)]\tLoss: 0.555279 \tOP: 0.604651\tOR: 0.257426\tOF1: 0.361111\n",
      "Train Epoch: 6 [2944/4307 (68%)]\tLoss: 0.564182 \tOP: 0.477273\tOR: 0.218750\tOF1: 0.300000\n",
      "Train Epoch: 6 [3008/4307 (69%)]\tLoss: 0.540892 \tOP: 0.638298\tOR: 0.297030\tOF1: 0.405405\n",
      "Train Epoch: 6 [3072/4307 (71%)]\tLoss: 0.563229 \tOP: 0.678571\tOR: 0.177570\tOF1: 0.281481\n",
      "Train Epoch: 6 [3136/4307 (72%)]\tLoss: 0.503455 \tOP: 0.750000\tOR: 0.333333\tOF1: 0.461538\n",
      "Train Epoch: 6 [3200/4307 (74%)]\tLoss: 0.561415 \tOP: 0.538462\tOR: 0.212121\tOF1: 0.304348\n",
      "Train Epoch: 6 [3264/4307 (75%)]\tLoss: 0.536971 \tOP: 0.806452\tOR: 0.221239\tOF1: 0.347222\n",
      "Train Epoch: 6 [3328/4307 (76%)]\tLoss: 0.476523 \tOP: 0.833333\tOR: 0.303030\tOF1: 0.444444\n",
      "Train Epoch: 6 [3392/4307 (78%)]\tLoss: 0.546689 \tOP: 0.631579\tOR: 0.233010\tOF1: 0.340426\n",
      "Train Epoch: 6 [3456/4307 (79%)]\tLoss: 0.501475 \tOP: 0.625000\tOR: 0.284091\tOF1: 0.390625\n",
      "Train Epoch: 6 [3520/4307 (81%)]\tLoss: 0.510079 \tOP: 0.729730\tOR: 0.284211\tOF1: 0.409091\n",
      "Train Epoch: 6 [3584/4307 (82%)]\tLoss: 0.504505 \tOP: 0.800000\tOR: 0.316832\tOF1: 0.453901\n",
      "Train Epoch: 6 [3648/4307 (84%)]\tLoss: 0.523645 \tOP: 0.547619\tOR: 0.258427\tOF1: 0.351145\n",
      "Train Epoch: 6 [3712/4307 (85%)]\tLoss: 0.488347 \tOP: 0.767442\tOR: 0.340206\tOF1: 0.471429\n",
      "Train Epoch: 6 [3776/4307 (87%)]\tLoss: 0.574769 \tOP: 0.500000\tOR: 0.235294\tOF1: 0.320000\n",
      "Train Epoch: 6 [3840/4307 (88%)]\tLoss: 0.501636 \tOP: 0.731707\tOR: 0.306122\tOF1: 0.431655\n",
      "Train Epoch: 6 [3904/4307 (90%)]\tLoss: 0.569257 \tOP: 0.621622\tOR: 0.219048\tOF1: 0.323944\n",
      "Train Epoch: 6 [3968/4307 (91%)]\tLoss: 0.534038 \tOP: 0.658537\tOR: 0.264706\tOF1: 0.377622\n",
      "Train Epoch: 6 [4032/4307 (93%)]\tLoss: 0.484912 \tOP: 0.722222\tOR: 0.270833\tOF1: 0.393939\n",
      "Train Epoch: 6 [4096/4307 (94%)]\tLoss: 0.447695 \tOP: 0.774194\tOR: 0.244898\tOF1: 0.372093\n",
      "Train Epoch: 6 [4160/4307 (96%)]\tLoss: 0.474688 \tOP: 0.697674\tOR: 0.306122\tOF1: 0.425532\n",
      "Train Epoch: 6 [4224/4307 (97%)]\tLoss: 0.554484 \tOP: 0.687500\tOR: 0.213592\tOF1: 0.325926\n",
      "Train Epoch: 6 [1273/4307 (99%)]\tLoss: 0.641962 \tOP: 0.384615\tOR: 0.161290\tOF1: 0.227273\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5210 \n",
      "OP: 0.671794\n",
      "OR: 0.255592\n",
      "OF1: 0.369326\n",
      "\n",
      "Train Epoch: 7 [0/4307 (0%)]\tLoss: 0.531476 \tOP: 0.742857\tOR: 0.240741\tOF1: 0.363636\n",
      "Train Epoch: 7 [64/4307 (1%)]\tLoss: 0.556419 \tOP: 0.666667\tOR: 0.250000\tOF1: 0.363636\n",
      "Train Epoch: 7 [128/4307 (3%)]\tLoss: 0.520689 \tOP: 0.675676\tOR: 0.250000\tOF1: 0.364964\n",
      "Train Epoch: 7 [192/4307 (4%)]\tLoss: 0.483854 \tOP: 0.743590\tOR: 0.295918\tOF1: 0.423358\n",
      "Train Epoch: 7 [256/4307 (6%)]\tLoss: 0.518061 \tOP: 0.617647\tOR: 0.205882\tOF1: 0.308824\n",
      "Train Epoch: 7 [320/4307 (7%)]\tLoss: 0.519529 \tOP: 0.710526\tOR: 0.264706\tOF1: 0.385714\n",
      "Train Epoch: 7 [384/4307 (9%)]\tLoss: 0.527883 \tOP: 0.666667\tOR: 0.237624\tOF1: 0.350365\n",
      "Train Epoch: 7 [448/4307 (10%)]\tLoss: 0.471762 \tOP: 0.810811\tOR: 0.300000\tOF1: 0.437956\n",
      "Train Epoch: 7 [512/4307 (12%)]\tLoss: 0.526955 \tOP: 0.615385\tOR: 0.247423\tOF1: 0.352941\n",
      "Train Epoch: 7 [576/4307 (13%)]\tLoss: 0.544806 \tOP: 0.755556\tOR: 0.309091\tOF1: 0.438710\n",
      "Train Epoch: 7 [640/4307 (15%)]\tLoss: 0.544413 \tOP: 0.590909\tOR: 0.262626\tOF1: 0.363636\n",
      "Train Epoch: 7 [704/4307 (16%)]\tLoss: 0.548650 \tOP: 0.700000\tOR: 0.336538\tOF1: 0.454545\n",
      "Train Epoch: 7 [768/4307 (18%)]\tLoss: 0.518771 \tOP: 0.659091\tOR: 0.292929\tOF1: 0.405594\n",
      "Train Epoch: 7 [832/4307 (19%)]\tLoss: 0.534486 \tOP: 0.673469\tOR: 0.323529\tOF1: 0.437086\n",
      "Train Epoch: 7 [896/4307 (21%)]\tLoss: 0.491522 \tOP: 0.744186\tOR: 0.320000\tOF1: 0.447552\n",
      "Train Epoch: 7 [960/4307 (22%)]\tLoss: 0.505234 \tOP: 0.604167\tOR: 0.298969\tOF1: 0.400000\n",
      "Train Epoch: 7 [1024/4307 (24%)]\tLoss: 0.529377 \tOP: 0.708333\tOR: 0.317757\tOF1: 0.438710\n",
      "Train Epoch: 7 [1088/4307 (25%)]\tLoss: 0.512220 \tOP: 0.738095\tOR: 0.310000\tOF1: 0.436620\n",
      "Train Epoch: 7 [1152/4307 (26%)]\tLoss: 0.491277 \tOP: 0.690909\tOR: 0.408602\tOF1: 0.513514\n",
      "Train Epoch: 7 [1216/4307 (28%)]\tLoss: 0.523562 \tOP: 0.640000\tOR: 0.296296\tOF1: 0.405063\n",
      "Train Epoch: 7 [1280/4307 (29%)]\tLoss: 0.528560 \tOP: 0.720000\tOR: 0.321429\tOF1: 0.444444\n",
      "Train Epoch: 7 [1344/4307 (31%)]\tLoss: 0.497653 \tOP: 0.629630\tOR: 0.343434\tOF1: 0.444444\n",
      "Train Epoch: 7 [1408/4307 (32%)]\tLoss: 0.517677 \tOP: 0.622222\tOR: 0.301075\tOF1: 0.405797\n",
      "Train Epoch: 7 [1472/4307 (34%)]\tLoss: 0.524035 \tOP: 0.685185\tOR: 0.381443\tOF1: 0.490066\n",
      "Train Epoch: 7 [1536/4307 (35%)]\tLoss: 0.491298 \tOP: 0.692308\tOR: 0.391304\tOF1: 0.500000\n",
      "Train Epoch: 7 [1600/4307 (37%)]\tLoss: 0.516203 \tOP: 0.723404\tOR: 0.346939\tOF1: 0.468966\n",
      "Train Epoch: 7 [1664/4307 (38%)]\tLoss: 0.512100 \tOP: 0.675000\tOR: 0.264706\tOF1: 0.380282\n",
      "Train Epoch: 7 [1728/4307 (40%)]\tLoss: 0.577514 \tOP: 0.581395\tOR: 0.247525\tOF1: 0.347222\n",
      "Train Epoch: 7 [1792/4307 (41%)]\tLoss: 0.535073 \tOP: 0.733333\tOR: 0.294643\tOF1: 0.420382\n",
      "Train Epoch: 7 [1856/4307 (43%)]\tLoss: 0.448221 \tOP: 0.756098\tOR: 0.326316\tOF1: 0.455882\n",
      "Train Epoch: 7 [1920/4307 (44%)]\tLoss: 0.477039 \tOP: 0.750000\tOR: 0.315789\tOF1: 0.444444\n",
      "Train Epoch: 7 [1984/4307 (46%)]\tLoss: 0.529102 \tOP: 0.738095\tOR: 0.276786\tOF1: 0.402597\n",
      "Train Epoch: 7 [2048/4307 (47%)]\tLoss: 0.518275 \tOP: 0.769231\tOR: 0.370370\tOF1: 0.500000\n",
      "Train Epoch: 7 [2112/4307 (49%)]\tLoss: 0.505274 \tOP: 0.627907\tOR: 0.296703\tOF1: 0.402985\n",
      "Train Epoch: 7 [2176/4307 (50%)]\tLoss: 0.512246 \tOP: 0.695652\tOR: 0.320000\tOF1: 0.438356\n",
      "Train Epoch: 7 [2240/4307 (51%)]\tLoss: 0.563697 \tOP: 0.511628\tOR: 0.217822\tOF1: 0.305556\n",
      "Train Epoch: 7 [2304/4307 (53%)]\tLoss: 0.458742 \tOP: 0.711111\tOR: 0.359551\tOF1: 0.477612\n",
      "Train Epoch: 7 [2368/4307 (54%)]\tLoss: 0.485099 \tOP: 0.673913\tOR: 0.306931\tOF1: 0.421769\n",
      "Train Epoch: 7 [2432/4307 (56%)]\tLoss: 0.560080 \tOP: 0.575000\tOR: 0.211009\tOF1: 0.308725\n",
      "Train Epoch: 7 [2496/4307 (57%)]\tLoss: 0.488446 \tOP: 0.673469\tOR: 0.330000\tOF1: 0.442953\n",
      "Train Epoch: 7 [2560/4307 (59%)]\tLoss: 0.556775 \tOP: 0.636364\tOR: 0.277228\tOF1: 0.386207\n",
      "Train Epoch: 7 [2624/4307 (60%)]\tLoss: 0.519705 \tOP: 0.574074\tOR: 0.333333\tOF1: 0.421769\n",
      "Train Epoch: 7 [2688/4307 (62%)]\tLoss: 0.587453 \tOP: 0.574468\tOR: 0.264706\tOF1: 0.362416\n",
      "Train Epoch: 7 [2752/4307 (63%)]\tLoss: 0.522069 \tOP: 0.681818\tOR: 0.309278\tOF1: 0.425532\n",
      "Train Epoch: 7 [2816/4307 (65%)]\tLoss: 0.489353 \tOP: 0.750000\tOR: 0.336735\tOF1: 0.464789\n",
      "Train Epoch: 7 [2880/4307 (66%)]\tLoss: 0.540333 \tOP: 0.731707\tOR: 0.285714\tOF1: 0.410959\n",
      "Train Epoch: 7 [2944/4307 (68%)]\tLoss: 0.515715 \tOP: 0.707317\tOR: 0.281553\tOF1: 0.402778\n",
      "Train Epoch: 7 [3008/4307 (69%)]\tLoss: 0.540399 \tOP: 0.594595\tOR: 0.215686\tOF1: 0.316547\n",
      "Train Epoch: 7 [3072/4307 (71%)]\tLoss: 0.490444 \tOP: 0.704545\tOR: 0.326316\tOF1: 0.446043\n",
      "Train Epoch: 7 [3136/4307 (72%)]\tLoss: 0.541453 \tOP: 0.571429\tOR: 0.277228\tOF1: 0.373333\n",
      "Train Epoch: 7 [3200/4307 (74%)]\tLoss: 0.538779 \tOP: 0.621622\tOR: 0.216981\tOF1: 0.321678\n",
      "Train Epoch: 7 [3264/4307 (75%)]\tLoss: 0.520199 \tOP: 0.642857\tOR: 0.272727\tOF1: 0.382979\n",
      "Train Epoch: 7 [3328/4307 (76%)]\tLoss: 0.486231 \tOP: 0.658537\tOR: 0.281250\tOF1: 0.394161\n",
      "Train Epoch: 7 [3392/4307 (78%)]\tLoss: 0.462713 \tOP: 0.772727\tOR: 0.343434\tOF1: 0.475524\n",
      "Train Epoch: 7 [3456/4307 (79%)]\tLoss: 0.473794 \tOP: 0.774194\tOR: 0.263736\tOF1: 0.393443\n",
      "Train Epoch: 7 [3520/4307 (81%)]\tLoss: 0.474402 \tOP: 0.694444\tOR: 0.271739\tOF1: 0.390625\n",
      "Train Epoch: 7 [3584/4307 (82%)]\tLoss: 0.467100 \tOP: 0.820513\tOR: 0.344086\tOF1: 0.484848\n",
      "Train Epoch: 7 [3648/4307 (84%)]\tLoss: 0.470124 \tOP: 0.666667\tOR: 0.333333\tOF1: 0.444444\n",
      "Train Epoch: 7 [3712/4307 (85%)]\tLoss: 0.474271 \tOP: 0.757576\tOR: 0.257732\tOF1: 0.384615\n",
      "Train Epoch: 7 [3776/4307 (87%)]\tLoss: 0.511855 \tOP: 0.666667\tOR: 0.285714\tOF1: 0.400000\n",
      "Train Epoch: 7 [3840/4307 (88%)]\tLoss: 0.552799 \tOP: 0.553191\tOR: 0.270833\tOF1: 0.363636\n",
      "Train Epoch: 7 [3904/4307 (90%)]\tLoss: 0.542490 \tOP: 0.591837\tOR: 0.302083\tOF1: 0.400000\n",
      "Train Epoch: 7 [3968/4307 (91%)]\tLoss: 0.533430 \tOP: 0.687500\tOR: 0.323529\tOF1: 0.440000\n",
      "Train Epoch: 7 [4032/4307 (93%)]\tLoss: 0.513501 \tOP: 0.714286\tOR: 0.357143\tOF1: 0.476190\n",
      "Train Epoch: 7 [4096/4307 (94%)]\tLoss: 0.493131 \tOP: 0.697674\tOR: 0.319149\tOF1: 0.437956\n",
      "Train Epoch: 7 [4160/4307 (96%)]\tLoss: 0.584191 \tOP: 0.652174\tOR: 0.285714\tOF1: 0.397351\n",
      "Train Epoch: 7 [4224/4307 (97%)]\tLoss: 0.505932 \tOP: 0.729730\tOR: 0.275510\tOF1: 0.400000\n",
      "Train Epoch: 7 [1273/4307 (99%)]\tLoss: 0.549858 \tOP: 0.454545\tOR: 0.151515\tOF1: 0.227273\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5248 \n",
      "OP: 0.673693\n",
      "OR: 0.285443\n",
      "OF1: 0.400075\n",
      "\n",
      "Train Epoch: 8 [0/4307 (0%)]\tLoss: 0.517764 \tOP: 0.694444\tOR: 0.245098\tOF1: 0.362319\n",
      "Train Epoch: 8 [64/4307 (1%)]\tLoss: 0.500299 \tOP: 0.818182\tOR: 0.352941\tOF1: 0.493151\n",
      "Train Epoch: 8 [128/4307 (3%)]\tLoss: 0.539468 \tOP: 0.702128\tOR: 0.320388\tOF1: 0.440000\n",
      "Train Epoch: 8 [192/4307 (4%)]\tLoss: 0.547955 \tOP: 0.543478\tOR: 0.245098\tOF1: 0.337838\n",
      "Train Epoch: 8 [256/4307 (6%)]\tLoss: 0.536454 \tOP: 0.704545\tOR: 0.281818\tOF1: 0.402597\n",
      "Train Epoch: 8 [320/4307 (7%)]\tLoss: 0.549797 \tOP: 0.604167\tOR: 0.292929\tOF1: 0.394558\n",
      "Train Epoch: 8 [384/4307 (9%)]\tLoss: 0.520070 \tOP: 0.688889\tOR: 0.289720\tOF1: 0.407895\n",
      "Train Epoch: 8 [448/4307 (10%)]\tLoss: 0.484841 \tOP: 0.725490\tOR: 0.397849\tOF1: 0.513889\n",
      "Train Epoch: 8 [512/4307 (12%)]\tLoss: 0.487201 \tOP: 0.750000\tOR: 0.352941\tOF1: 0.480000\n",
      "Train Epoch: 8 [576/4307 (13%)]\tLoss: 0.509986 \tOP: 0.680000\tOR: 0.330097\tOF1: 0.444444\n",
      "Train Epoch: 8 [640/4307 (15%)]\tLoss: 0.480349 \tOP: 0.750000\tOR: 0.360000\tOF1: 0.486486\n",
      "Train Epoch: 8 [704/4307 (16%)]\tLoss: 0.458688 \tOP: 0.721311\tOR: 0.444444\tOF1: 0.550000\n",
      "Train Epoch: 8 [768/4307 (18%)]\tLoss: 0.516030 \tOP: 0.698113\tOR: 0.373737\tOF1: 0.486842\n",
      "Train Epoch: 8 [832/4307 (19%)]\tLoss: 0.460815 \tOP: 0.689655\tOR: 0.416667\tOF1: 0.519481\n",
      "Train Epoch: 8 [896/4307 (21%)]\tLoss: 0.586790 \tOP: 0.616667\tOR: 0.373737\tOF1: 0.465409\n",
      "Train Epoch: 8 [960/4307 (22%)]\tLoss: 0.570246 \tOP: 0.666667\tOR: 0.345455\tOF1: 0.455090\n",
      "Train Epoch: 8 [1024/4307 (24%)]\tLoss: 0.483930 \tOP: 0.629630\tOR: 0.346939\tOF1: 0.447368\n",
      "Train Epoch: 8 [1088/4307 (25%)]\tLoss: 0.525889 \tOP: 0.576923\tOR: 0.306122\tOF1: 0.400000\n",
      "Train Epoch: 8 [1152/4307 (26%)]\tLoss: 0.529170 \tOP: 0.604651\tOR: 0.265306\tOF1: 0.368794\n",
      "Train Epoch: 8 [1216/4307 (28%)]\tLoss: 0.474034 \tOP: 0.770833\tOR: 0.389474\tOF1: 0.517483\n",
      "Train Epoch: 8 [1280/4307 (29%)]\tLoss: 0.474865 \tOP: 0.750000\tOR: 0.406250\tOF1: 0.527027\n",
      "Train Epoch: 8 [1344/4307 (31%)]\tLoss: 0.532478 \tOP: 0.707317\tOR: 0.284314\tOF1: 0.405594\n",
      "Train Epoch: 8 [1408/4307 (32%)]\tLoss: 0.482298 \tOP: 0.693878\tOR: 0.365591\tOF1: 0.478873\n",
      "Train Epoch: 8 [1472/4307 (34%)]\tLoss: 0.553937 \tOP: 0.605263\tOR: 0.216981\tOF1: 0.319444\n",
      "Train Epoch: 8 [1536/4307 (35%)]\tLoss: 0.523255 \tOP: 0.805556\tOR: 0.273585\tOF1: 0.408451\n",
      "Train Epoch: 8 [1600/4307 (37%)]\tLoss: 0.504438 \tOP: 0.634146\tOR: 0.279570\tOF1: 0.388060\n",
      "Train Epoch: 8 [1664/4307 (38%)]\tLoss: 0.467219 \tOP: 0.694444\tOR: 0.277778\tOF1: 0.396825\n",
      "Train Epoch: 8 [1728/4307 (40%)]\tLoss: 0.487367 \tOP: 0.810811\tOR: 0.283019\tOF1: 0.419580\n",
      "Train Epoch: 8 [1792/4307 (41%)]\tLoss: 0.468366 \tOP: 0.714286\tOR: 0.319149\tOF1: 0.441176\n",
      "Train Epoch: 8 [1856/4307 (43%)]\tLoss: 0.502337 \tOP: 0.785714\tOR: 0.314286\tOF1: 0.448980\n",
      "Train Epoch: 8 [1920/4307 (44%)]\tLoss: 0.524829 \tOP: 0.660000\tOR: 0.333333\tOF1: 0.442953\n",
      "Train Epoch: 8 [1984/4307 (46%)]\tLoss: 0.518348 \tOP: 0.666667\tOR: 0.315789\tOF1: 0.428571\n",
      "Train Epoch: 8 [2048/4307 (47%)]\tLoss: 0.508047 \tOP: 0.642857\tOR: 0.264706\tOF1: 0.375000\n",
      "Train Epoch: 8 [2112/4307 (49%)]\tLoss: 0.510988 \tOP: 0.740000\tOR: 0.362745\tOF1: 0.486842\n",
      "Train Epoch: 8 [2176/4307 (50%)]\tLoss: 0.554058 \tOP: 0.590909\tOR: 0.252427\tOF1: 0.353741\n",
      "Train Epoch: 8 [2240/4307 (51%)]\tLoss: 0.557185 \tOP: 0.580000\tOR: 0.287129\tOF1: 0.384106\n",
      "Train Epoch: 8 [2304/4307 (53%)]\tLoss: 0.520648 \tOP: 0.607843\tOR: 0.340659\tOF1: 0.436620\n",
      "Train Epoch: 8 [2368/4307 (54%)]\tLoss: 0.476452 \tOP: 0.642857\tOR: 0.296703\tOF1: 0.406015\n",
      "Train Epoch: 8 [2432/4307 (56%)]\tLoss: 0.547154 \tOP: 0.612245\tOR: 0.297030\tOF1: 0.400000\n",
      "Train Epoch: 8 [2496/4307 (57%)]\tLoss: 0.517966 \tOP: 0.743590\tOR: 0.263636\tOF1: 0.389262\n",
      "Train Epoch: 8 [2560/4307 (59%)]\tLoss: 0.568053 \tOP: 0.707317\tOR: 0.263636\tOF1: 0.384106\n",
      "Train Epoch: 8 [2624/4307 (60%)]\tLoss: 0.527256 \tOP: 0.687500\tOR: 0.320388\tOF1: 0.437086\n",
      "Train Epoch: 8 [2688/4307 (62%)]\tLoss: 0.517860 \tOP: 0.622642\tOR: 0.333333\tOF1: 0.434211\n",
      "Train Epoch: 8 [2752/4307 (63%)]\tLoss: 0.530652 \tOP: 0.659574\tOR: 0.326316\tOF1: 0.436620\n",
      "Train Epoch: 8 [2816/4307 (65%)]\tLoss: 0.460104 \tOP: 0.825000\tOR: 0.351064\tOF1: 0.492537\n",
      "Train Epoch: 8 [2880/4307 (66%)]\tLoss: 0.543938 \tOP: 0.736842\tOR: 0.254545\tOF1: 0.378378\n",
      "Train Epoch: 8 [2944/4307 (68%)]\tLoss: 0.530295 \tOP: 0.659091\tOR: 0.281553\tOF1: 0.394558\n",
      "Train Epoch: 8 [3008/4307 (69%)]\tLoss: 0.524081 \tOP: 0.714286\tOR: 0.245098\tOF1: 0.364964\n",
      "Train Epoch: 8 [3072/4307 (71%)]\tLoss: 0.417395 \tOP: 0.765957\tOR: 0.409091\tOF1: 0.533333\n",
      "Train Epoch: 8 [3136/4307 (72%)]\tLoss: 0.528106 \tOP: 0.685714\tOR: 0.228571\tOF1: 0.342857\n",
      "Train Epoch: 8 [3200/4307 (74%)]\tLoss: 0.574807 \tOP: 0.575000\tOR: 0.225490\tOF1: 0.323944\n",
      "Train Epoch: 8 [3264/4307 (75%)]\tLoss: 0.505102 \tOP: 0.701754\tOR: 0.408163\tOF1: 0.516129\n",
      "Train Epoch: 8 [3328/4307 (76%)]\tLoss: 0.485572 \tOP: 0.690476\tOR: 0.308511\tOF1: 0.426471\n",
      "Train Epoch: 8 [3392/4307 (78%)]\tLoss: 0.500783 \tOP: 0.686275\tOR: 0.368421\tOF1: 0.479452\n",
      "Train Epoch: 8 [3456/4307 (79%)]\tLoss: 0.521645 \tOP: 0.619048\tOR: 0.276596\tOF1: 0.382353\n",
      "Train Epoch: 8 [3520/4307 (81%)]\tLoss: 0.514575 \tOP: 0.681818\tOR: 0.280374\tOF1: 0.397351\n",
      "Train Epoch: 8 [3584/4307 (82%)]\tLoss: 0.526575 \tOP: 0.648649\tOR: 0.222222\tOF1: 0.331034\n",
      "Train Epoch: 8 [3648/4307 (84%)]\tLoss: 0.505169 \tOP: 0.673913\tOR: 0.322917\tOF1: 0.436620\n",
      "Train Epoch: 8 [3712/4307 (85%)]\tLoss: 0.523257 \tOP: 0.717391\tOR: 0.320388\tOF1: 0.442953\n",
      "Train Epoch: 8 [3776/4307 (87%)]\tLoss: 0.528889 \tOP: 0.729167\tOR: 0.343137\tOF1: 0.466667\n",
      "Train Epoch: 8 [3840/4307 (88%)]\tLoss: 0.482958 \tOP: 0.767442\tOR: 0.351064\tOF1: 0.481752\n",
      "Train Epoch: 8 [3904/4307 (90%)]\tLoss: 0.522448 \tOP: 0.684211\tOR: 0.262626\tOF1: 0.379562\n",
      "Train Epoch: 8 [3968/4307 (91%)]\tLoss: 0.524310 \tOP: 0.666667\tOR: 0.274510\tOF1: 0.388889\n",
      "Train Epoch: 8 [4032/4307 (93%)]\tLoss: 0.543458 \tOP: 0.536585\tOR: 0.207547\tOF1: 0.299320\n",
      "Train Epoch: 8 [4096/4307 (94%)]\tLoss: 0.525334 \tOP: 0.609756\tOR: 0.268817\tOF1: 0.373134\n",
      "Train Epoch: 8 [4160/4307 (96%)]\tLoss: 0.480446 \tOP: 0.805556\tOR: 0.318681\tOF1: 0.456693\n",
      "Train Epoch: 8 [4224/4307 (97%)]\tLoss: 0.437334 \tOP: 0.809524\tOR: 0.365591\tOF1: 0.503704\n",
      "Train Epoch: 8 [1273/4307 (99%)]\tLoss: 0.527989 \tOP: 0.705882\tOR: 0.387097\tOF1: 0.500000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5236 \n",
      "OP: 0.660727\n",
      "OR: 0.277043\n",
      "OF1: 0.389561\n",
      "\n",
      "Train Epoch: 9 [0/4307 (0%)]\tLoss: 0.529491 \tOP: 0.617021\tOR: 0.290000\tOF1: 0.394558\n",
      "Train Epoch: 9 [64/4307 (1%)]\tLoss: 0.463779 \tOP: 0.729730\tOR: 0.275510\tOF1: 0.400000\n",
      "Train Epoch: 9 [128/4307 (3%)]\tLoss: 0.515338 \tOP: 0.684211\tOR: 0.260000\tOF1: 0.376812\n",
      "Train Epoch: 9 [192/4307 (4%)]\tLoss: 0.526856 \tOP: 0.742857\tOR: 0.262626\tOF1: 0.388060\n",
      "Train Epoch: 9 [256/4307 (6%)]\tLoss: 0.501455 \tOP: 0.725000\tOR: 0.287129\tOF1: 0.411348\n",
      "Train Epoch: 9 [320/4307 (7%)]\tLoss: 0.525256 \tOP: 0.636364\tOR: 0.271845\tOF1: 0.380952\n",
      "Train Epoch: 9 [384/4307 (9%)]\tLoss: 0.534551 \tOP: 0.731707\tOR: 0.285714\tOF1: 0.410959\n",
      "Train Epoch: 9 [448/4307 (10%)]\tLoss: 0.502313 \tOP: 0.682927\tOR: 0.285714\tOF1: 0.402878\n",
      "Train Epoch: 9 [512/4307 (12%)]\tLoss: 0.478353 \tOP: 0.697674\tOR: 0.312500\tOF1: 0.431655\n",
      "Train Epoch: 9 [576/4307 (13%)]\tLoss: 0.517460 \tOP: 0.736842\tOR: 0.288660\tOF1: 0.414815\n",
      "Train Epoch: 9 [640/4307 (15%)]\tLoss: 0.544922 \tOP: 0.666667\tOR: 0.293578\tOF1: 0.407643\n",
      "Train Epoch: 9 [704/4307 (16%)]\tLoss: 0.510631 \tOP: 0.745098\tOR: 0.380000\tOF1: 0.503311\n",
      "Train Epoch: 9 [768/4307 (18%)]\tLoss: 0.477116 \tOP: 0.660000\tOR: 0.351064\tOF1: 0.458333\n",
      "Train Epoch: 9 [832/4307 (19%)]\tLoss: 0.492445 \tOP: 0.625000\tOR: 0.333333\tOF1: 0.434783\n",
      "Train Epoch: 9 [896/4307 (21%)]\tLoss: 0.529865 \tOP: 0.717391\tOR: 0.311321\tOF1: 0.434211\n",
      "Train Epoch: 9 [960/4307 (22%)]\tLoss: 0.470274 \tOP: 0.773585\tOR: 0.418367\tOF1: 0.543046\n",
      "Train Epoch: 9 [1024/4307 (24%)]\tLoss: 0.458737 \tOP: 0.741379\tOR: 0.457447\tOF1: 0.565789\n",
      "Train Epoch: 9 [1088/4307 (25%)]\tLoss: 0.466443 \tOP: 0.728814\tOR: 0.430000\tOF1: 0.540881\n",
      "Train Epoch: 9 [1152/4307 (26%)]\tLoss: 0.550960 \tOP: 0.637931\tOR: 0.355769\tOF1: 0.456790\n",
      "Train Epoch: 9 [1216/4307 (28%)]\tLoss: 0.471361 \tOP: 0.666667\tOR: 0.437500\tOF1: 0.528302\n",
      "Train Epoch: 9 [1280/4307 (29%)]\tLoss: 0.488065 \tOP: 0.666667\tOR: 0.425532\tOF1: 0.519481\n",
      "Train Epoch: 9 [1344/4307 (31%)]\tLoss: 0.518795 \tOP: 0.767857\tOR: 0.409524\tOF1: 0.534161\n",
      "Train Epoch: 9 [1408/4307 (32%)]\tLoss: 0.461781 \tOP: 0.684211\tOR: 0.438202\tOF1: 0.534247\n",
      "Train Epoch: 9 [1472/4307 (34%)]\tLoss: 0.495855 \tOP: 0.695652\tOR: 0.344086\tOF1: 0.460432\n",
      "Train Epoch: 9 [1536/4307 (35%)]\tLoss: 0.535524 \tOP: 0.666667\tOR: 0.252427\tOF1: 0.366197\n",
      "Train Epoch: 9 [1600/4307 (37%)]\tLoss: 0.553597 \tOP: 0.657895\tOR: 0.238095\tOF1: 0.349650\n",
      "Train Epoch: 9 [1664/4307 (38%)]\tLoss: 0.541415 \tOP: 0.600000\tOR: 0.252632\tOF1: 0.355556\n",
      "Train Epoch: 9 [1728/4307 (40%)]\tLoss: 0.471815 \tOP: 0.795455\tOR: 0.364583\tOF1: 0.500000\n",
      "Train Epoch: 9 [1792/4307 (41%)]\tLoss: 0.483957 \tOP: 0.775000\tOR: 0.316327\tOF1: 0.449275\n",
      "Train Epoch: 9 [1856/4307 (43%)]\tLoss: 0.495394 \tOP: 0.777778\tOR: 0.277228\tOF1: 0.408759\n",
      "Train Epoch: 9 [1920/4307 (44%)]\tLoss: 0.522639 \tOP: 0.813953\tOR: 0.333333\tOF1: 0.472973\n",
      "Train Epoch: 9 [1984/4307 (46%)]\tLoss: 0.490339 \tOP: 0.666667\tOR: 0.320000\tOF1: 0.432432\n",
      "Train Epoch: 9 [2048/4307 (47%)]\tLoss: 0.513639 \tOP: 0.625000\tOR: 0.326087\tOF1: 0.428571\n",
      "Train Epoch: 9 [2112/4307 (49%)]\tLoss: 0.528490 \tOP: 0.706897\tOR: 0.383178\tOF1: 0.496970\n",
      "Train Epoch: 9 [2176/4307 (50%)]\tLoss: 0.485193 \tOP: 0.636364\tOR: 0.388889\tOF1: 0.482759\n",
      "Train Epoch: 9 [2240/4307 (51%)]\tLoss: 0.452219 \tOP: 0.725490\tOR: 0.393617\tOF1: 0.510345\n",
      "Train Epoch: 9 [2304/4307 (53%)]\tLoss: 0.567311 \tOP: 0.555556\tOR: 0.300000\tOF1: 0.389610\n",
      "Train Epoch: 9 [2368/4307 (54%)]\tLoss: 0.487220 \tOP: 0.700000\tOR: 0.437500\tOF1: 0.538462\n",
      "Train Epoch: 9 [2432/4307 (56%)]\tLoss: 0.508783 \tOP: 0.754386\tOR: 0.394495\tOF1: 0.518072\n",
      "Train Epoch: 9 [2496/4307 (57%)]\tLoss: 0.477321 \tOP: 0.687500\tOR: 0.340206\tOF1: 0.455172\n",
      "Train Epoch: 9 [2560/4307 (59%)]\tLoss: 0.533799 \tOP: 0.727273\tOR: 0.301887\tOF1: 0.426667\n",
      "Train Epoch: 9 [2624/4307 (60%)]\tLoss: 0.540558 \tOP: 0.571429\tOR: 0.235294\tOF1: 0.333333\n",
      "Train Epoch: 9 [2688/4307 (62%)]\tLoss: 0.515288 \tOP: 0.707317\tOR: 0.278846\tOF1: 0.400000\n",
      "Train Epoch: 9 [2752/4307 (63%)]\tLoss: 0.499280 \tOP: 0.731707\tOR: 0.309278\tOF1: 0.434783\n",
      "Train Epoch: 9 [2816/4307 (65%)]\tLoss: 0.503169 \tOP: 0.638298\tOR: 0.306122\tOF1: 0.413793\n",
      "Train Epoch: 9 [2880/4307 (66%)]\tLoss: 0.500516 \tOP: 0.700000\tOR: 0.280000\tOF1: 0.400000\n",
      "Train Epoch: 9 [2944/4307 (68%)]\tLoss: 0.482743 \tOP: 0.739130\tOR: 0.369565\tOF1: 0.492754\n",
      "Train Epoch: 9 [3008/4307 (69%)]\tLoss: 0.514086 \tOP: 0.660000\tOR: 0.314286\tOF1: 0.425806\n",
      "Train Epoch: 9 [3072/4307 (71%)]\tLoss: 0.537400 \tOP: 0.767442\tOR: 0.308411\tOF1: 0.440000\n",
      "Train Epoch: 9 [3136/4307 (72%)]\tLoss: 0.508104 \tOP: 0.600000\tOR: 0.309278\tOF1: 0.408163\n",
      "Train Epoch: 9 [3200/4307 (74%)]\tLoss: 0.556462 \tOP: 0.673469\tOR: 0.314286\tOF1: 0.428571\n",
      "Train Epoch: 9 [3264/4307 (75%)]\tLoss: 0.533521 \tOP: 0.596491\tOR: 0.350515\tOF1: 0.441558\n",
      "Train Epoch: 9 [3328/4307 (76%)]\tLoss: 0.526459 \tOP: 0.647059\tOR: 0.343750\tOF1: 0.448980\n",
      "Train Epoch: 9 [3392/4307 (78%)]\tLoss: 0.547425 \tOP: 0.674419\tOR: 0.287129\tOF1: 0.402778\n",
      "Train Epoch: 9 [3456/4307 (79%)]\tLoss: 0.546692 \tOP: 0.707317\tOR: 0.284314\tOF1: 0.405594\n",
      "Train Epoch: 9 [3520/4307 (81%)]\tLoss: 0.481512 \tOP: 0.757576\tOR: 0.252525\tOF1: 0.378788\n",
      "Train Epoch: 9 [3584/4307 (82%)]\tLoss: 0.553469 \tOP: 0.697674\tOR: 0.265487\tOF1: 0.384615\n",
      "Train Epoch: 9 [3648/4307 (84%)]\tLoss: 0.510837 \tOP: 0.666667\tOR: 0.231579\tOF1: 0.343750\n",
      "Train Epoch: 9 [3712/4307 (85%)]\tLoss: 0.534012 \tOP: 0.783784\tOR: 0.295918\tOF1: 0.429630\n",
      "Train Epoch: 9 [3776/4307 (87%)]\tLoss: 0.516438 \tOP: 0.628571\tOR: 0.231579\tOF1: 0.338462\n",
      "Train Epoch: 9 [3840/4307 (88%)]\tLoss: 0.559980 \tOP: 0.611111\tOR: 0.200000\tOF1: 0.301370\n",
      "Train Epoch: 9 [3904/4307 (90%)]\tLoss: 0.546061 \tOP: 0.641026\tOR: 0.240385\tOF1: 0.349650\n",
      "Train Epoch: 9 [3968/4307 (91%)]\tLoss: 0.485049 \tOP: 0.711111\tOR: 0.336842\tOF1: 0.457143\n",
      "Train Epoch: 9 [4032/4307 (93%)]\tLoss: 0.551035 \tOP: 0.659574\tOR: 0.292453\tOF1: 0.405229\n",
      "Train Epoch: 9 [4096/4307 (94%)]\tLoss: 0.462478 \tOP: 0.743590\tOR: 0.302083\tOF1: 0.429630\n",
      "Train Epoch: 9 [4160/4307 (96%)]\tLoss: 0.556193 \tOP: 0.589744\tOR: 0.227723\tOF1: 0.328571\n",
      "Train Epoch: 9 [4224/4307 (97%)]\tLoss: 0.542225 \tOP: 0.605263\tOR: 0.223301\tOF1: 0.326241\n",
      "Train Epoch: 9 [1273/4307 (99%)]\tLoss: 0.629902 \tOP: 0.600000\tOR: 0.243243\tOF1: 0.346154\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5215 \n",
      "OP: 0.653260\n",
      "OR: 0.274032\n",
      "OF1: 0.385294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model4.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model4(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        loss_lst_train4.append(loss.data.item())\n",
    "        OF1_lst_train4.append(OF1)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model4.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    OP_final = 0\n",
    "    OR_final = 0\n",
    "    OF1_final = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model4(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        OP_final += OP\n",
    "        OR_final += OR\n",
    "        OF1_final += OF1\n",
    "        \n",
    "    loss_lst_test4.append(test_loss.data.item()/i)\n",
    "    OF1_lst_test4.append(OF1_final/i)\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP_final/i, OR_final/i, OF1_final/i))\n",
    "\n",
    "loss_lst_train4 = []\n",
    "OF1_lst_train4 = []\n",
    "\n",
    "loss_lst_test4 = []\n",
    "OF1_lst_test4 = []    \n",
    "    \n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
