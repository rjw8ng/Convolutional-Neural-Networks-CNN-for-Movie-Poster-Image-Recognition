{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "from torch.nn.modules import MSELoss, L1Loss, BCELoss\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "from numpy import array, asarray, ndarray, swapaxes\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training controls\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "training_size = 0.8\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 268, 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data holders\n",
    "x_test = []\n",
    "x_train = []\n",
    "y_test= []\n",
    "y_train= []\n",
    "tempY = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdbId', 'Imdb Link', 'Title', 'IMDB Score', 'Genre', 'Poster']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opening the dataset\n",
    "dataset = csv.reader(open(\"MovieGenre3.csv\",encoding=\"utf8\",errors='replace'), delimiter=\",\")\n",
    "\n",
    "# skipping the header line\n",
    "next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract images from zip folder\n",
    "\n",
    "import zipfile as zf\n",
    "\n",
    "files = zf.ZipFile(\"Poster.zip\", 'r')\n",
    "files.extractall()\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of image files in SampleMoviePosters folder\n",
    "flist=glob.glob('Poster/*.jpg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1354"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = []\n",
    "\n",
    "for path in flist:\n",
    "    start = path.rfind(\"/\")+1\n",
    "    end = len(path)-4\n",
    "    image_ids.append(path[start:end])\n",
    "    \n",
    "#image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160905</td>\n",
       "      <td>http://www.imdb.com/title/tt160905</td>\n",
       "      <td>Spooky House (2002)</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Comedy|Family</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427531</td>\n",
       "      <td>http://www.imdb.com/title/tt427531</td>\n",
       "      <td>Mezzo Forte</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Animation|Comedy|Crime</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4175088</td>\n",
       "      <td>http://www.imdb.com/title/tt4175088</td>\n",
       "      <td>Radical Grace (2015)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>Documentary|News</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1686053</td>\n",
       "      <td>http://www.imdb.com/title/tt1686053</td>\n",
       "      <td>Apnoia (2010)</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22921</td>\n",
       "      <td>http://www.imdb.com/title/tt22921</td>\n",
       "      <td>Broadway to Cheyenne (1932)</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>821638</td>\n",
       "      <td>http://www.imdb.com/title/tt821638</td>\n",
       "      <td>Bury My Heart at Wounded Knee (2007)</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Drama|History|Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>87835</td>\n",
       "      <td>http://www.imdb.com/title/tt87835</td>\n",
       "      <td>Oh, God! You Devil (1984)</td>\n",
       "      <td>5.3</td>\n",
       "      <td>Comedy|Fantasy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>103710</td>\n",
       "      <td>http://www.imdb.com/title/tt103710</td>\n",
       "      <td>AprÌ¬s l'amour (1992)</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>108265</td>\n",
       "      <td>http://www.imdb.com/title/tt108265</td>\n",
       "      <td>Swing Kids (1993)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Drama|Music</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>56468</td>\n",
       "      <td>http://www.imdb.com/title/tt56468</td>\n",
       "      <td>I sequestrati di Altona (1962)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Drama|History</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       imdbId                            Imdb Link  \\\n",
       "0      160905   http://www.imdb.com/title/tt160905   \n",
       "1      427531   http://www.imdb.com/title/tt427531   \n",
       "2     4175088  http://www.imdb.com/title/tt4175088   \n",
       "3     1686053  http://www.imdb.com/title/tt1686053   \n",
       "4       22921    http://www.imdb.com/title/tt22921   \n",
       "...       ...                                  ...   \n",
       "1349   821638   http://www.imdb.com/title/tt821638   \n",
       "1350    87835    http://www.imdb.com/title/tt87835   \n",
       "1351   103710   http://www.imdb.com/title/tt103710   \n",
       "1352   108265   http://www.imdb.com/title/tt108265   \n",
       "1353    56468    http://www.imdb.com/title/tt56468   \n",
       "\n",
       "                                     Title  IMDB Score  \\\n",
       "0                      Spooky House (2002)         5.4   \n",
       "1                              Mezzo Forte         6.9   \n",
       "2                     Radical Grace (2015)         8.3   \n",
       "3                            Apnoia (2010)         5.6   \n",
       "4              Broadway to Cheyenne (1932)         4.8   \n",
       "...                                    ...         ...   \n",
       "1349  Bury My Heart at Wounded Knee (2007)         7.2   \n",
       "1350             Oh, God! You Devil (1984)         5.3   \n",
       "1351                 AprÌ¬s l'amour (1992)         6.3   \n",
       "1352                     Swing Kids (1993)         6.8   \n",
       "1353        I sequestrati di Altona (1962)         7.0   \n",
       "\n",
       "                       Genre  \\\n",
       "0              Comedy|Family   \n",
       "1     Animation|Comedy|Crime   \n",
       "2           Documentary|News   \n",
       "3             Drama|Thriller   \n",
       "4                    Western   \n",
       "...                      ...   \n",
       "1349   Drama|History|Western   \n",
       "1350          Comedy|Fantasy   \n",
       "1351           Drama|Romance   \n",
       "1352             Drama|Music   \n",
       "1353           Drama|History   \n",
       "\n",
       "                                                 Poster  \n",
       "0     https://images-na.ssl-images-amazon.com/images...  \n",
       "1     https://images-na.ssl-images-amazon.com/images...  \n",
       "2     https://images-na.ssl-images-amazon.com/images...  \n",
       "3     https://images-na.ssl-images-amazon.com/images...  \n",
       "4     https://images-na.ssl-images-amazon.com/images...  \n",
       "...                                                 ...  \n",
       "1349  https://images-na.ssl-images-amazon.com/images...  \n",
       "1350  https://images-na.ssl-images-amazon.com/images...  \n",
       "1351  https://images-na.ssl-images-amazon.com/images...  \n",
       "1352  https://images-na.ssl-images-amazon.com/images...  \n",
       "1353  https://images-na.ssl-images-amazon.com/images...  \n",
       "\n",
       "[1354 rows x 6 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset2 = pd.read_csv(\"MovieGenre3.csv\")\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "indexlist = []\n",
    "classes = tuple()\n",
    "ids = dataset2.imdbId.values.tolist()\n",
    "for image_id in image_ids:\n",
    "    genres = tuple((dataset2[dataset2[\"imdbId\"] == int(image_id)][\"Genre\"].values[0]).split(\"|\"))\n",
    "    if int(image_id) in ids:\n",
    "        indexlist.append(image_id)\n",
    "    y.append(genres)\n",
    "    classes = classes + genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y)\n",
    "y = mlb.transform(y)\n",
    "classes = set(classes)\n",
    "classes = list(classes)\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>...</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>News</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788556</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4767340</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363473</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393775</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018920</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244244</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897945</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151987</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Action  Adventure  Animation  Biography  Comedy  Crime  Documentary  \\\n",
       "87913         0          0          0          0       0      0            0   \n",
       "2788556       0          0          0          0       1      0            0   \n",
       "4767340       0          0          0          0       1      0            0   \n",
       "363473        0          0          0          1       0      0            0   \n",
       "393775        0          0          0          0       1      0            0   \n",
       "...         ...        ...        ...        ...     ...    ...          ...   \n",
       "1018920       0          0          0          0       0      0            1   \n",
       "244244        1          0          0          0       0      1            0   \n",
       "1897945       0          0          0          0       1      0            0   \n",
       "377569        0          0          0          0       0      0            0   \n",
       "151987        1          1          0          0       0      1            0   \n",
       "\n",
       "         Drama  Family  Fantasy  ...  Musical  Mystery  News  Romance  Sci-Fi  \\\n",
       "87913        1       0        0  ...        0        0     0        0       0   \n",
       "2788556      1       0        0  ...        0        0     0        1       0   \n",
       "4767340      1       0        0  ...        0        0     0        0       0   \n",
       "363473       1       0        0  ...        0        0     0        0       0   \n",
       "393775       1       0        0  ...        0        0     0        0       0   \n",
       "...        ...     ...      ...  ...      ...      ...   ...      ...     ...   \n",
       "1018920      0       0        0  ...        0        0     0        0       0   \n",
       "244244       0       0        0  ...        0        0     0        0       0   \n",
       "1897945      0       0        0  ...        0        0     0        1       0   \n",
       "377569       1       0        0  ...        0        0     0        0       0   \n",
       "151987       0       0        0  ...        0        0     0        0       0   \n",
       "\n",
       "         Short  Sport  Thriller  War  Western  \n",
       "87913        0      0         0    0        0  \n",
       "2788556      0      0         0    0        0  \n",
       "4767340      0      0         0    0        0  \n",
       "363473       0      0         0    0        0  \n",
       "393775       0      0         0    0        0  \n",
       "...        ...    ...       ...  ...      ...  \n",
       "1018920      1      0         0    0        0  \n",
       "244244       0      0         1    0        0  \n",
       "1897945      0      0         0    0        0  \n",
       "377569       0      0         0    0        0  \n",
       "151987       0      0         0    0        0  \n",
       "\n",
       "[1354 rows x 24 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame(y, columns = classes, index = indexlist)\n",
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "      <th>genrelst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160905</td>\n",
       "      <td>http://www.imdb.com/title/tt160905</td>\n",
       "      <td>Spooky House (2002)</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Comedy|Family</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427531</td>\n",
       "      <td>http://www.imdb.com/title/tt427531</td>\n",
       "      <td>Mezzo Forte</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Animation|Comedy|Crime</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4175088</td>\n",
       "      <td>http://www.imdb.com/title/tt4175088</td>\n",
       "      <td>Radical Grace (2015)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>Documentary|News</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1686053</td>\n",
       "      <td>http://www.imdb.com/title/tt1686053</td>\n",
       "      <td>Apnoia (2010)</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22921</td>\n",
       "      <td>http://www.imdb.com/title/tt22921</td>\n",
       "      <td>Broadway to Cheyenne (1932)</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>821638</td>\n",
       "      <td>http://www.imdb.com/title/tt821638</td>\n",
       "      <td>Bury My Heart at Wounded Knee (2007)</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Drama|History|Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>87835</td>\n",
       "      <td>http://www.imdb.com/title/tt87835</td>\n",
       "      <td>Oh, God! You Devil (1984)</td>\n",
       "      <td>5.3</td>\n",
       "      <td>Comedy|Fantasy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>103710</td>\n",
       "      <td>http://www.imdb.com/title/tt103710</td>\n",
       "      <td>AprÌ¬s l'amour (1992)</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>108265</td>\n",
       "      <td>http://www.imdb.com/title/tt108265</td>\n",
       "      <td>Swing Kids (1993)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Drama|Music</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>56468</td>\n",
       "      <td>http://www.imdb.com/title/tt56468</td>\n",
       "      <td>I sequestrati di Altona (1962)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Drama|History</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       imdbId                            Imdb Link  \\\n",
       "0      160905   http://www.imdb.com/title/tt160905   \n",
       "1      427531   http://www.imdb.com/title/tt427531   \n",
       "2     4175088  http://www.imdb.com/title/tt4175088   \n",
       "3     1686053  http://www.imdb.com/title/tt1686053   \n",
       "4       22921    http://www.imdb.com/title/tt22921   \n",
       "...       ...                                  ...   \n",
       "1349   821638   http://www.imdb.com/title/tt821638   \n",
       "1350    87835    http://www.imdb.com/title/tt87835   \n",
       "1351   103710   http://www.imdb.com/title/tt103710   \n",
       "1352   108265   http://www.imdb.com/title/tt108265   \n",
       "1353    56468    http://www.imdb.com/title/tt56468   \n",
       "\n",
       "                                     Title  IMDB Score  \\\n",
       "0                      Spooky House (2002)         5.4   \n",
       "1                              Mezzo Forte         6.9   \n",
       "2                     Radical Grace (2015)         8.3   \n",
       "3                            Apnoia (2010)         5.6   \n",
       "4              Broadway to Cheyenne (1932)         4.8   \n",
       "...                                    ...         ...   \n",
       "1349  Bury My Heart at Wounded Knee (2007)         7.2   \n",
       "1350             Oh, God! You Devil (1984)         5.3   \n",
       "1351                 AprÌ¬s l'amour (1992)         6.3   \n",
       "1352                     Swing Kids (1993)         6.8   \n",
       "1353        I sequestrati di Altona (1962)         7.0   \n",
       "\n",
       "                       Genre  \\\n",
       "0              Comedy|Family   \n",
       "1     Animation|Comedy|Crime   \n",
       "2           Documentary|News   \n",
       "3             Drama|Thriller   \n",
       "4                    Western   \n",
       "...                      ...   \n",
       "1349   Drama|History|Western   \n",
       "1350          Comedy|Fantasy   \n",
       "1351           Drama|Romance   \n",
       "1352             Drama|Music   \n",
       "1353           Drama|History   \n",
       "\n",
       "                                                 Poster  \\\n",
       "0     https://images-na.ssl-images-amazon.com/images...   \n",
       "1     https://images-na.ssl-images-amazon.com/images...   \n",
       "2     https://images-na.ssl-images-amazon.com/images...   \n",
       "3     https://images-na.ssl-images-amazon.com/images...   \n",
       "4     https://images-na.ssl-images-amazon.com/images...   \n",
       "...                                                 ...   \n",
       "1349  https://images-na.ssl-images-amazon.com/images...   \n",
       "1350  https://images-na.ssl-images-amazon.com/images...   \n",
       "1351  https://images-na.ssl-images-amazon.com/images...   \n",
       "1352  https://images-na.ssl-images-amazon.com/images...   \n",
       "1353  https://images-na.ssl-images-amazon.com/images...   \n",
       "\n",
       "                                               genrelst  \n",
       "0     [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "1349  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1350  [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "1351  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1352  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "1353  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "\n",
       "[1354 rows x 7 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df_reset = y_df.reset_index()\n",
    "\n",
    "shape = y_df_reset.shape[1]\n",
    "\n",
    "index_value = []\n",
    "genre_lst = []\n",
    "\n",
    "for i in range(len(y_df_reset)):\n",
    "    index_value.append(int(y_df_reset.loc[i,\"index\"]))\n",
    "    temp_list = []\n",
    "    for j in y_df_reset.columns[1:]:\n",
    "        temp_list.append(y_df_reset.loc[i,j])\n",
    "    genre_lst.append(temp_list)\n",
    "\n",
    "df = pd.DataFrame(list(zip(index_value, genre_lst)),\n",
    "               columns =['imdbId', 'genrelst'])\n",
    "\n",
    "result = dataset2.merge(df, on=\"imdbId\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(result)):\n",
    "    tempY.append((int(result['imdbId'].iloc[x]),result['genrelst'].iloc[x]))\n",
    "\n",
    "#tempY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1083"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting the length of training data\n",
    "length=int(len(flist)*training_size)\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the data about the images that are available\n",
    "i=0\n",
    "for filename in flist:\n",
    "    name=int(filename.split('/')[-1][:-4])\n",
    "    for z in tempY:\n",
    "        if(z[0]==name):\n",
    "            \n",
    "            img = array(cv2.imread(filename))\n",
    "            img = swapaxes(img, 2,0)\n",
    "            img = swapaxes(img, 2,1)\n",
    "\n",
    "            if(i<length):\n",
    "                x_train.append(img)\n",
    "                y_train.append(z[1])\n",
    "                i+=1\n",
    "            else:\n",
    "                x_test.append(img)\n",
    "                y_test.append(z[1])\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data from lists to numpy arrays\n",
    "x_train=asarray(x_train,dtype=float)\n",
    "x_test=asarray(x_test,dtype=float)\n",
    "y_train=asarray(y_train,dtype=float)\n",
    "y_test=asarray(y_test,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling down the RGB data\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1083, 3, 268, 182)\n",
      "1083 train samples\n",
      "271 test samples\n"
     ]
    }
   ],
   "source": [
    "#printing stats about the features\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = x_train.shape[0]\n",
    "\n",
    "x_train=torch.from_numpy(x_train)\n",
    "x_test=torch.from_numpy(x_test)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_test=torch.from_numpy(y_test)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train, y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(x_test, y_test)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric calculation\n",
    "\n",
    "def metric(scores, targets, types):\n",
    "    \"\"\"\n",
    "    :param scores: the output the model predict\n",
    "    :param targets: the gt label\n",
    "    :return: OP, OR, OF1, CP, CR, CF1\n",
    "    calculate the Precision of every class by: TP/TP+FP i.e. TP/total predict\n",
    "    calculate the Recall by: TP/total GT\n",
    "    \"\"\"\n",
    "    num, num_class = scores.shape\n",
    "    gt_num = np.zeros(num_class)\n",
    "    tp_num = np.zeros(num_class)\n",
    "    predict_num = np.zeros(num_class)\n",
    "\n",
    "\n",
    "    for index in range(num_class):\n",
    "        score = scores[:, index]\n",
    "        target = targets[:, index]\n",
    "        if types == 'wider':\n",
    "            tmp = np.where(target == 99)[0]\n",
    "            # score[tmp] = 0\n",
    "            target[tmp] = 0\n",
    "        \n",
    "        if types == 'voc07':\n",
    "            tmp = np.where(target != 0)[0]\n",
    "            score = score[tmp]\n",
    "            target = target[tmp]\n",
    "            neg_id = np.where(target == -1)[0]\n",
    "            target[neg_id] = 0\n",
    "\n",
    "\n",
    "        gt_num[index] = np.sum(target == 1)\n",
    "        predict_num[index] = np.sum(score >= 0.5)\n",
    "        tp_num[index] = np.sum(target * (score >= 0.5))\n",
    "\n",
    "    predict_num[predict_num == 0] = 1  # avoid dividing 0\n",
    "    OP = np.sum(tp_num) / np.sum(predict_num) #OP (Overall Precision) is the ratio of the number of correctly predicted positive samples to the total number of positive predictions made by the model\n",
    "    OR = np.sum(tp_num) / np.sum(gt_num) #OR (Overall Recall) is the ratio of the number of correctly predicted positive samples to the total number of positive samples in the ground truth.\n",
    "    OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n",
    "\n",
    "    return OP, OR, OF1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 model\n",
    "from torchvision import models\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(2048, len(classes))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNet()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1083 (0%)]\tLoss: 0.936435 \tOP: 0.102450\tOR: 0.707692\tOF1: 0.178988\n",
      "Train Epoch: 0 [32/1083 (3%)]\tLoss: 0.916090 \tOP: 0.126844\tOR: 0.573333\tOF1: 0.207729\n",
      "Train Epoch: 0 [64/1083 (6%)]\tLoss: 0.909799 \tOP: 0.126050\tOR: 0.454545\tOF1: 0.197368\n",
      "Train Epoch: 0 [96/1083 (9%)]\tLoss: 0.893205 \tOP: 0.196078\tOR: 0.416667\tOF1: 0.266667\n",
      "Train Epoch: 0 [128/1083 (12%)]\tLoss: 0.878796 \tOP: 0.280899\tOR: 0.333333\tOF1: 0.304878\n",
      "Train Epoch: 0 [160/1083 (15%)]\tLoss: 0.869117 \tOP: 0.298246\tOR: 0.242857\tOF1: 0.267717\n",
      "Train Epoch: 0 [192/1083 (18%)]\tLoss: 0.860602 \tOP: 0.209302\tOR: 0.140625\tOF1: 0.168224\n",
      "Train Epoch: 0 [224/1083 (21%)]\tLoss: 0.846014 \tOP: 0.147059\tOR: 0.070423\tOF1: 0.095238\n",
      "Train Epoch: 0 [256/1083 (24%)]\tLoss: 0.836337 \tOP: 0.205882\tOR: 0.101449\tOF1: 0.135922\n",
      "Train Epoch: 0 [288/1083 (26%)]\tLoss: 0.823763 \tOP: 0.218750\tOR: 0.095890\tOF1: 0.133333\n",
      "Train Epoch: 0 [320/1083 (29%)]\tLoss: 0.817207 \tOP: 0.115385\tOR: 0.044776\tOF1: 0.064516\n",
      "Train Epoch: 0 [352/1083 (32%)]\tLoss: 0.807347 \tOP: 0.178571\tOR: 0.074627\tOF1: 0.105263\n",
      "Train Epoch: 0 [384/1083 (35%)]\tLoss: 0.796841 \tOP: 0.040000\tOR: 0.013699\tOF1: 0.020408\n",
      "Train Epoch: 0 [416/1083 (38%)]\tLoss: 0.793245 \tOP: 0.120000\tOR: 0.053571\tOF1: 0.074074\n",
      "Train Epoch: 0 [448/1083 (41%)]\tLoss: 0.783064 \tOP: 0.040000\tOR: 0.014706\tOF1: 0.021505\n",
      "Train Epoch: 0 [480/1083 (44%)]\tLoss: 0.774554 \tOP: 0.041667\tOR: 0.014925\tOF1: 0.021978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-bbf89c8d258d>:40: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [512/1083 (47%)]\tLoss: 0.771123 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [544/1083 (50%)]\tLoss: 0.762891 \tOP: 0.041667\tOR: 0.014925\tOF1: 0.021978\n",
      "Train Epoch: 0 [576/1083 (53%)]\tLoss: 0.755874 \tOP: 0.083333\tOR: 0.029851\tOF1: 0.043956\n",
      "Train Epoch: 0 [608/1083 (56%)]\tLoss: 0.751655 \tOP: 0.040000\tOR: 0.014925\tOF1: 0.021739\n",
      "Train Epoch: 0 [640/1083 (59%)]\tLoss: 0.746706 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [672/1083 (62%)]\tLoss: 0.743058 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [704/1083 (65%)]\tLoss: 0.737531 \tOP: 0.041667\tOR: 0.015152\tOF1: 0.022222\n",
      "Train Epoch: 0 [736/1083 (68%)]\tLoss: 0.732539 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [768/1083 (71%)]\tLoss: 0.732089 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [800/1083 (74%)]\tLoss: 0.730384 \tOP: 0.041667\tOR: 0.016667\tOF1: 0.023810\n",
      "Train Epoch: 0 [832/1083 (76%)]\tLoss: 0.725454 \tOP: 0.041667\tOR: 0.015625\tOF1: 0.022727\n",
      "Train Epoch: 0 [864/1083 (79%)]\tLoss: 0.722797 \tOP: 0.041667\tOR: 0.014085\tOF1: 0.021053\n",
      "Train Epoch: 0 [896/1083 (82%)]\tLoss: 0.722954 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [928/1083 (85%)]\tLoss: 0.718700 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [960/1083 (88%)]\tLoss: 0.717791 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [992/1083 (91%)]\tLoss: 0.715714 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [1024/1083 (94%)]\tLoss: 0.713836 \tOP: 0.041667\tOR: 0.013699\tOF1: 0.020619\n",
      "Train Epoch: 0 [891/1083 (97%)]\tLoss: 0.713006 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-a590fe277d9d>:45: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.7098 \n",
      "OP: 0.000000\n",
      "OR: 0.000000\n",
      "OF1: nan\n",
      "\n",
      "Train Epoch: 1 [0/1083 (0%)]\tLoss: 0.709878 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [32/1083 (3%)]\tLoss: 0.709574 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [64/1083 (6%)]\tLoss: 0.708295 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [96/1083 (9%)]\tLoss: 0.706410 \tOP: 0.041667\tOR: 0.014706\tOF1: 0.021739\n",
      "Train Epoch: 1 [128/1083 (12%)]\tLoss: 0.705194 \tOP: 0.153846\tOR: 0.059701\tOF1: 0.086022\n",
      "Train Epoch: 1 [160/1083 (15%)]\tLoss: 0.704906 \tOP: 0.041667\tOR: 0.013699\tOF1: 0.020619\n",
      "Train Epoch: 1 [192/1083 (18%)]\tLoss: 0.703846 \tOP: 0.120000\tOR: 0.047619\tOF1: 0.068182\n",
      "Train Epoch: 1 [224/1083 (21%)]\tLoss: 0.704516 \tOP: 0.080000\tOR: 0.031746\tOF1: 0.045455\n",
      "Train Epoch: 1 [256/1083 (24%)]\tLoss: 0.702763 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [288/1083 (26%)]\tLoss: 0.703229 \tOP: 0.083333\tOR: 0.028571\tOF1: 0.042553\n",
      "Train Epoch: 1 [320/1083 (29%)]\tLoss: 0.701199 \tOP: 0.185185\tOR: 0.074627\tOF1: 0.106383\n",
      "Train Epoch: 1 [352/1083 (32%)]\tLoss: 0.701967 \tOP: 0.120000\tOR: 0.044118\tOF1: 0.064516\n",
      "Train Epoch: 1 [384/1083 (35%)]\tLoss: 0.700914 \tOP: 0.178571\tOR: 0.075758\tOF1: 0.106383\n",
      "Train Epoch: 1 [416/1083 (38%)]\tLoss: 0.698615 \tOP: 0.241379\tOR: 0.111111\tOF1: 0.152174\n",
      "Train Epoch: 1 [448/1083 (41%)]\tLoss: 0.701299 \tOP: 0.137931\tOR: 0.057143\tOF1: 0.080808\n",
      "Train Epoch: 1 [480/1083 (44%)]\tLoss: 0.699453 \tOP: 0.225806\tOR: 0.095890\tOF1: 0.134615\n",
      "Train Epoch: 1 [512/1083 (47%)]\tLoss: 0.700507 \tOP: 0.242424\tOR: 0.112676\tOF1: 0.153846\n",
      "Train Epoch: 1 [544/1083 (50%)]\tLoss: 0.697570 \tOP: 0.266667\tOR: 0.115942\tOF1: 0.161616\n",
      "Train Epoch: 1 [576/1083 (53%)]\tLoss: 0.697588 \tOP: 0.272727\tOR: 0.116883\tOF1: 0.163636\n",
      "Train Epoch: 1 [608/1083 (56%)]\tLoss: 0.698073 \tOP: 0.272727\tOR: 0.136364\tOF1: 0.181818\n",
      "Train Epoch: 1 [640/1083 (59%)]\tLoss: 0.701489 \tOP: 0.181818\tOR: 0.083333\tOF1: 0.114286\n",
      "Train Epoch: 1 [672/1083 (62%)]\tLoss: 0.700755 \tOP: 0.156250\tOR: 0.068493\tOF1: 0.095238\n",
      "Train Epoch: 1 [704/1083 (65%)]\tLoss: 0.698960 \tOP: 0.241379\tOR: 0.098592\tOF1: 0.140000\n",
      "Train Epoch: 1 [736/1083 (68%)]\tLoss: 0.697631 \tOP: 0.272727\tOR: 0.152542\tOF1: 0.195652\n",
      "Train Epoch: 1 [768/1083 (71%)]\tLoss: 0.697458 \tOP: 0.172414\tOR: 0.076923\tOF1: 0.106383\n",
      "Train Epoch: 1 [800/1083 (74%)]\tLoss: 0.695857 \tOP: 0.281250\tOR: 0.123288\tOF1: 0.171429\n",
      "Train Epoch: 1 [832/1083 (76%)]\tLoss: 0.696889 \tOP: 0.281250\tOR: 0.136364\tOF1: 0.183673\n",
      "Train Epoch: 1 [864/1083 (79%)]\tLoss: 0.696245 \tOP: 0.178571\tOR: 0.069444\tOF1: 0.100000\n",
      "Train Epoch: 1 [896/1083 (82%)]\tLoss: 0.695197 \tOP: 0.281250\tOR: 0.113924\tOF1: 0.162162\n",
      "Train Epoch: 1 [928/1083 (85%)]\tLoss: 0.694712 \tOP: 0.323529\tOR: 0.159420\tOF1: 0.213592\n",
      "Train Epoch: 1 [960/1083 (88%)]\tLoss: 0.695967 \tOP: 0.250000\tOR: 0.126984\tOF1: 0.168421\n",
      "Train Epoch: 1 [992/1083 (91%)]\tLoss: 0.695823 \tOP: 0.241379\tOR: 0.109375\tOF1: 0.150538\n",
      "Train Epoch: 1 [1024/1083 (94%)]\tLoss: 0.694591 \tOP: 0.323529\tOR: 0.174603\tOF1: 0.226804\n",
      "Train Epoch: 1 [891/1083 (97%)]\tLoss: 0.696761 \tOP: 0.250000\tOR: 0.142857\tOF1: 0.181818\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6969 \n",
      "OP: 0.185185\n",
      "OR: 0.142857\n",
      "OF1: 0.161290\n",
      "\n",
      "Train Epoch: 2 [0/1083 (0%)]\tLoss: 0.693150 \tOP: 0.352941\tOR: 0.157895\tOF1: 0.218182\n",
      "Train Epoch: 2 [32/1083 (3%)]\tLoss: 0.691630 \tOP: 0.388889\tOR: 0.205882\tOF1: 0.269231\n",
      "Train Epoch: 2 [64/1083 (6%)]\tLoss: 0.691641 \tOP: 0.371429\tOR: 0.196970\tOF1: 0.257426\n",
      "Train Epoch: 2 [96/1083 (9%)]\tLoss: 0.692804 \tOP: 0.333333\tOR: 0.150685\tOF1: 0.207547\n",
      "Train Epoch: 2 [128/1083 (12%)]\tLoss: 0.692461 \tOP: 0.361111\tOR: 0.206349\tOF1: 0.262626\n",
      "Train Epoch: 2 [160/1083 (15%)]\tLoss: 0.691912 \tOP: 0.371429\tOR: 0.203125\tOF1: 0.262626\n",
      "Train Epoch: 2 [192/1083 (18%)]\tLoss: 0.690336 \tOP: 0.421053\tOR: 0.250000\tOF1: 0.313725\n",
      "Train Epoch: 2 [224/1083 (21%)]\tLoss: 0.691692 \tOP: 0.388889\tOR: 0.202899\tOF1: 0.266667\n",
      "Train Epoch: 2 [256/1083 (24%)]\tLoss: 0.693276 \tOP: 0.323529\tOR: 0.177419\tOF1: 0.229167\n",
      "Train Epoch: 2 [288/1083 (26%)]\tLoss: 0.690619 \tOP: 0.405405\tOR: 0.234375\tOF1: 0.297030\n",
      "Train Epoch: 2 [320/1083 (29%)]\tLoss: 0.692180 \tOP: 0.378378\tOR: 0.191781\tOF1: 0.254545\n",
      "Train Epoch: 2 [352/1083 (32%)]\tLoss: 0.691823 \tOP: 0.361111\tOR: 0.196970\tOF1: 0.254902\n",
      "Train Epoch: 2 [384/1083 (35%)]\tLoss: 0.690276 \tOP: 0.410256\tOR: 0.225352\tOF1: 0.290909\n",
      "Train Epoch: 2 [416/1083 (38%)]\tLoss: 0.689772 \tOP: 0.421053\tOR: 0.258065\tOF1: 0.320000\n",
      "Train Epoch: 2 [448/1083 (41%)]\tLoss: 0.691291 \tOP: 0.394737\tOR: 0.189873\tOF1: 0.256410\n",
      "Train Epoch: 2 [480/1083 (44%)]\tLoss: 0.690342 \tOP: 0.435897\tOR: 0.265625\tOF1: 0.330097\n",
      "Train Epoch: 2 [512/1083 (47%)]\tLoss: 0.691811 \tOP: 0.378378\tOR: 0.208955\tOF1: 0.269231\n",
      "Train Epoch: 2 [544/1083 (50%)]\tLoss: 0.689856 \tOP: 0.414634\tOR: 0.261538\tOF1: 0.320755\n",
      "Train Epoch: 2 [576/1083 (53%)]\tLoss: 0.689398 \tOP: 0.435897\tOR: 0.253731\tOF1: 0.320755\n",
      "Train Epoch: 2 [608/1083 (56%)]\tLoss: 0.693819 \tOP: 0.324324\tOR: 0.173913\tOF1: 0.226415\n",
      "Train Epoch: 2 [640/1083 (59%)]\tLoss: 0.690071 \tOP: 0.378378\tOR: 0.186667\tOF1: 0.250000\n",
      "Train Epoch: 2 [672/1083 (62%)]\tLoss: 0.691073 \tOP: 0.378378\tOR: 0.215385\tOF1: 0.274510\n",
      "Train Epoch: 2 [704/1083 (65%)]\tLoss: 0.689752 \tOP: 0.439024\tOR: 0.285714\tOF1: 0.346154\n",
      "Train Epoch: 2 [736/1083 (68%)]\tLoss: 0.687895 \tOP: 0.476190\tOR: 0.273973\tOF1: 0.347826\n",
      "Train Epoch: 2 [768/1083 (71%)]\tLoss: 0.689169 \tOP: 0.450000\tOR: 0.264706\tOF1: 0.333333\n",
      "Train Epoch: 2 [800/1083 (74%)]\tLoss: 0.688378 \tOP: 0.435897\tOR: 0.226667\tOF1: 0.298246\n",
      "Train Epoch: 2 [832/1083 (76%)]\tLoss: 0.689121 \tOP: 0.421053\tOR: 0.238806\tOF1: 0.304762\n",
      "Train Epoch: 2 [864/1083 (79%)]\tLoss: 0.691449 \tOP: 0.378378\tOR: 0.212121\tOF1: 0.271845\n",
      "Train Epoch: 2 [896/1083 (82%)]\tLoss: 0.690880 \tOP: 0.368421\tOR: 0.205882\tOF1: 0.264151\n",
      "Train Epoch: 2 [928/1083 (85%)]\tLoss: 0.690178 \tOP: 0.421053\tOR: 0.216216\tOF1: 0.285714\n",
      "Train Epoch: 2 [960/1083 (88%)]\tLoss: 0.691042 \tOP: 0.358974\tOR: 0.215385\tOF1: 0.269231\n",
      "Train Epoch: 2 [992/1083 (91%)]\tLoss: 0.690869 \tOP: 0.400000\tOR: 0.246154\tOF1: 0.304762\n",
      "Train Epoch: 2 [1024/1083 (94%)]\tLoss: 0.690709 \tOP: 0.405405\tOR: 0.214286\tOF1: 0.280374\n",
      "Train Epoch: 2 [891/1083 (97%)]\tLoss: 0.690344 \tOP: 0.361111\tOR: 0.209677\tOF1: 0.265306\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6957 \n",
      "OP: 0.225806\n",
      "OR: 0.200000\n",
      "OF1: 0.212121\n",
      "\n",
      "Train Epoch: 3 [0/1083 (0%)]\tLoss: 0.687409 \tOP: 0.465116\tOR: 0.266667\tOF1: 0.338983\n",
      "Train Epoch: 3 [32/1083 (3%)]\tLoss: 0.688813 \tOP: 0.435897\tOR: 0.265625\tOF1: 0.330097\n",
      "Train Epoch: 3 [64/1083 (6%)]\tLoss: 0.690227 \tOP: 0.388889\tOR: 0.202899\tOF1: 0.266667\n",
      "Train Epoch: 3 [96/1083 (9%)]\tLoss: 0.686471 \tOP: 0.476190\tOR: 0.281690\tOF1: 0.353982\n",
      "Train Epoch: 3 [128/1083 (12%)]\tLoss: 0.687481 \tOP: 0.463415\tOR: 0.260274\tOF1: 0.333333\n",
      "Train Epoch: 3 [160/1083 (15%)]\tLoss: 0.687974 \tOP: 0.425000\tOR: 0.232877\tOF1: 0.300885\n",
      "Train Epoch: 3 [192/1083 (18%)]\tLoss: 0.687720 \tOP: 0.450000\tOR: 0.264706\tOF1: 0.333333\n",
      "Train Epoch: 3 [224/1083 (21%)]\tLoss: 0.689651 \tOP: 0.405405\tOR: 0.202703\tOF1: 0.270270\n",
      "Train Epoch: 3 [256/1083 (24%)]\tLoss: 0.686362 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 3 [288/1083 (26%)]\tLoss: 0.686287 \tOP: 0.476190\tOR: 0.307692\tOF1: 0.373832\n",
      "Train Epoch: 3 [320/1083 (29%)]\tLoss: 0.687664 \tOP: 0.463415\tOR: 0.246753\tOF1: 0.322034\n",
      "Train Epoch: 3 [352/1083 (32%)]\tLoss: 0.689382 \tOP: 0.405405\tOR: 0.230769\tOF1: 0.294118\n",
      "Train Epoch: 3 [384/1083 (35%)]\tLoss: 0.687442 \tOP: 0.463415\tOR: 0.263889\tOF1: 0.336283\n",
      "Train Epoch: 3 [416/1083 (38%)]\tLoss: 0.686301 \tOP: 0.463415\tOR: 0.292308\tOF1: 0.358491\n",
      "Train Epoch: 3 [448/1083 (41%)]\tLoss: 0.685782 \tOP: 0.476190\tOR: 0.294118\tOF1: 0.363636\n",
      "Train Epoch: 3 [480/1083 (44%)]\tLoss: 0.683828 \tOP: 0.541667\tOR: 0.371429\tOF1: 0.440678\n",
      "Train Epoch: 3 [512/1083 (47%)]\tLoss: 0.686501 \tOP: 0.463415\tOR: 0.296875\tOF1: 0.361905\n",
      "Train Epoch: 3 [544/1083 (50%)]\tLoss: 0.687281 \tOP: 0.476190\tOR: 0.312500\tOF1: 0.377358\n",
      "Train Epoch: 3 [576/1083 (53%)]\tLoss: 0.687413 \tOP: 0.476190\tOR: 0.307692\tOF1: 0.373832\n",
      "Train Epoch: 3 [608/1083 (56%)]\tLoss: 0.683728 \tOP: 0.531915\tOR: 0.342466\tOF1: 0.416667\n",
      "Train Epoch: 3 [640/1083 (59%)]\tLoss: 0.687686 \tOP: 0.450000\tOR: 0.250000\tOF1: 0.321429\n",
      "Train Epoch: 3 [672/1083 (62%)]\tLoss: 0.686078 \tOP: 0.476190\tOR: 0.285714\tOF1: 0.357143\n",
      "Train Epoch: 3 [704/1083 (65%)]\tLoss: 0.686399 \tOP: 0.488372\tOR: 0.308824\tOF1: 0.378378\n",
      "Train Epoch: 3 [736/1083 (68%)]\tLoss: 0.686897 \tOP: 0.463415\tOR: 0.287879\tOF1: 0.355140\n",
      "Train Epoch: 3 [768/1083 (71%)]\tLoss: 0.686059 \tOP: 0.476190\tOR: 0.303030\tOF1: 0.370370\n",
      "Train Epoch: 3 [800/1083 (74%)]\tLoss: 0.689533 \tOP: 0.414634\tOR: 0.250000\tOF1: 0.311927\n",
      "Train Epoch: 3 [832/1083 (76%)]\tLoss: 0.688639 \tOP: 0.425000\tOR: 0.283333\tOF1: 0.340000\n",
      "Train Epoch: 3 [864/1083 (79%)]\tLoss: 0.686006 \tOP: 0.488372\tOR: 0.300000\tOF1: 0.371681\n",
      "Train Epoch: 3 [896/1083 (82%)]\tLoss: 0.685281 \tOP: 0.500000\tOR: 0.318841\tOF1: 0.389381\n",
      "Train Epoch: 3 [928/1083 (85%)]\tLoss: 0.684959 \tOP: 0.488372\tOR: 0.291667\tOF1: 0.365217\n",
      "Train Epoch: 3 [960/1083 (88%)]\tLoss: 0.686049 \tOP: 0.488372\tOR: 0.338710\tOF1: 0.400000\n",
      "Train Epoch: 3 [992/1083 (91%)]\tLoss: 0.686269 \tOP: 0.465116\tOR: 0.298507\tOF1: 0.363636\n",
      "Train Epoch: 3 [1024/1083 (94%)]\tLoss: 0.686184 \tOP: 0.476190\tOR: 0.307692\tOF1: 0.373832\n",
      "Train Epoch: 3 [891/1083 (97%)]\tLoss: 0.686304 \tOP: 0.435897\tOR: 0.333333\tOF1: 0.377778\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6943 \n",
      "OP: 0.200000\n",
      "OR: 0.171429\n",
      "OF1: 0.184615\n",
      "\n",
      "Train Epoch: 4 [0/1083 (0%)]\tLoss: 0.682708 \tOP: 0.541667\tOR: 0.376812\tOF1: 0.444444\n",
      "Train Epoch: 4 [32/1083 (3%)]\tLoss: 0.685719 \tOP: 0.488372\tOR: 0.295775\tOF1: 0.368421\n",
      "Train Epoch: 4 [64/1083 (6%)]\tLoss: 0.686907 \tOP: 0.435897\tOR: 0.253731\tOF1: 0.320755\n",
      "Train Epoch: 4 [96/1083 (9%)]\tLoss: 0.685779 \tOP: 0.488889\tOR: 0.343750\tOF1: 0.403670\n",
      "Train Epoch: 4 [128/1083 (12%)]\tLoss: 0.684695 \tOP: 0.500000\tOR: 0.314286\tOF1: 0.385965\n",
      "Train Epoch: 4 [160/1083 (15%)]\tLoss: 0.685029 \tOP: 0.500000\tOR: 0.343750\tOF1: 0.407407\n",
      "Train Epoch: 4 [192/1083 (18%)]\tLoss: 0.685753 \tOP: 0.463415\tOR: 0.275362\tOF1: 0.345455\n",
      "Train Epoch: 4 [224/1083 (21%)]\tLoss: 0.684225 \tOP: 0.511111\tOR: 0.294872\tOF1: 0.373984\n",
      "Train Epoch: 4 [256/1083 (24%)]\tLoss: 0.686656 \tOP: 0.452381\tOR: 0.306452\tOF1: 0.365385\n",
      "Train Epoch: 4 [288/1083 (26%)]\tLoss: 0.684674 \tOP: 0.511111\tOR: 0.389831\tOF1: 0.442308\n",
      "Train Epoch: 4 [320/1083 (29%)]\tLoss: 0.684867 \tOP: 0.500000\tOR: 0.338462\tOF1: 0.403670\n",
      "Train Epoch: 4 [352/1083 (32%)]\tLoss: 0.685368 \tOP: 0.488372\tOR: 0.269231\tOF1: 0.347107\n",
      "Train Epoch: 4 [384/1083 (35%)]\tLoss: 0.686410 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 4 [416/1083 (38%)]\tLoss: 0.685102 \tOP: 0.500000\tOR: 0.353846\tOF1: 0.414414\n",
      "Train Epoch: 4 [448/1083 (41%)]\tLoss: 0.683028 \tOP: 0.531915\tOR: 0.357143\tOF1: 0.427350\n",
      "Train Epoch: 4 [480/1083 (44%)]\tLoss: 0.685627 \tOP: 0.476190\tOR: 0.273973\tOF1: 0.347826\n",
      "Train Epoch: 4 [512/1083 (47%)]\tLoss: 0.683074 \tOP: 0.531915\tOR: 0.367647\tOF1: 0.434783\n",
      "Train Epoch: 4 [544/1083 (50%)]\tLoss: 0.682642 \tOP: 0.541667\tOR: 0.366197\tOF1: 0.436975\n",
      "Train Epoch: 4 [576/1083 (53%)]\tLoss: 0.681862 \tOP: 0.551020\tOR: 0.409091\tOF1: 0.469565\n",
      "Train Epoch: 4 [608/1083 (56%)]\tLoss: 0.685076 \tOP: 0.488889\tOR: 0.349206\tOF1: 0.407407\n",
      "Train Epoch: 4 [640/1083 (59%)]\tLoss: 0.686570 \tOP: 0.465116\tOR: 0.307692\tOF1: 0.370370\n",
      "Train Epoch: 4 [672/1083 (62%)]\tLoss: 0.684746 \tOP: 0.488372\tOR: 0.300000\tOF1: 0.371681\n",
      "Train Epoch: 4 [704/1083 (65%)]\tLoss: 0.687473 \tOP: 0.425000\tOR: 0.261538\tOF1: 0.323810\n",
      "Train Epoch: 4 [736/1083 (68%)]\tLoss: 0.685416 \tOP: 0.488372\tOR: 0.300000\tOF1: 0.371681\n",
      "Train Epoch: 4 [768/1083 (71%)]\tLoss: 0.685093 \tOP: 0.476190\tOR: 0.270270\tOF1: 0.344828\n",
      "Train Epoch: 4 [800/1083 (74%)]\tLoss: 0.683528 \tOP: 0.511111\tOR: 0.315068\tOF1: 0.389831\n",
      "Train Epoch: 4 [832/1083 (76%)]\tLoss: 0.683899 \tOP: 0.511111\tOR: 0.338235\tOF1: 0.407080\n",
      "Train Epoch: 4 [864/1083 (79%)]\tLoss: 0.684016 \tOP: 0.521739\tOR: 0.393443\tOF1: 0.448598\n",
      "Train Epoch: 4 [896/1083 (82%)]\tLoss: 0.682370 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 4 [928/1083 (85%)]\tLoss: 0.686928 \tOP: 0.439024\tOR: 0.257143\tOF1: 0.324324\n",
      "Train Epoch: 4 [960/1083 (88%)]\tLoss: 0.687207 \tOP: 0.435897\tOR: 0.250000\tOF1: 0.317757\n",
      "Train Epoch: 4 [992/1083 (91%)]\tLoss: 0.686357 \tOP: 0.450000\tOR: 0.257143\tOF1: 0.327273\n",
      "Train Epoch: 4 [1024/1083 (94%)]\tLoss: 0.686177 \tOP: 0.450000\tOR: 0.250000\tOF1: 0.321429\n",
      "Train Epoch: 4 [891/1083 (97%)]\tLoss: 0.683015 \tOP: 0.476190\tOR: 0.357143\tOF1: 0.408163\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6942 \n",
      "OP: 0.277778\n",
      "OR: 0.285714\n",
      "OF1: 0.281690\n",
      "\n",
      "Train Epoch: 5 [0/1083 (0%)]\tLoss: 0.684484 \tOP: 0.488372\tOR: 0.291667\tOF1: 0.365217\n",
      "Train Epoch: 5 [32/1083 (3%)]\tLoss: 0.682590 \tOP: 0.521739\tOR: 0.338028\tOF1: 0.410256\n",
      "Train Epoch: 5 [64/1083 (6%)]\tLoss: 0.685281 \tOP: 0.476190\tOR: 0.289855\tOF1: 0.360360\n",
      "Train Epoch: 5 [96/1083 (9%)]\tLoss: 0.683059 \tOP: 0.521739\tOR: 0.358209\tOF1: 0.424779\n",
      "Train Epoch: 5 [128/1083 (12%)]\tLoss: 0.683823 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 5 [160/1083 (15%)]\tLoss: 0.684852 \tOP: 0.476190\tOR: 0.294118\tOF1: 0.363636\n",
      "Train Epoch: 5 [192/1083 (18%)]\tLoss: 0.684341 \tOP: 0.488889\tOR: 0.309859\tOF1: 0.379310\n",
      "Train Epoch: 5 [224/1083 (21%)]\tLoss: 0.685294 \tOP: 0.463415\tOR: 0.292308\tOF1: 0.358491\n",
      "Train Epoch: 5 [256/1083 (24%)]\tLoss: 0.681710 \tOP: 0.551020\tOR: 0.435484\tOF1: 0.486486\n",
      "Train Epoch: 5 [288/1083 (26%)]\tLoss: 0.685493 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 5 [320/1083 (29%)]\tLoss: 0.684237 \tOP: 0.488372\tOR: 0.323077\tOF1: 0.388889\n",
      "Train Epoch: 5 [352/1083 (32%)]\tLoss: 0.681343 \tOP: 0.551020\tOR: 0.415385\tOF1: 0.473684\n",
      "Train Epoch: 5 [384/1083 (35%)]\tLoss: 0.685378 \tOP: 0.465116\tOR: 0.285714\tOF1: 0.353982\n",
      "Train Epoch: 5 [416/1083 (38%)]\tLoss: 0.681223 \tOP: 0.551020\tOR: 0.415385\tOF1: 0.473684\n",
      "Train Epoch: 5 [448/1083 (41%)]\tLoss: 0.682598 \tOP: 0.531915\tOR: 0.390625\tOF1: 0.450450\n",
      "Train Epoch: 5 [480/1083 (44%)]\tLoss: 0.684790 \tOP: 0.488372\tOR: 0.338710\tOF1: 0.400000\n",
      "Train Epoch: 5 [512/1083 (47%)]\tLoss: 0.684926 \tOP: 0.488889\tOR: 0.360656\tOF1: 0.415094\n",
      "Train Epoch: 5 [544/1083 (50%)]\tLoss: 0.684403 \tOP: 0.488372\tOR: 0.291667\tOF1: 0.365217\n",
      "Train Epoch: 5 [576/1083 (53%)]\tLoss: 0.683760 \tOP: 0.500000\tOR: 0.301370\tOF1: 0.376068\n",
      "Train Epoch: 5 [608/1083 (56%)]\tLoss: 0.683439 \tOP: 0.511111\tOR: 0.310811\tOF1: 0.386555\n",
      "Train Epoch: 5 [640/1083 (59%)]\tLoss: 0.682320 \tOP: 0.531915\tOR: 0.337838\tOF1: 0.413223\n",
      "Train Epoch: 5 [672/1083 (62%)]\tLoss: 0.685115 \tOP: 0.488372\tOR: 0.269231\tOF1: 0.347107\n",
      "Train Epoch: 5 [704/1083 (65%)]\tLoss: 0.682060 \tOP: 0.531915\tOR: 0.342466\tOF1: 0.416667\n",
      "Train Epoch: 5 [736/1083 (68%)]\tLoss: 0.680788 \tOP: 0.560000\tOR: 0.378378\tOF1: 0.451613\n",
      "Train Epoch: 5 [768/1083 (71%)]\tLoss: 0.686521 \tOP: 0.439024\tOR: 0.264706\tOF1: 0.330275\n",
      "Train Epoch: 5 [800/1083 (74%)]\tLoss: 0.682096 \tOP: 0.531915\tOR: 0.378788\tOF1: 0.442478\n",
      "Train Epoch: 5 [832/1083 (76%)]\tLoss: 0.685372 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 5 [864/1083 (79%)]\tLoss: 0.681699 \tOP: 0.541667\tOR: 0.393939\tOF1: 0.456140\n",
      "Train Epoch: 5 [896/1083 (82%)]\tLoss: 0.683604 \tOP: 0.500000\tOR: 0.392857\tOF1: 0.440000\n",
      "Train Epoch: 5 [928/1083 (85%)]\tLoss: 0.685745 \tOP: 0.463415\tOR: 0.279412\tOF1: 0.348624\n",
      "Train Epoch: 5 [960/1083 (88%)]\tLoss: 0.682443 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 5 [992/1083 (91%)]\tLoss: 0.683881 \tOP: 0.500000\tOR: 0.328358\tOF1: 0.396396\n",
      "Train Epoch: 5 [1024/1083 (94%)]\tLoss: 0.682500 \tOP: 0.531915\tOR: 0.378788\tOF1: 0.442478\n",
      "Train Epoch: 5 [891/1083 (97%)]\tLoss: 0.684913 \tOP: 0.450000\tOR: 0.300000\tOF1: 0.360000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6950 \n",
      "OP: 0.205882\n",
      "OR: 0.200000\n",
      "OF1: 0.202899\n",
      "\n",
      "Train Epoch: 6 [0/1083 (0%)]\tLoss: 0.681892 \tOP: 0.541667\tOR: 0.456140\tOF1: 0.495238\n",
      "Train Epoch: 6 [32/1083 (3%)]\tLoss: 0.680715 \tOP: 0.560000\tOR: 0.400000\tOF1: 0.466667\n",
      "Train Epoch: 6 [64/1083 (6%)]\tLoss: 0.684214 \tOP: 0.488372\tOR: 0.313433\tOF1: 0.381818\n",
      "Train Epoch: 6 [96/1083 (9%)]\tLoss: 0.683161 \tOP: 0.511111\tOR: 0.348485\tOF1: 0.414414\n",
      "Train Epoch: 6 [128/1083 (12%)]\tLoss: 0.685317 \tOP: 0.450000\tOR: 0.250000\tOF1: 0.321429\n",
      "Train Epoch: 6 [160/1083 (15%)]\tLoss: 0.681709 \tOP: 0.541667\tOR: 0.406250\tOF1: 0.464286\n",
      "Train Epoch: 6 [192/1083 (18%)]\tLoss: 0.681717 \tOP: 0.541667\tOR: 0.351351\tOF1: 0.426230\n",
      "Train Epoch: 6 [224/1083 (21%)]\tLoss: 0.682130 \tOP: 0.531915\tOR: 0.367647\tOF1: 0.434783\n",
      "Train Epoch: 6 [256/1083 (24%)]\tLoss: 0.685169 \tOP: 0.463415\tOR: 0.306452\tOF1: 0.368932\n",
      "Train Epoch: 6 [288/1083 (26%)]\tLoss: 0.684308 \tOP: 0.488372\tOR: 0.308824\tOF1: 0.378378\n",
      "Train Epoch: 6 [320/1083 (29%)]\tLoss: 0.683500 \tOP: 0.500000\tOR: 0.353846\tOF1: 0.414414\n",
      "Train Epoch: 6 [352/1083 (32%)]\tLoss: 0.682820 \tOP: 0.511111\tOR: 0.353846\tOF1: 0.418182\n",
      "Train Epoch: 6 [384/1083 (35%)]\tLoss: 0.683303 \tOP: 0.511111\tOR: 0.365079\tOF1: 0.425926\n",
      "Train Epoch: 6 [416/1083 (38%)]\tLoss: 0.683468 \tOP: 0.500000\tOR: 0.323529\tOF1: 0.392857\n",
      "Train Epoch: 6 [448/1083 (41%)]\tLoss: 0.685316 \tOP: 0.476190\tOR: 0.281690\tOF1: 0.353982\n",
      "Train Epoch: 6 [480/1083 (44%)]\tLoss: 0.684093 \tOP: 0.488372\tOR: 0.283784\tOF1: 0.358974\n",
      "Train Epoch: 6 [512/1083 (47%)]\tLoss: 0.684540 \tOP: 0.477273\tOR: 0.304348\tOF1: 0.371681\n",
      "Train Epoch: 6 [544/1083 (50%)]\tLoss: 0.682980 \tOP: 0.511111\tOR: 0.333333\tOF1: 0.403509\n",
      "Train Epoch: 6 [576/1083 (53%)]\tLoss: 0.683512 \tOP: 0.500000\tOR: 0.318841\tOF1: 0.389381\n",
      "Train Epoch: 6 [608/1083 (56%)]\tLoss: 0.680222 \tOP: 0.568627\tOR: 0.376623\tOF1: 0.453125\n",
      "Train Epoch: 6 [640/1083 (59%)]\tLoss: 0.681933 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 6 [672/1083 (62%)]\tLoss: 0.682480 \tOP: 0.521739\tOR: 0.358209\tOF1: 0.424779\n",
      "Train Epoch: 6 [704/1083 (65%)]\tLoss: 0.682053 \tOP: 0.551020\tOR: 0.380282\tOF1: 0.450000\n",
      "Train Epoch: 6 [736/1083 (68%)]\tLoss: 0.683985 \tOP: 0.488372\tOR: 0.291667\tOF1: 0.365217\n",
      "Train Epoch: 6 [768/1083 (71%)]\tLoss: 0.682945 \tOP: 0.511111\tOR: 0.328571\tOF1: 0.400000\n",
      "Train Epoch: 6 [800/1083 (74%)]\tLoss: 0.681945 \tOP: 0.541667\tOR: 0.406250\tOF1: 0.464286\n",
      "Train Epoch: 6 [832/1083 (76%)]\tLoss: 0.683971 \tOP: 0.488372\tOR: 0.338710\tOF1: 0.400000\n",
      "Train Epoch: 6 [864/1083 (79%)]\tLoss: 0.682767 \tOP: 0.510638\tOR: 0.358209\tOF1: 0.421053\n",
      "Train Epoch: 6 [896/1083 (82%)]\tLoss: 0.684141 \tOP: 0.488372\tOR: 0.318182\tOF1: 0.385321\n",
      "Train Epoch: 6 [928/1083 (85%)]\tLoss: 0.684140 \tOP: 0.476190\tOR: 0.281690\tOF1: 0.353982\n",
      "Train Epoch: 6 [960/1083 (88%)]\tLoss: 0.684093 \tOP: 0.476190\tOR: 0.289855\tOF1: 0.360360\n",
      "Train Epoch: 6 [992/1083 (91%)]\tLoss: 0.680934 \tOP: 0.551020\tOR: 0.341772\tOF1: 0.421875\n",
      "Train Epoch: 6 [1024/1083 (94%)]\tLoss: 0.681872 \tOP: 0.531915\tOR: 0.390625\tOF1: 0.450450\n",
      "Train Epoch: 6 [891/1083 (97%)]\tLoss: 0.678455 \tOP: 0.551020\tOR: 0.465517\tOF1: 0.504673\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6933 \n",
      "OP: 0.193548\n",
      "OR: 0.171429\n",
      "OF1: 0.181818\n",
      "\n",
      "Train Epoch: 7 [0/1083 (0%)]\tLoss: 0.682847 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 7 [32/1083 (3%)]\tLoss: 0.681574 \tOP: 0.541667\tOR: 0.376812\tOF1: 0.444444\n",
      "Train Epoch: 7 [64/1083 (6%)]\tLoss: 0.687695 \tOP: 0.384615\tOR: 0.211268\tOF1: 0.272727\n",
      "Train Epoch: 7 [96/1083 (9%)]\tLoss: 0.684764 \tOP: 0.476190\tOR: 0.285714\tOF1: 0.357143\n",
      "Train Epoch: 7 [128/1083 (12%)]\tLoss: 0.681587 \tOP: 0.541667\tOR: 0.356164\tOF1: 0.429752\n",
      "Train Epoch: 7 [160/1083 (15%)]\tLoss: 0.679810 \tOP: 0.568627\tOR: 0.432836\tOF1: 0.491525\n",
      "Train Epoch: 7 [192/1083 (18%)]\tLoss: 0.680975 \tOP: 0.551020\tOR: 0.415385\tOF1: 0.473684\n",
      "Train Epoch: 7 [224/1083 (21%)]\tLoss: 0.682180 \tOP: 0.521739\tOR: 0.311688\tOF1: 0.390244\n",
      "Train Epoch: 7 [256/1083 (24%)]\tLoss: 0.681341 \tOP: 0.541667\tOR: 0.366197\tOF1: 0.436975\n",
      "Train Epoch: 7 [288/1083 (26%)]\tLoss: 0.683677 \tOP: 0.500000\tOR: 0.333333\tOF1: 0.400000\n",
      "Train Epoch: 7 [320/1083 (29%)]\tLoss: 0.681622 \tOP: 0.531915\tOR: 0.367647\tOF1: 0.434783\n",
      "Train Epoch: 7 [352/1083 (32%)]\tLoss: 0.684731 \tOP: 0.476190\tOR: 0.303030\tOF1: 0.370370\n",
      "Train Epoch: 7 [384/1083 (35%)]\tLoss: 0.682267 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 7 [416/1083 (38%)]\tLoss: 0.684911 \tOP: 0.477273\tOR: 0.362069\tOF1: 0.411765\n",
      "Train Epoch: 7 [448/1083 (41%)]\tLoss: 0.685144 \tOP: 0.477273\tOR: 0.308824\tOF1: 0.375000\n",
      "Train Epoch: 7 [480/1083 (44%)]\tLoss: 0.682075 \tOP: 0.531915\tOR: 0.384615\tOF1: 0.446429\n",
      "Train Epoch: 7 [512/1083 (47%)]\tLoss: 0.681680 \tOP: 0.531915\tOR: 0.357143\tOF1: 0.427350\n",
      "Train Epoch: 7 [544/1083 (50%)]\tLoss: 0.682141 \tOP: 0.521739\tOR: 0.358209\tOF1: 0.424779\n",
      "Train Epoch: 7 [576/1083 (53%)]\tLoss: 0.682052 \tOP: 0.521739\tOR: 0.324324\tOF1: 0.400000\n",
      "Train Epoch: 7 [608/1083 (56%)]\tLoss: 0.681134 \tOP: 0.551020\tOR: 0.380282\tOF1: 0.450000\n",
      "Train Epoch: 7 [640/1083 (59%)]\tLoss: 0.680890 \tOP: 0.551020\tOR: 0.421875\tOF1: 0.477876\n",
      "Train Epoch: 7 [672/1083 (62%)]\tLoss: 0.684646 \tOP: 0.465116\tOR: 0.294118\tOF1: 0.360360\n",
      "Train Epoch: 7 [704/1083 (65%)]\tLoss: 0.682376 \tOP: 0.531915\tOR: 0.403226\tOF1: 0.458716\n",
      "Train Epoch: 7 [736/1083 (68%)]\tLoss: 0.682965 \tOP: 0.511111\tOR: 0.310811\tOF1: 0.386555\n",
      "Train Epoch: 7 [768/1083 (71%)]\tLoss: 0.684076 \tOP: 0.488372\tOR: 0.287671\tOF1: 0.362069\n",
      "Train Epoch: 7 [800/1083 (74%)]\tLoss: 0.681853 \tOP: 0.531915\tOR: 0.342466\tOF1: 0.416667\n",
      "Train Epoch: 7 [832/1083 (76%)]\tLoss: 0.687241 \tOP: 0.421053\tOR: 0.228571\tOF1: 0.296296\n",
      "Train Epoch: 7 [864/1083 (79%)]\tLoss: 0.684743 \tOP: 0.465116\tOR: 0.289855\tOF1: 0.357143\n",
      "Train Epoch: 7 [896/1083 (82%)]\tLoss: 0.680494 \tOP: 0.560000\tOR: 0.400000\tOF1: 0.466667\n",
      "Train Epoch: 7 [928/1083 (85%)]\tLoss: 0.682150 \tOP: 0.541667\tOR: 0.426230\tOF1: 0.477064\n",
      "Train Epoch: 7 [960/1083 (88%)]\tLoss: 0.680917 \tOP: 0.551020\tOR: 0.442623\tOF1: 0.490909\n",
      "Train Epoch: 7 [992/1083 (91%)]\tLoss: 0.681952 \tOP: 0.541667\tOR: 0.376812\tOF1: 0.444444\n",
      "Train Epoch: 7 [1024/1083 (94%)]\tLoss: 0.683858 \tOP: 0.500000\tOR: 0.328571\tOF1: 0.396552\n",
      "Train Epoch: 7 [891/1083 (97%)]\tLoss: 0.681979 \tOP: 0.488372\tOR: 0.344262\tOF1: 0.403846\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6941 \n",
      "OP: 0.257143\n",
      "OR: 0.257143\n",
      "OF1: 0.257143\n",
      "\n",
      "Train Epoch: 8 [0/1083 (0%)]\tLoss: 0.684440 \tOP: 0.476190\tOR: 0.303030\tOF1: 0.370370\n",
      "Train Epoch: 8 [32/1083 (3%)]\tLoss: 0.682455 \tOP: 0.521739\tOR: 0.369231\tOF1: 0.432432\n",
      "Train Epoch: 8 [64/1083 (6%)]\tLoss: 0.682789 \tOP: 0.511111\tOR: 0.333333\tOF1: 0.403509\n",
      "Train Epoch: 8 [96/1083 (9%)]\tLoss: 0.684374 \tOP: 0.476190\tOR: 0.277778\tOF1: 0.350877\n",
      "Train Epoch: 8 [128/1083 (12%)]\tLoss: 0.679049 \tOP: 0.576923\tOR: 0.454545\tOF1: 0.508475\n",
      "Train Epoch: 8 [160/1083 (15%)]\tLoss: 0.682504 \tOP: 0.511111\tOR: 0.338235\tOF1: 0.407080\n",
      "Train Epoch: 8 [192/1083 (18%)]\tLoss: 0.680308 \tOP: 0.580000\tOR: 0.397260\tOF1: 0.471545\n",
      "Train Epoch: 8 [224/1083 (21%)]\tLoss: 0.681177 \tOP: 0.541667\tOR: 0.426230\tOF1: 0.477064\n",
      "Train Epoch: 8 [256/1083 (24%)]\tLoss: 0.680896 \tOP: 0.562500\tOR: 0.409091\tOF1: 0.473684\n",
      "Train Epoch: 8 [288/1083 (26%)]\tLoss: 0.681525 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 8 [320/1083 (29%)]\tLoss: 0.684607 \tOP: 0.477273\tOR: 0.304348\tOF1: 0.371681\n",
      "Train Epoch: 8 [352/1083 (32%)]\tLoss: 0.681683 \tOP: 0.543478\tOR: 0.378788\tOF1: 0.446429\n",
      "Train Epoch: 8 [384/1083 (35%)]\tLoss: 0.687298 \tOP: 0.435897\tOR: 0.242857\tOF1: 0.311927\n",
      "Train Epoch: 8 [416/1083 (38%)]\tLoss: 0.682450 \tOP: 0.531915\tOR: 0.328947\tOF1: 0.406504\n",
      "Train Epoch: 8 [448/1083 (41%)]\tLoss: 0.682208 \tOP: 0.521739\tOR: 0.369231\tOF1: 0.432432\n",
      "Train Epoch: 8 [480/1083 (44%)]\tLoss: 0.680250 \tOP: 0.571429\tOR: 0.424242\tOF1: 0.486957\n",
      "Train Epoch: 8 [512/1083 (47%)]\tLoss: 0.684909 \tOP: 0.500000\tOR: 0.323529\tOF1: 0.392857\n",
      "Train Epoch: 8 [544/1083 (50%)]\tLoss: 0.680943 \tOP: 0.560000\tOR: 0.411765\tOF1: 0.474576\n",
      "Train Epoch: 8 [576/1083 (53%)]\tLoss: 0.680351 \tOP: 0.560000\tOR: 0.411765\tOF1: 0.474576\n",
      "Train Epoch: 8 [608/1083 (56%)]\tLoss: 0.680795 \tOP: 0.551020\tOR: 0.397059\tOF1: 0.461538\n",
      "Train Epoch: 8 [640/1083 (59%)]\tLoss: 0.683879 \tOP: 0.500000\tOR: 0.323944\tOF1: 0.393162\n",
      "Train Epoch: 8 [672/1083 (62%)]\tLoss: 0.686650 \tOP: 0.435897\tOR: 0.220779\tOF1: 0.293103\n",
      "Train Epoch: 8 [704/1083 (65%)]\tLoss: 0.680924 \tOP: 0.553191\tOR: 0.366197\tOF1: 0.440678\n",
      "Train Epoch: 8 [736/1083 (68%)]\tLoss: 0.682157 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 8 [768/1083 (71%)]\tLoss: 0.681750 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 8 [800/1083 (74%)]\tLoss: 0.681558 \tOP: 0.553191\tOR: 0.433333\tOF1: 0.485981\n",
      "Train Epoch: 8 [832/1083 (76%)]\tLoss: 0.681967 \tOP: 0.533333\tOR: 0.387097\tOF1: 0.448598\n",
      "Train Epoch: 8 [864/1083 (79%)]\tLoss: 0.683163 \tOP: 0.511111\tOR: 0.365079\tOF1: 0.425926\n",
      "Train Epoch: 8 [896/1083 (82%)]\tLoss: 0.681149 \tOP: 0.551020\tOR: 0.409091\tOF1: 0.469565\n",
      "Train Epoch: 8 [928/1083 (85%)]\tLoss: 0.680232 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 8 [960/1083 (88%)]\tLoss: 0.682573 \tOP: 0.521739\tOR: 0.352941\tOF1: 0.421053\n",
      "Train Epoch: 8 [992/1083 (91%)]\tLoss: 0.682689 \tOP: 0.511111\tOR: 0.310811\tOF1: 0.386555\n",
      "Train Epoch: 8 [1024/1083 (94%)]\tLoss: 0.682024 \tOP: 0.553191\tOR: 0.346667\tOF1: 0.426230\n",
      "Train Epoch: 8 [891/1083 (97%)]\tLoss: 0.682650 \tOP: 0.487805\tOR: 0.322581\tOF1: 0.388350\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6930 \n",
      "OP: 0.200000\n",
      "OR: 0.171429\n",
      "OF1: 0.184615\n",
      "\n",
      "Train Epoch: 9 [0/1083 (0%)]\tLoss: 0.681441 \tOP: 0.543478\tOR: 0.357143\tOF1: 0.431034\n",
      "Train Epoch: 9 [32/1083 (3%)]\tLoss: 0.680738 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 9 [64/1083 (6%)]\tLoss: 0.682775 \tOP: 0.511111\tOR: 0.333333\tOF1: 0.403509\n",
      "Train Epoch: 9 [96/1083 (9%)]\tLoss: 0.679359 \tOP: 0.580000\tOR: 0.432836\tOF1: 0.495726\n",
      "Train Epoch: 9 [128/1083 (12%)]\tLoss: 0.680371 \tOP: 0.560000\tOR: 0.400000\tOF1: 0.466667\n",
      "Train Epoch: 9 [160/1083 (15%)]\tLoss: 0.682577 \tOP: 0.533333\tOR: 0.358209\tOF1: 0.428571\n",
      "Train Epoch: 9 [192/1083 (18%)]\tLoss: 0.681483 \tOP: 0.553191\tOR: 0.393939\tOF1: 0.460177\n",
      "Train Epoch: 9 [224/1083 (21%)]\tLoss: 0.681542 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 9 [256/1083 (24%)]\tLoss: 0.681139 \tOP: 0.553191\tOR: 0.356164\tOF1: 0.433333\n",
      "Train Epoch: 9 [288/1083 (26%)]\tLoss: 0.680013 \tOP: 0.576923\tOR: 0.394737\tOF1: 0.468750\n",
      "Train Epoch: 9 [320/1083 (29%)]\tLoss: 0.680331 \tOP: 0.568627\tOR: 0.432836\tOF1: 0.491525\n",
      "Train Epoch: 9 [352/1083 (32%)]\tLoss: 0.681551 \tOP: 0.553191\tOR: 0.419355\tOF1: 0.477064\n",
      "Train Epoch: 9 [384/1083 (35%)]\tLoss: 0.682723 \tOP: 0.522727\tOR: 0.353846\tOF1: 0.422018\n",
      "Train Epoch: 9 [416/1083 (38%)]\tLoss: 0.680354 \tOP: 0.560000\tOR: 0.430769\tOF1: 0.486957\n",
      "Train Epoch: 9 [448/1083 (41%)]\tLoss: 0.682777 \tOP: 0.543478\tOR: 0.423729\tOF1: 0.476190\n",
      "Train Epoch: 9 [480/1083 (44%)]\tLoss: 0.684178 \tOP: 0.488372\tOR: 0.291667\tOF1: 0.365217\n",
      "Train Epoch: 9 [512/1083 (47%)]\tLoss: 0.684411 \tOP: 0.488372\tOR: 0.333333\tOF1: 0.396226\n",
      "Train Epoch: 9 [544/1083 (50%)]\tLoss: 0.682459 \tOP: 0.522727\tOR: 0.359375\tOF1: 0.425926\n",
      "Train Epoch: 9 [576/1083 (53%)]\tLoss: 0.683508 \tOP: 0.500000\tOR: 0.309859\tOF1: 0.382609\n",
      "Train Epoch: 9 [608/1083 (56%)]\tLoss: 0.681458 \tOP: 0.551020\tOR: 0.380282\tOF1: 0.450000\n",
      "Train Epoch: 9 [640/1083 (59%)]\tLoss: 0.682741 \tOP: 0.531915\tOR: 0.352113\tOF1: 0.423729\n",
      "Train Epoch: 9 [672/1083 (62%)]\tLoss: 0.682212 \tOP: 0.541667\tOR: 0.376812\tOF1: 0.444444\n",
      "Train Epoch: 9 [704/1083 (65%)]\tLoss: 0.682793 \tOP: 0.521739\tOR: 0.369231\tOF1: 0.432432\n",
      "Train Epoch: 9 [736/1083 (68%)]\tLoss: 0.681215 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 9 [768/1083 (71%)]\tLoss: 0.678715 \tOP: 0.596154\tOR: 0.455882\tOF1: 0.516667\n",
      "Train Epoch: 9 [800/1083 (74%)]\tLoss: 0.684815 \tOP: 0.488372\tOR: 0.287671\tOF1: 0.362069\n",
      "Train Epoch: 9 [832/1083 (76%)]\tLoss: 0.680180 \tOP: 0.571429\tOR: 0.400000\tOF1: 0.470588\n",
      "Train Epoch: 9 [864/1083 (79%)]\tLoss: 0.682218 \tOP: 0.533333\tOR: 0.338028\tOF1: 0.413793\n",
      "Train Epoch: 9 [896/1083 (82%)]\tLoss: 0.684014 \tOP: 0.500000\tOR: 0.343284\tOF1: 0.407080\n",
      "Train Epoch: 9 [928/1083 (85%)]\tLoss: 0.679543 \tOP: 0.580000\tOR: 0.397260\tOF1: 0.471545\n",
      "Train Epoch: 9 [960/1083 (88%)]\tLoss: 0.684038 \tOP: 0.476190\tOR: 0.312500\tOF1: 0.377358\n",
      "Train Epoch: 9 [992/1083 (91%)]\tLoss: 0.680884 \tOP: 0.551020\tOR: 0.415385\tOF1: 0.473684\n",
      "Train Epoch: 9 [1024/1083 (94%)]\tLoss: 0.681927 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 9 [891/1083 (97%)]\tLoss: 0.682517 \tOP: 0.487805\tOR: 0.344828\tOF1: 0.404040\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6930 \n",
      "OP: 0.193548\n",
      "OR: 0.171429\n",
      "OF1: 0.181818\n",
      "\n",
      "Train Epoch: 10 [0/1083 (0%)]\tLoss: 0.678512 \tOP: 0.600000\tOR: 0.492537\tOF1: 0.540984\n",
      "Train Epoch: 10 [32/1083 (3%)]\tLoss: 0.680090 \tOP: 0.580000\tOR: 0.453125\tOF1: 0.508772\n",
      "Train Epoch: 10 [64/1083 (6%)]\tLoss: 0.682811 \tOP: 0.500000\tOR: 0.310811\tOF1: 0.383333\n",
      "Train Epoch: 10 [96/1083 (9%)]\tLoss: 0.682041 \tOP: 0.533333\tOR: 0.369231\tOF1: 0.436364\n",
      "Train Epoch: 10 [128/1083 (12%)]\tLoss: 0.680497 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 10 [160/1083 (15%)]\tLoss: 0.681964 \tOP: 0.543478\tOR: 0.396825\tOF1: 0.458716\n",
      "Train Epoch: 10 [192/1083 (18%)]\tLoss: 0.681093 \tOP: 0.553191\tOR: 0.433333\tOF1: 0.485981\n",
      "Train Epoch: 10 [224/1083 (21%)]\tLoss: 0.683496 \tOP: 0.511111\tOR: 0.328571\tOF1: 0.400000\n",
      "Train Epoch: 10 [256/1083 (24%)]\tLoss: 0.684306 \tOP: 0.487805\tOR: 0.317460\tOF1: 0.384615\n",
      "Train Epoch: 10 [288/1083 (26%)]\tLoss: 0.681549 \tOP: 0.543478\tOR: 0.378788\tOF1: 0.446429\n",
      "Train Epoch: 10 [320/1083 (29%)]\tLoss: 0.679595 \tOP: 0.580000\tOR: 0.386667\tOF1: 0.464000\n",
      "Train Epoch: 10 [352/1083 (32%)]\tLoss: 0.680583 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 10 [384/1083 (35%)]\tLoss: 0.682555 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 10 [416/1083 (38%)]\tLoss: 0.681062 \tOP: 0.553191\tOR: 0.342105\tOF1: 0.422764\n",
      "Train Epoch: 10 [448/1083 (41%)]\tLoss: 0.682915 \tOP: 0.511111\tOR: 0.310811\tOF1: 0.386555\n",
      "Train Epoch: 10 [480/1083 (44%)]\tLoss: 0.682884 \tOP: 0.522727\tOR: 0.319444\tOF1: 0.396552\n",
      "Train Epoch: 10 [512/1083 (47%)]\tLoss: 0.681473 \tOP: 0.541667\tOR: 0.412698\tOF1: 0.468468\n",
      "Train Epoch: 10 [544/1083 (50%)]\tLoss: 0.682445 \tOP: 0.522727\tOR: 0.353846\tOF1: 0.422018\n",
      "Train Epoch: 10 [576/1083 (53%)]\tLoss: 0.678958 \tOP: 0.576923\tOR: 0.454545\tOF1: 0.508475\n",
      "Train Epoch: 10 [608/1083 (56%)]\tLoss: 0.684481 \tOP: 0.500000\tOR: 0.349206\tOF1: 0.411215\n",
      "Train Epoch: 10 [640/1083 (59%)]\tLoss: 0.680462 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 10 [672/1083 (62%)]\tLoss: 0.679964 \tOP: 0.571429\tOR: 0.451613\tOF1: 0.504505\n",
      "Train Epoch: 10 [704/1083 (65%)]\tLoss: 0.680601 \tOP: 0.551020\tOR: 0.375000\tOF1: 0.446281\n",
      "Train Epoch: 10 [736/1083 (68%)]\tLoss: 0.680767 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 10 [768/1083 (71%)]\tLoss: 0.682801 \tOP: 0.533333\tOR: 0.333333\tOF1: 0.410256\n",
      "Train Epoch: 10 [800/1083 (74%)]\tLoss: 0.681886 \tOP: 0.533333\tOR: 0.333333\tOF1: 0.410256\n",
      "Train Epoch: 10 [832/1083 (76%)]\tLoss: 0.679286 \tOP: 0.580000\tOR: 0.420290\tOF1: 0.487395\n",
      "Train Epoch: 10 [864/1083 (79%)]\tLoss: 0.680453 \tOP: 0.560000\tOR: 0.444444\tOF1: 0.495575\n",
      "Train Epoch: 10 [896/1083 (82%)]\tLoss: 0.680306 \tOP: 0.562500\tOR: 0.360000\tOF1: 0.439024\n",
      "Train Epoch: 10 [928/1083 (85%)]\tLoss: 0.682864 \tOP: 0.521739\tOR: 0.380952\tOF1: 0.440367\n",
      "Train Epoch: 10 [960/1083 (88%)]\tLoss: 0.681729 \tOP: 0.543478\tOR: 0.367647\tOF1: 0.438596\n",
      "Train Epoch: 10 [992/1083 (91%)]\tLoss: 0.681341 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 10 [1024/1083 (94%)]\tLoss: 0.681031 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 10 [891/1083 (97%)]\tLoss: 0.683477 \tOP: 0.463415\tOR: 0.358491\tOF1: 0.404255\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6933 \n",
      "OP: 0.218750\n",
      "OR: 0.200000\n",
      "OF1: 0.208955\n",
      "\n",
      "Train Epoch: 11 [0/1083 (0%)]\tLoss: 0.680599 \tOP: 0.562500\tOR: 0.457627\tOF1: 0.504673\n",
      "Train Epoch: 11 [32/1083 (3%)]\tLoss: 0.682186 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 11 [64/1083 (6%)]\tLoss: 0.680058 \tOP: 0.571429\tOR: 0.444444\tOF1: 0.500000\n",
      "Train Epoch: 11 [96/1083 (9%)]\tLoss: 0.678274 \tOP: 0.603774\tOR: 0.484848\tOF1: 0.537815\n",
      "Train Epoch: 11 [128/1083 (12%)]\tLoss: 0.681127 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 11 [160/1083 (15%)]\tLoss: 0.682006 \tOP: 0.533333\tOR: 0.400000\tOF1: 0.457143\n",
      "Train Epoch: 11 [192/1083 (18%)]\tLoss: 0.682860 \tOP: 0.522727\tOR: 0.319444\tOF1: 0.396552\n",
      "Train Epoch: 11 [224/1083 (21%)]\tLoss: 0.680121 \tOP: 0.571429\tOR: 0.424242\tOF1: 0.486957\n",
      "Train Epoch: 11 [256/1083 (24%)]\tLoss: 0.681338 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 11 [288/1083 (26%)]\tLoss: 0.680615 \tOP: 0.562500\tOR: 0.380282\tOF1: 0.453782\n",
      "Train Epoch: 11 [320/1083 (29%)]\tLoss: 0.680055 \tOP: 0.571429\tOR: 0.363636\tOF1: 0.444444\n",
      "Train Epoch: 11 [352/1083 (32%)]\tLoss: 0.681925 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 11 [384/1083 (35%)]\tLoss: 0.683565 \tOP: 0.500000\tOR: 0.313433\tOF1: 0.385321\n",
      "Train Epoch: 11 [416/1083 (38%)]\tLoss: 0.681100 \tOP: 0.553191\tOR: 0.419355\tOF1: 0.477064\n",
      "Train Epoch: 11 [448/1083 (41%)]\tLoss: 0.679389 \tOP: 0.580000\tOR: 0.386667\tOF1: 0.464000\n",
      "Train Epoch: 11 [480/1083 (44%)]\tLoss: 0.682746 \tOP: 0.531915\tOR: 0.333333\tOF1: 0.409836\n",
      "Train Epoch: 11 [512/1083 (47%)]\tLoss: 0.679150 \tOP: 0.580000\tOR: 0.420290\tOF1: 0.487395\n",
      "Train Epoch: 11 [544/1083 (50%)]\tLoss: 0.682186 \tOP: 0.533333\tOR: 0.338028\tOF1: 0.413793\n",
      "Train Epoch: 11 [576/1083 (53%)]\tLoss: 0.680956 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 11 [608/1083 (56%)]\tLoss: 0.679003 \tOP: 0.568627\tOR: 0.402778\tOF1: 0.471545\n",
      "Train Epoch: 11 [640/1083 (59%)]\tLoss: 0.681160 \tOP: 0.541667\tOR: 0.400000\tOF1: 0.460177\n",
      "Train Epoch: 11 [672/1083 (62%)]\tLoss: 0.682627 \tOP: 0.520833\tOR: 0.352113\tOF1: 0.420168\n",
      "Train Epoch: 11 [704/1083 (65%)]\tLoss: 0.683429 \tOP: 0.511111\tOR: 0.377049\tOF1: 0.433962\n",
      "Train Epoch: 11 [736/1083 (68%)]\tLoss: 0.681376 \tOP: 0.553191\tOR: 0.393939\tOF1: 0.460177\n",
      "Train Epoch: 11 [768/1083 (71%)]\tLoss: 0.680086 \tOP: 0.560000\tOR: 0.400000\tOF1: 0.466667\n",
      "Train Epoch: 11 [800/1083 (74%)]\tLoss: 0.680217 \tOP: 0.571429\tOR: 0.341463\tOF1: 0.427481\n",
      "Train Epoch: 11 [832/1083 (76%)]\tLoss: 0.680177 \tOP: 0.571429\tOR: 0.451613\tOF1: 0.504505\n",
      "Train Epoch: 11 [864/1083 (79%)]\tLoss: 0.682880 \tOP: 0.522727\tOR: 0.315068\tOF1: 0.393162\n",
      "Train Epoch: 11 [896/1083 (82%)]\tLoss: 0.679883 \tOP: 0.568627\tOR: 0.414286\tOF1: 0.479339\n",
      "Train Epoch: 11 [928/1083 (85%)]\tLoss: 0.683746 \tOP: 0.511111\tOR: 0.348485\tOF1: 0.414414\n",
      "Train Epoch: 11 [960/1083 (88%)]\tLoss: 0.682902 \tOP: 0.522727\tOR: 0.403509\tOF1: 0.455446\n",
      "Train Epoch: 11 [992/1083 (91%)]\tLoss: 0.681749 \tOP: 0.531915\tOR: 0.378788\tOF1: 0.442478\n",
      "Train Epoch: 11 [1024/1083 (94%)]\tLoss: 0.682353 \tOP: 0.522727\tOR: 0.328571\tOF1: 0.403509\n",
      "Train Epoch: 11 [891/1083 (97%)]\tLoss: 0.683964 \tOP: 0.450000\tOR: 0.290323\tOF1: 0.352941\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6929 \n",
      "OP: 0.242424\n",
      "OR: 0.228571\n",
      "OF1: 0.235294\n",
      "\n",
      "Train Epoch: 12 [0/1083 (0%)]\tLoss: 0.681026 \tOP: 0.541667\tOR: 0.388060\tOF1: 0.452174\n",
      "Train Epoch: 12 [32/1083 (3%)]\tLoss: 0.683440 \tOP: 0.511628\tOR: 0.343750\tOF1: 0.411215\n",
      "Train Epoch: 12 [64/1083 (6%)]\tLoss: 0.679556 \tOP: 0.580000\tOR: 0.453125\tOF1: 0.508772\n",
      "Train Epoch: 12 [96/1083 (9%)]\tLoss: 0.682695 \tOP: 0.520833\tOR: 0.378788\tOF1: 0.438596\n",
      "Train Epoch: 12 [128/1083 (12%)]\tLoss: 0.680306 \tOP: 0.562500\tOR: 0.415385\tOF1: 0.477876\n",
      "Train Epoch: 12 [160/1083 (15%)]\tLoss: 0.680441 \tOP: 0.551020\tOR: 0.360000\tOF1: 0.435484\n",
      "Train Epoch: 12 [192/1083 (18%)]\tLoss: 0.681546 \tOP: 0.533333\tOR: 0.333333\tOF1: 0.410256\n",
      "Train Epoch: 12 [224/1083 (21%)]\tLoss: 0.679000 \tOP: 0.588235\tOR: 0.434783\tOF1: 0.500000\n",
      "Train Epoch: 12 [256/1083 (24%)]\tLoss: 0.681612 \tOP: 0.541667\tOR: 0.419355\tOF1: 0.472727\n",
      "Train Epoch: 12 [288/1083 (26%)]\tLoss: 0.679934 \tOP: 0.571429\tOR: 0.437500\tOF1: 0.495575\n",
      "Train Epoch: 12 [320/1083 (29%)]\tLoss: 0.680164 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 12 [352/1083 (32%)]\tLoss: 0.678783 \tOP: 0.596154\tOR: 0.442857\tOF1: 0.508197\n",
      "Train Epoch: 12 [384/1083 (35%)]\tLoss: 0.680594 \tOP: 0.562500\tOR: 0.364865\tOF1: 0.442623\n",
      "Train Epoch: 12 [416/1083 (38%)]\tLoss: 0.681580 \tOP: 0.543478\tOR: 0.328947\tOF1: 0.409836\n",
      "Train Epoch: 12 [448/1083 (41%)]\tLoss: 0.680053 \tOP: 0.571429\tOR: 0.378378\tOF1: 0.455285\n",
      "Train Epoch: 12 [480/1083 (44%)]\tLoss: 0.681585 \tOP: 0.541667\tOR: 0.400000\tOF1: 0.460177\n",
      "Train Epoch: 12 [512/1083 (47%)]\tLoss: 0.679860 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 12 [544/1083 (50%)]\tLoss: 0.682760 \tOP: 0.533333\tOR: 0.380952\tOF1: 0.444444\n",
      "Train Epoch: 12 [576/1083 (53%)]\tLoss: 0.680303 \tOP: 0.551020\tOR: 0.380282\tOF1: 0.450000\n",
      "Train Epoch: 12 [608/1083 (56%)]\tLoss: 0.681893 \tOP: 0.530612\tOR: 0.426230\tOF1: 0.472727\n",
      "Train Epoch: 12 [640/1083 (59%)]\tLoss: 0.683479 \tOP: 0.500000\tOR: 0.343750\tOF1: 0.407407\n",
      "Train Epoch: 12 [672/1083 (62%)]\tLoss: 0.680397 \tOP: 0.551020\tOR: 0.391304\tOF1: 0.457627\n",
      "Train Epoch: 12 [704/1083 (65%)]\tLoss: 0.680229 \tOP: 0.562500\tOR: 0.442623\tOF1: 0.495413\n",
      "Train Epoch: 12 [736/1083 (68%)]\tLoss: 0.679636 \tOP: 0.580000\tOR: 0.432836\tOF1: 0.495726\n",
      "Train Epoch: 12 [768/1083 (71%)]\tLoss: 0.682402 \tOP: 0.522727\tOR: 0.315068\tOF1: 0.393162\n",
      "Train Epoch: 12 [800/1083 (74%)]\tLoss: 0.681765 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 12 [832/1083 (76%)]\tLoss: 0.683286 \tOP: 0.521739\tOR: 0.328767\tOF1: 0.403361\n",
      "Train Epoch: 12 [864/1083 (79%)]\tLoss: 0.681474 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 12 [896/1083 (82%)]\tLoss: 0.679376 \tOP: 0.580000\tOR: 0.439394\tOF1: 0.500000\n",
      "Train Epoch: 12 [928/1083 (85%)]\tLoss: 0.682844 \tOP: 0.511111\tOR: 0.302632\tOF1: 0.380165\n",
      "Train Epoch: 12 [960/1083 (88%)]\tLoss: 0.684545 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 12 [992/1083 (91%)]\tLoss: 0.682239 \tOP: 0.533333\tOR: 0.333333\tOF1: 0.410256\n",
      "Train Epoch: 12 [1024/1083 (94%)]\tLoss: 0.679265 \tOP: 0.576923\tOR: 0.441176\tOF1: 0.500000\n",
      "Train Epoch: 12 [891/1083 (97%)]\tLoss: 0.682059 \tOP: 0.488372\tOR: 0.396226\tOF1: 0.437500\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6922 \n",
      "OP: 0.185185\n",
      "OR: 0.142857\n",
      "OF1: 0.161290\n",
      "\n",
      "Train Epoch: 13 [0/1083 (0%)]\tLoss: 0.678879 \tOP: 0.596154\tOR: 0.469697\tOF1: 0.525424\n",
      "Train Epoch: 13 [32/1083 (3%)]\tLoss: 0.683531 \tOP: 0.511628\tOR: 0.305556\tOF1: 0.382609\n",
      "Train Epoch: 13 [64/1083 (6%)]\tLoss: 0.681834 \tOP: 0.533333\tOR: 0.369231\tOF1: 0.436364\n",
      "Train Epoch: 13 [96/1083 (9%)]\tLoss: 0.680582 \tOP: 0.571429\tOR: 0.430769\tOF1: 0.491228\n",
      "Train Epoch: 13 [128/1083 (12%)]\tLoss: 0.679111 \tOP: 0.588235\tOR: 0.454545\tOF1: 0.512821\n",
      "Train Epoch: 13 [160/1083 (15%)]\tLoss: 0.678332 \tOP: 0.596154\tOR: 0.484375\tOF1: 0.534483\n",
      "Train Epoch: 13 [192/1083 (18%)]\tLoss: 0.684658 \tOP: 0.478261\tOR: 0.349206\tOF1: 0.403670\n",
      "Train Epoch: 13 [224/1083 (21%)]\tLoss: 0.680428 \tOP: 0.551020\tOR: 0.385714\tOF1: 0.453782\n",
      "Train Epoch: 13 [256/1083 (24%)]\tLoss: 0.679097 \tOP: 0.588235\tOR: 0.428571\tOF1: 0.495868\n",
      "Train Epoch: 13 [288/1083 (26%)]\tLoss: 0.680898 \tOP: 0.541667\tOR: 0.400000\tOF1: 0.460177\n",
      "Train Epoch: 13 [320/1083 (29%)]\tLoss: 0.680480 \tOP: 0.562500\tOR: 0.380282\tOF1: 0.453782\n",
      "Train Epoch: 13 [352/1083 (32%)]\tLoss: 0.681762 \tOP: 0.543478\tOR: 0.396825\tOF1: 0.458716\n",
      "Train Epoch: 13 [384/1083 (35%)]\tLoss: 0.679532 \tOP: 0.580000\tOR: 0.439394\tOF1: 0.500000\n",
      "Train Epoch: 13 [416/1083 (38%)]\tLoss: 0.680606 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 13 [448/1083 (41%)]\tLoss: 0.680214 \tOP: 0.562500\tOR: 0.450000\tOF1: 0.500000\n",
      "Train Epoch: 13 [480/1083 (44%)]\tLoss: 0.680739 \tOP: 0.562500\tOR: 0.409091\tOF1: 0.473684\n",
      "Train Epoch: 13 [512/1083 (47%)]\tLoss: 0.681220 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 13 [544/1083 (50%)]\tLoss: 0.684735 \tOP: 0.487805\tOR: 0.289855\tOF1: 0.363636\n",
      "Train Epoch: 13 [576/1083 (53%)]\tLoss: 0.680977 \tOP: 0.562500\tOR: 0.402985\tOF1: 0.469565\n",
      "Train Epoch: 13 [608/1083 (56%)]\tLoss: 0.681132 \tOP: 0.551020\tOR: 0.360000\tOF1: 0.435484\n",
      "Train Epoch: 13 [640/1083 (59%)]\tLoss: 0.679390 \tOP: 0.580000\tOR: 0.414286\tOF1: 0.483333\n",
      "Train Epoch: 13 [672/1083 (62%)]\tLoss: 0.681116 \tOP: 0.543478\tOR: 0.324675\tOF1: 0.406504\n",
      "Train Epoch: 13 [704/1083 (65%)]\tLoss: 0.680747 \tOP: 0.553191\tOR: 0.351351\tOF1: 0.429752\n",
      "Train Epoch: 13 [736/1083 (68%)]\tLoss: 0.680879 \tOP: 0.541667\tOR: 0.464286\tOF1: 0.500000\n",
      "Train Epoch: 13 [768/1083 (71%)]\tLoss: 0.680371 \tOP: 0.562500\tOR: 0.409091\tOF1: 0.473684\n",
      "Train Epoch: 13 [800/1083 (74%)]\tLoss: 0.680912 \tOP: 0.553191\tOR: 0.333333\tOF1: 0.416000\n",
      "Train Epoch: 13 [832/1083 (76%)]\tLoss: 0.683199 \tOP: 0.500000\tOR: 0.293333\tOF1: 0.369748\n",
      "Train Epoch: 13 [864/1083 (79%)]\tLoss: 0.681479 \tOP: 0.541667\tOR: 0.433333\tOF1: 0.481481\n",
      "Train Epoch: 13 [896/1083 (82%)]\tLoss: 0.680281 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 13 [928/1083 (85%)]\tLoss: 0.679440 \tOP: 0.580000\tOR: 0.420290\tOF1: 0.487395\n",
      "Train Epoch: 13 [960/1083 (88%)]\tLoss: 0.679623 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 13 [992/1083 (91%)]\tLoss: 0.680262 \tOP: 0.571429\tOR: 0.411765\tOF1: 0.478632\n",
      "Train Epoch: 13 [1024/1083 (94%)]\tLoss: 0.679248 \tOP: 0.588235\tOR: 0.416667\tOF1: 0.487805\n",
      "Train Epoch: 13 [891/1083 (97%)]\tLoss: 0.681044 \tOP: 0.511628\tOR: 0.366667\tOF1: 0.427184\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6920 \n",
      "OP: 0.250000\n",
      "OR: 0.228571\n",
      "OF1: 0.238806\n",
      "\n",
      "Train Epoch: 14 [0/1083 (0%)]\tLoss: 0.677878 \tOP: 0.603774\tOR: 0.438356\tOF1: 0.507937\n",
      "Train Epoch: 14 [32/1083 (3%)]\tLoss: 0.679980 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 14 [64/1083 (6%)]\tLoss: 0.680285 \tOP: 0.562500\tOR: 0.442623\tOF1: 0.495413\n",
      "Train Epoch: 14 [96/1083 (9%)]\tLoss: 0.681412 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 14 [128/1083 (12%)]\tLoss: 0.683551 \tOP: 0.500000\tOR: 0.272727\tOF1: 0.352941\n",
      "Train Epoch: 14 [160/1083 (15%)]\tLoss: 0.679725 \tOP: 0.571429\tOR: 0.417910\tOF1: 0.482759\n",
      "Train Epoch: 14 [192/1083 (18%)]\tLoss: 0.680213 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 14 [224/1083 (21%)]\tLoss: 0.682218 \tOP: 0.521739\tOR: 0.328767\tOF1: 0.403361\n",
      "Train Epoch: 14 [256/1083 (24%)]\tLoss: 0.679883 \tOP: 0.560000\tOR: 0.417910\tOF1: 0.478632\n",
      "Train Epoch: 14 [288/1083 (26%)]\tLoss: 0.680740 \tOP: 0.553191\tOR: 0.472727\tOF1: 0.509804\n",
      "Train Epoch: 14 [320/1083 (29%)]\tLoss: 0.681352 \tOP: 0.543478\tOR: 0.409836\tOF1: 0.467290\n",
      "Train Epoch: 14 [352/1083 (32%)]\tLoss: 0.682390 \tOP: 0.533333\tOR: 0.380952\tOF1: 0.444444\n",
      "Train Epoch: 14 [384/1083 (35%)]\tLoss: 0.681927 \tOP: 0.533333\tOR: 0.393443\tOF1: 0.452830\n",
      "Train Epoch: 14 [416/1083 (38%)]\tLoss: 0.679564 \tOP: 0.571429\tOR: 0.405797\tOF1: 0.474576\n",
      "Train Epoch: 14 [448/1083 (41%)]\tLoss: 0.683414 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 14 [480/1083 (44%)]\tLoss: 0.680021 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 14 [512/1083 (47%)]\tLoss: 0.680842 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 14 [544/1083 (50%)]\tLoss: 0.678360 \tOP: 0.596154\tOR: 0.397436\tOF1: 0.476923\n",
      "Train Epoch: 14 [576/1083 (53%)]\tLoss: 0.680039 \tOP: 0.571429\tOR: 0.400000\tOF1: 0.470588\n",
      "Train Epoch: 14 [608/1083 (56%)]\tLoss: 0.681517 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 14 [640/1083 (59%)]\tLoss: 0.682764 \tOP: 0.511628\tOR: 0.309859\tOF1: 0.385965\n",
      "Train Epoch: 14 [672/1083 (62%)]\tLoss: 0.679182 \tOP: 0.588235\tOR: 0.461538\tOF1: 0.517241\n",
      "Train Epoch: 14 [704/1083 (65%)]\tLoss: 0.682071 \tOP: 0.533333\tOR: 0.358209\tOF1: 0.428571\n",
      "Train Epoch: 14 [736/1083 (68%)]\tLoss: 0.679965 \tOP: 0.562500\tOR: 0.435484\tOF1: 0.490909\n",
      "Train Epoch: 14 [768/1083 (71%)]\tLoss: 0.683130 \tOP: 0.500000\tOR: 0.328358\tOF1: 0.396396\n",
      "Train Epoch: 14 [800/1083 (74%)]\tLoss: 0.678656 \tOP: 0.588235\tOR: 0.416667\tOF1: 0.487805\n",
      "Train Epoch: 14 [832/1083 (76%)]\tLoss: 0.679680 \tOP: 0.571429\tOR: 0.411765\tOF1: 0.478632\n",
      "Train Epoch: 14 [864/1083 (79%)]\tLoss: 0.680750 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 14 [896/1083 (82%)]\tLoss: 0.680788 \tOP: 0.553191\tOR: 0.356164\tOF1: 0.433333\n",
      "Train Epoch: 14 [928/1083 (85%)]\tLoss: 0.679496 \tOP: 0.580000\tOR: 0.397260\tOF1: 0.471545\n",
      "Train Epoch: 14 [960/1083 (88%)]\tLoss: 0.679860 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 14 [992/1083 (91%)]\tLoss: 0.679286 \tOP: 0.580000\tOR: 0.467742\tOF1: 0.517857\n",
      "Train Epoch: 14 [1024/1083 (94%)]\tLoss: 0.678517 \tOP: 0.603774\tOR: 0.484848\tOF1: 0.537815\n",
      "Train Epoch: 14 [891/1083 (97%)]\tLoss: 0.679689 \tOP: 0.533333\tOR: 0.421053\tOF1: 0.470588\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6927 \n",
      "OP: 0.257143\n",
      "OR: 0.257143\n",
      "OF1: 0.257143\n",
      "\n",
      "Train Epoch: 15 [0/1083 (0%)]\tLoss: 0.679409 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 15 [32/1083 (3%)]\tLoss: 0.682894 \tOP: 0.500000\tOR: 0.343750\tOF1: 0.407407\n",
      "Train Epoch: 15 [64/1083 (6%)]\tLoss: 0.680200 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 15 [96/1083 (9%)]\tLoss: 0.681767 \tOP: 0.543478\tOR: 0.409836\tOF1: 0.467290\n",
      "Train Epoch: 15 [128/1083 (12%)]\tLoss: 0.680336 \tOP: 0.562500\tOR: 0.397059\tOF1: 0.465517\n",
      "Train Epoch: 15 [160/1083 (15%)]\tLoss: 0.679462 \tOP: 0.580000\tOR: 0.426471\tOF1: 0.491525\n",
      "Train Epoch: 15 [192/1083 (18%)]\tLoss: 0.683177 \tOP: 0.500000\tOR: 0.289474\tOF1: 0.366667\n",
      "Train Epoch: 15 [224/1083 (21%)]\tLoss: 0.680726 \tOP: 0.553191\tOR: 0.426230\tOF1: 0.481481\n",
      "Train Epoch: 15 [256/1083 (24%)]\tLoss: 0.679877 \tOP: 0.571429\tOR: 0.417910\tOF1: 0.482759\n",
      "Train Epoch: 15 [288/1083 (26%)]\tLoss: 0.682547 \tOP: 0.510638\tOR: 0.358209\tOF1: 0.421053\n",
      "Train Epoch: 15 [320/1083 (29%)]\tLoss: 0.677743 \tOP: 0.603774\tOR: 0.492308\tOF1: 0.542373\n",
      "Train Epoch: 15 [352/1083 (32%)]\tLoss: 0.678767 \tOP: 0.588235\tOR: 0.526316\tOF1: 0.555556\n",
      "Train Epoch: 15 [384/1083 (35%)]\tLoss: 0.680781 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 15 [416/1083 (38%)]\tLoss: 0.682145 \tOP: 0.531915\tOR: 0.320513\tOF1: 0.400000\n",
      "Train Epoch: 15 [448/1083 (41%)]\tLoss: 0.681785 \tOP: 0.533333\tOR: 0.324324\tOF1: 0.403361\n",
      "Train Epoch: 15 [480/1083 (44%)]\tLoss: 0.679920 \tOP: 0.571429\tOR: 0.459016\tOF1: 0.509091\n",
      "Train Epoch: 15 [512/1083 (47%)]\tLoss: 0.681454 \tOP: 0.543478\tOR: 0.416667\tOF1: 0.471698\n",
      "Train Epoch: 15 [544/1083 (50%)]\tLoss: 0.679143 \tOP: 0.580000\tOR: 0.420290\tOF1: 0.487395\n",
      "Train Epoch: 15 [576/1083 (53%)]\tLoss: 0.678792 \tOP: 0.588235\tOR: 0.447761\tOF1: 0.508475\n",
      "Train Epoch: 15 [608/1083 (56%)]\tLoss: 0.680794 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 15 [640/1083 (59%)]\tLoss: 0.680697 \tOP: 0.553191\tOR: 0.356164\tOF1: 0.433333\n",
      "Train Epoch: 15 [672/1083 (62%)]\tLoss: 0.680806 \tOP: 0.553191\tOR: 0.371429\tOF1: 0.444444\n",
      "Train Epoch: 15 [704/1083 (65%)]\tLoss: 0.678592 \tOP: 0.576923\tOR: 0.468750\tOF1: 0.517241\n",
      "Train Epoch: 15 [736/1083 (68%)]\tLoss: 0.680086 \tOP: 0.562500\tOR: 0.402985\tOF1: 0.469565\n",
      "Train Epoch: 15 [768/1083 (71%)]\tLoss: 0.683180 \tOP: 0.522727\tOR: 0.291139\tOF1: 0.373984\n",
      "Train Epoch: 15 [800/1083 (74%)]\tLoss: 0.680774 \tOP: 0.553191\tOR: 0.366197\tOF1: 0.440678\n",
      "Train Epoch: 15 [832/1083 (76%)]\tLoss: 0.678509 \tOP: 0.596154\tOR: 0.442857\tOF1: 0.508197\n",
      "Train Epoch: 15 [864/1083 (79%)]\tLoss: 0.679725 \tOP: 0.571429\tOR: 0.411765\tOF1: 0.478632\n",
      "Train Epoch: 15 [896/1083 (82%)]\tLoss: 0.678307 \tOP: 0.596154\tOR: 0.430556\tOF1: 0.500000\n",
      "Train Epoch: 15 [928/1083 (85%)]\tLoss: 0.680354 \tOP: 0.560000\tOR: 0.405797\tOF1: 0.470588\n",
      "Train Epoch: 15 [960/1083 (88%)]\tLoss: 0.683419 \tOP: 0.500000\tOR: 0.318841\tOF1: 0.389381\n",
      "Train Epoch: 15 [992/1083 (91%)]\tLoss: 0.682176 \tOP: 0.522727\tOR: 0.365079\tOF1: 0.429907\n",
      "Train Epoch: 15 [1024/1083 (94%)]\tLoss: 0.678248 \tOP: 0.596154\tOR: 0.418919\tOF1: 0.492063\n",
      "Train Epoch: 15 [891/1083 (97%)]\tLoss: 0.679750 \tOP: 0.533333\tOR: 0.428571\tOF1: 0.475248\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6922 \n",
      "OP: 0.272727\n",
      "OR: 0.257143\n",
      "OF1: 0.264706\n",
      "\n",
      "Train Epoch: 16 [0/1083 (0%)]\tLoss: 0.678840 \tOP: 0.588235\tOR: 0.405405\tOF1: 0.480000\n",
      "Train Epoch: 16 [32/1083 (3%)]\tLoss: 0.681645 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 16 [64/1083 (6%)]\tLoss: 0.682138 \tOP: 0.529412\tOR: 0.385714\tOF1: 0.446281\n",
      "Train Epoch: 16 [96/1083 (9%)]\tLoss: 0.680633 \tOP: 0.553191\tOR: 0.371429\tOF1: 0.444444\n",
      "Train Epoch: 16 [128/1083 (12%)]\tLoss: 0.680965 \tOP: 0.541667\tOR: 0.412698\tOF1: 0.468468\n",
      "Train Epoch: 16 [160/1083 (15%)]\tLoss: 0.678803 \tOP: 0.588235\tOR: 0.441176\tOF1: 0.504202\n",
      "Train Epoch: 16 [192/1083 (18%)]\tLoss: 0.682273 \tOP: 0.522727\tOR: 0.323944\tOF1: 0.400000\n",
      "Train Epoch: 16 [224/1083 (21%)]\tLoss: 0.679164 \tOP: 0.580000\tOR: 0.475410\tOF1: 0.522523\n",
      "Train Epoch: 16 [256/1083 (24%)]\tLoss: 0.679751 \tOP: 0.571429\tOR: 0.400000\tOF1: 0.470588\n",
      "Train Epoch: 16 [288/1083 (26%)]\tLoss: 0.680974 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 16 [320/1083 (29%)]\tLoss: 0.681701 \tOP: 0.533333\tOR: 0.413793\tOF1: 0.466019\n",
      "Train Epoch: 16 [352/1083 (32%)]\tLoss: 0.682111 \tOP: 0.533333\tOR: 0.338028\tOF1: 0.413793\n",
      "Train Epoch: 16 [384/1083 (35%)]\tLoss: 0.679555 \tOP: 0.571429\tOR: 0.466667\tOF1: 0.513761\n",
      "Train Epoch: 16 [416/1083 (38%)]\tLoss: 0.679109 \tOP: 0.580000\tOR: 0.414286\tOF1: 0.483333\n",
      "Train Epoch: 16 [448/1083 (41%)]\tLoss: 0.679653 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 16 [480/1083 (44%)]\tLoss: 0.681783 \tOP: 0.521739\tOR: 0.338028\tOF1: 0.410256\n",
      "Train Epoch: 16 [512/1083 (47%)]\tLoss: 0.685790 \tOP: 0.469388\tOR: 0.365079\tOF1: 0.410714\n",
      "Train Epoch: 16 [544/1083 (50%)]\tLoss: 0.679061 \tOP: 0.584906\tOR: 0.436620\tOF1: 0.500000\n",
      "Train Epoch: 16 [576/1083 (53%)]\tLoss: 0.678428 \tOP: 0.596154\tOR: 0.436620\tOF1: 0.504065\n",
      "Train Epoch: 16 [608/1083 (56%)]\tLoss: 0.681800 \tOP: 0.533333\tOR: 0.324324\tOF1: 0.403361\n",
      "Train Epoch: 16 [640/1083 (59%)]\tLoss: 0.678649 \tOP: 0.588235\tOR: 0.468750\tOF1: 0.521739\n",
      "Train Epoch: 16 [672/1083 (62%)]\tLoss: 0.682316 \tOP: 0.521739\tOR: 0.347826\tOF1: 0.417391\n",
      "Train Epoch: 16 [704/1083 (65%)]\tLoss: 0.681357 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 16 [736/1083 (68%)]\tLoss: 0.678817 \tOP: 0.596154\tOR: 0.462687\tOF1: 0.521008\n",
      "Train Epoch: 16 [768/1083 (71%)]\tLoss: 0.679326 \tOP: 0.580000\tOR: 0.397260\tOF1: 0.471545\n",
      "Train Epoch: 16 [800/1083 (74%)]\tLoss: 0.681771 \tOP: 0.533333\tOR: 0.363636\tOF1: 0.432432\n",
      "Train Epoch: 16 [832/1083 (76%)]\tLoss: 0.679856 \tOP: 0.571429\tOR: 0.373333\tOF1: 0.451613\n",
      "Train Epoch: 16 [864/1083 (79%)]\tLoss: 0.681586 \tOP: 0.543478\tOR: 0.337838\tOF1: 0.416667\n",
      "Train Epoch: 16 [896/1083 (82%)]\tLoss: 0.679918 \tOP: 0.571429\tOR: 0.424242\tOF1: 0.486957\n",
      "Train Epoch: 16 [928/1083 (85%)]\tLoss: 0.680041 \tOP: 0.562500\tOR: 0.360000\tOF1: 0.439024\n",
      "Train Epoch: 16 [960/1083 (88%)]\tLoss: 0.679945 \tOP: 0.562500\tOR: 0.421875\tOF1: 0.482143\n",
      "Train Epoch: 16 [992/1083 (91%)]\tLoss: 0.685522 \tOP: 0.461538\tOR: 0.305085\tOF1: 0.367347\n",
      "Train Epoch: 16 [1024/1083 (94%)]\tLoss: 0.680665 \tOP: 0.553191\tOR: 0.371429\tOF1: 0.444444\n",
      "Train Epoch: 16 [891/1083 (97%)]\tLoss: 0.679210 \tOP: 0.533333\tOR: 0.413793\tOF1: 0.466019\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6928 \n",
      "OP: 0.187500\n",
      "OR: 0.171429\n",
      "OF1: 0.179104\n",
      "\n",
      "Train Epoch: 17 [0/1083 (0%)]\tLoss: 0.683222 \tOP: 0.488372\tOR: 0.313433\tOF1: 0.381818\n",
      "Train Epoch: 17 [32/1083 (3%)]\tLoss: 0.682826 \tOP: 0.511111\tOR: 0.323944\tOF1: 0.396552\n",
      "Train Epoch: 17 [64/1083 (6%)]\tLoss: 0.682764 \tOP: 0.511111\tOR: 0.338235\tOF1: 0.407080\n",
      "Train Epoch: 17 [96/1083 (9%)]\tLoss: 0.678798 \tOP: 0.588235\tOR: 0.400000\tOF1: 0.476190\n",
      "Train Epoch: 17 [128/1083 (12%)]\tLoss: 0.678764 \tOP: 0.588235\tOR: 0.468750\tOF1: 0.521739\n",
      "Train Epoch: 17 [160/1083 (15%)]\tLoss: 0.681886 \tOP: 0.531915\tOR: 0.384615\tOF1: 0.446429\n",
      "Train Epoch: 17 [192/1083 (18%)]\tLoss: 0.679681 \tOP: 0.571429\tOR: 0.400000\tOF1: 0.470588\n",
      "Train Epoch: 17 [224/1083 (21%)]\tLoss: 0.682026 \tOP: 0.533333\tOR: 0.393443\tOF1: 0.452830\n",
      "Train Epoch: 17 [256/1083 (24%)]\tLoss: 0.684357 \tOP: 0.500000\tOR: 0.343284\tOF1: 0.407080\n",
      "Train Epoch: 17 [288/1083 (26%)]\tLoss: 0.679837 \tOP: 0.571429\tOR: 0.417910\tOF1: 0.482759\n",
      "Train Epoch: 17 [320/1083 (29%)]\tLoss: 0.679651 \tOP: 0.571429\tOR: 0.444444\tOF1: 0.500000\n",
      "Train Epoch: 17 [352/1083 (32%)]\tLoss: 0.681836 \tOP: 0.533333\tOR: 0.338028\tOF1: 0.413793\n",
      "Train Epoch: 17 [384/1083 (35%)]\tLoss: 0.682920 \tOP: 0.521739\tOR: 0.369231\tOF1: 0.432432\n",
      "Train Epoch: 17 [416/1083 (38%)]\tLoss: 0.683998 \tOP: 0.488372\tOR: 0.344262\tOF1: 0.403846\n",
      "Train Epoch: 17 [448/1083 (41%)]\tLoss: 0.679685 \tOP: 0.562500\tOR: 0.421875\tOF1: 0.482143\n",
      "Train Epoch: 17 [480/1083 (44%)]\tLoss: 0.679592 \tOP: 0.571429\tOR: 0.430769\tOF1: 0.491228\n",
      "Train Epoch: 17 [512/1083 (47%)]\tLoss: 0.678610 \tOP: 0.588235\tOR: 0.454545\tOF1: 0.512821\n",
      "Train Epoch: 17 [544/1083 (50%)]\tLoss: 0.683429 \tOP: 0.500000\tOR: 0.289474\tOF1: 0.366667\n",
      "Train Epoch: 17 [576/1083 (53%)]\tLoss: 0.680126 \tOP: 0.571429\tOR: 0.405797\tOF1: 0.474576\n",
      "Train Epoch: 17 [608/1083 (56%)]\tLoss: 0.682142 \tOP: 0.533333\tOR: 0.307692\tOF1: 0.390244\n",
      "Train Epoch: 17 [640/1083 (59%)]\tLoss: 0.678937 \tOP: 0.588235\tOR: 0.441176\tOF1: 0.504202\n",
      "Train Epoch: 17 [672/1083 (62%)]\tLoss: 0.679756 \tOP: 0.571429\tOR: 0.509091\tOF1: 0.538462\n",
      "Train Epoch: 17 [704/1083 (65%)]\tLoss: 0.681126 \tOP: 0.543478\tOR: 0.357143\tOF1: 0.431034\n",
      "Train Epoch: 17 [736/1083 (68%)]\tLoss: 0.679805 \tOP: 0.568627\tOR: 0.439394\tOF1: 0.495726\n",
      "Train Epoch: 17 [768/1083 (71%)]\tLoss: 0.680937 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 17 [800/1083 (74%)]\tLoss: 0.678831 \tOP: 0.588235\tOR: 0.428571\tOF1: 0.495868\n",
      "Train Epoch: 17 [832/1083 (76%)]\tLoss: 0.681624 \tOP: 0.541667\tOR: 0.366197\tOF1: 0.436975\n",
      "Train Epoch: 17 [864/1083 (79%)]\tLoss: 0.680612 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 17 [896/1083 (82%)]\tLoss: 0.680235 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 17 [928/1083 (85%)]\tLoss: 0.678698 \tOP: 0.588235\tOR: 0.410959\tOF1: 0.483871\n",
      "Train Epoch: 17 [960/1083 (88%)]\tLoss: 0.678167 \tOP: 0.603774\tOR: 0.421053\tOF1: 0.496124\n",
      "Train Epoch: 17 [992/1083 (91%)]\tLoss: 0.683917 \tOP: 0.488889\tOR: 0.318841\tOF1: 0.385965\n",
      "Train Epoch: 17 [1024/1083 (94%)]\tLoss: 0.681034 \tOP: 0.551020\tOR: 0.380282\tOF1: 0.450000\n",
      "Train Epoch: 17 [891/1083 (97%)]\tLoss: 0.678220 \tOP: 0.553191\tOR: 0.440678\tOF1: 0.490566\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6938 \n",
      "OP: 0.212121\n",
      "OR: 0.200000\n",
      "OF1: 0.205882\n",
      "\n",
      "Train Epoch: 18 [0/1083 (0%)]\tLoss: 0.678332 \tOP: 0.596154\tOR: 0.484375\tOF1: 0.534483\n",
      "Train Epoch: 18 [32/1083 (3%)]\tLoss: 0.679896 \tOP: 0.571429\tOR: 0.424242\tOF1: 0.486957\n",
      "Train Epoch: 18 [64/1083 (6%)]\tLoss: 0.681205 \tOP: 0.543478\tOR: 0.423729\tOF1: 0.476190\n",
      "Train Epoch: 18 [96/1083 (9%)]\tLoss: 0.681875 \tOP: 0.531915\tOR: 0.367647\tOF1: 0.434783\n",
      "Train Epoch: 18 [128/1083 (12%)]\tLoss: 0.682132 \tOP: 0.530612\tOR: 0.388060\tOF1: 0.448276\n",
      "Train Epoch: 18 [160/1083 (15%)]\tLoss: 0.682539 \tOP: 0.511111\tOR: 0.353846\tOF1: 0.418182\n",
      "Train Epoch: 18 [192/1083 (18%)]\tLoss: 0.680171 \tOP: 0.562500\tOR: 0.364865\tOF1: 0.442623\n",
      "Train Epoch: 18 [224/1083 (21%)]\tLoss: 0.679179 \tOP: 0.580000\tOR: 0.397260\tOF1: 0.471545\n",
      "Train Epoch: 18 [256/1083 (24%)]\tLoss: 0.680137 \tOP: 0.562500\tOR: 0.364865\tOF1: 0.442623\n",
      "Train Epoch: 18 [288/1083 (26%)]\tLoss: 0.679656 \tOP: 0.571429\tOR: 0.430769\tOF1: 0.491228\n",
      "Train Epoch: 18 [320/1083 (29%)]\tLoss: 0.681319 \tOP: 0.541667\tOR: 0.376812\tOF1: 0.444444\n",
      "Train Epoch: 18 [352/1083 (32%)]\tLoss: 0.678464 \tOP: 0.596154\tOR: 0.449275\tOF1: 0.512397\n",
      "Train Epoch: 18 [384/1083 (35%)]\tLoss: 0.683379 \tOP: 0.500000\tOR: 0.359375\tOF1: 0.418182\n",
      "Train Epoch: 18 [416/1083 (38%)]\tLoss: 0.680825 \tOP: 0.553191\tOR: 0.412698\tOF1: 0.472727\n",
      "Train Epoch: 18 [448/1083 (41%)]\tLoss: 0.680893 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 18 [480/1083 (44%)]\tLoss: 0.680931 \tOP: 0.551020\tOR: 0.355263\tOF1: 0.432000\n",
      "Train Epoch: 18 [512/1083 (47%)]\tLoss: 0.678519 \tOP: 0.596154\tOR: 0.500000\tOF1: 0.543860\n",
      "Train Epoch: 18 [544/1083 (50%)]\tLoss: 0.681741 \tOP: 0.543478\tOR: 0.316456\tOF1: 0.400000\n",
      "Train Epoch: 18 [576/1083 (53%)]\tLoss: 0.679723 \tOP: 0.571429\tOR: 0.405797\tOF1: 0.474576\n",
      "Train Epoch: 18 [608/1083 (56%)]\tLoss: 0.678437 \tOP: 0.596154\tOR: 0.442857\tOF1: 0.508197\n",
      "Train Epoch: 18 [640/1083 (59%)]\tLoss: 0.680313 \tOP: 0.562500\tOR: 0.397059\tOF1: 0.465517\n",
      "Train Epoch: 18 [672/1083 (62%)]\tLoss: 0.680845 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 18 [704/1083 (65%)]\tLoss: 0.679148 \tOP: 0.580000\tOR: 0.420290\tOF1: 0.487395\n",
      "Train Epoch: 18 [736/1083 (68%)]\tLoss: 0.682863 \tOP: 0.511628\tOR: 0.323529\tOF1: 0.396396\n",
      "Train Epoch: 18 [768/1083 (71%)]\tLoss: 0.682282 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 18 [800/1083 (74%)]\tLoss: 0.680062 \tOP: 0.568627\tOR: 0.475410\tOF1: 0.517857\n",
      "Train Epoch: 18 [832/1083 (76%)]\tLoss: 0.681742 \tOP: 0.531915\tOR: 0.352113\tOF1: 0.423729\n",
      "Train Epoch: 18 [864/1083 (79%)]\tLoss: 0.681643 \tOP: 0.531915\tOR: 0.347222\tOF1: 0.420168\n",
      "Train Epoch: 18 [896/1083 (82%)]\tLoss: 0.679723 \tOP: 0.571429\tOR: 0.417910\tOF1: 0.482759\n",
      "Train Epoch: 18 [928/1083 (85%)]\tLoss: 0.680179 \tOP: 0.562500\tOR: 0.397059\tOF1: 0.465517\n",
      "Train Epoch: 18 [960/1083 (88%)]\tLoss: 0.680916 \tOP: 0.541667\tOR: 0.329114\tOF1: 0.409449\n",
      "Train Epoch: 18 [992/1083 (91%)]\tLoss: 0.680659 \tOP: 0.553191\tOR: 0.393939\tOF1: 0.460177\n",
      "Train Epoch: 18 [1024/1083 (94%)]\tLoss: 0.680141 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 18 [891/1083 (97%)]\tLoss: 0.682266 \tOP: 0.476190\tOR: 0.357143\tOF1: 0.408163\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6939 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n",
      "Train Epoch: 19 [0/1083 (0%)]\tLoss: 0.679211 \tOP: 0.576923\tOR: 0.422535\tOF1: 0.487805\n",
      "Train Epoch: 19 [32/1083 (3%)]\tLoss: 0.677927 \tOP: 0.603774\tOR: 0.438356\tOF1: 0.507937\n",
      "Train Epoch: 19 [64/1083 (6%)]\tLoss: 0.678370 \tOP: 0.588235\tOR: 0.468750\tOF1: 0.521739\n",
      "Train Epoch: 19 [96/1083 (9%)]\tLoss: 0.677842 \tOP: 0.603774\tOR: 0.457143\tOF1: 0.520325\n",
      "Train Epoch: 19 [128/1083 (12%)]\tLoss: 0.682781 \tOP: 0.511628\tOR: 0.289474\tOF1: 0.369748\n",
      "Train Epoch: 19 [160/1083 (15%)]\tLoss: 0.685555 \tOP: 0.468085\tOR: 0.309859\tOF1: 0.372881\n",
      "Train Epoch: 19 [192/1083 (18%)]\tLoss: 0.682580 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 19 [224/1083 (21%)]\tLoss: 0.682149 \tOP: 0.522727\tOR: 0.310811\tOF1: 0.389831\n",
      "Train Epoch: 19 [256/1083 (24%)]\tLoss: 0.678510 \tOP: 0.588235\tOR: 0.483871\tOF1: 0.530973\n",
      "Train Epoch: 19 [288/1083 (26%)]\tLoss: 0.681457 \tOP: 0.541667\tOR: 0.356164\tOF1: 0.429752\n",
      "Train Epoch: 19 [320/1083 (29%)]\tLoss: 0.678689 \tOP: 0.588235\tOR: 0.416667\tOF1: 0.487805\n",
      "Train Epoch: 19 [352/1083 (32%)]\tLoss: 0.680924 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 19 [384/1083 (35%)]\tLoss: 0.677554 \tOP: 0.611111\tOR: 0.478261\tOF1: 0.536585\n",
      "Train Epoch: 19 [416/1083 (38%)]\tLoss: 0.680149 \tOP: 0.568627\tOR: 0.483333\tOF1: 0.522523\n",
      "Train Epoch: 19 [448/1083 (41%)]\tLoss: 0.678161 \tOP: 0.596154\tOR: 0.455882\tOF1: 0.516667\n",
      "Train Epoch: 19 [480/1083 (44%)]\tLoss: 0.681639 \tOP: 0.533333\tOR: 0.387097\tOF1: 0.448598\n",
      "Train Epoch: 19 [512/1083 (47%)]\tLoss: 0.680193 \tOP: 0.562500\tOR: 0.364865\tOF1: 0.442623\n",
      "Train Epoch: 19 [544/1083 (50%)]\tLoss: 0.681994 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 19 [576/1083 (53%)]\tLoss: 0.680306 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 19 [608/1083 (56%)]\tLoss: 0.678520 \tOP: 0.588235\tOR: 0.422535\tOF1: 0.491803\n",
      "Train Epoch: 19 [640/1083 (59%)]\tLoss: 0.681386 \tOP: 0.543478\tOR: 0.396825\tOF1: 0.458716\n",
      "Train Epoch: 19 [672/1083 (62%)]\tLoss: 0.681853 \tOP: 0.531915\tOR: 0.378788\tOF1: 0.442478\n",
      "Train Epoch: 19 [704/1083 (65%)]\tLoss: 0.679334 \tOP: 0.571429\tOR: 0.466667\tOF1: 0.513761\n",
      "Train Epoch: 19 [736/1083 (68%)]\tLoss: 0.679743 \tOP: 0.571429\tOR: 0.405797\tOF1: 0.474576\n",
      "Train Epoch: 19 [768/1083 (71%)]\tLoss: 0.681272 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 19 [800/1083 (74%)]\tLoss: 0.681694 \tOP: 0.533333\tOR: 0.380952\tOF1: 0.444444\n",
      "Train Epoch: 19 [832/1083 (76%)]\tLoss: 0.678708 \tOP: 0.588235\tOR: 0.454545\tOF1: 0.512821\n",
      "Train Epoch: 19 [864/1083 (79%)]\tLoss: 0.680330 \tOP: 0.551020\tOR: 0.375000\tOF1: 0.446281\n",
      "Train Epoch: 19 [896/1083 (82%)]\tLoss: 0.679488 \tOP: 0.580000\tOR: 0.426471\tOF1: 0.491525\n",
      "Train Epoch: 19 [928/1083 (85%)]\tLoss: 0.681974 \tOP: 0.522727\tOR: 0.319444\tOF1: 0.396552\n",
      "Train Epoch: 19 [960/1083 (88%)]\tLoss: 0.680884 \tOP: 0.543478\tOR: 0.378788\tOF1: 0.446429\n",
      "Train Epoch: 19 [992/1083 (91%)]\tLoss: 0.680180 \tOP: 0.562500\tOR: 0.409091\tOF1: 0.473684\n",
      "Train Epoch: 19 [1024/1083 (94%)]\tLoss: 0.680558 \tOP: 0.553191\tOR: 0.406250\tOF1: 0.468468\n",
      "Train Epoch: 19 [891/1083 (97%)]\tLoss: 0.679324 \tOP: 0.533333\tOR: 0.380952\tOF1: 0.444444\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6929 \n",
      "OP: 0.242424\n",
      "OR: 0.228571\n",
      "OF1: 0.235294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'wider')\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "        \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for n in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for m in range(len(classes)):\n",
    "        #        if target[n][m] == 1 or preds[n][m] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[n][m] == target[n][m]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'wider')\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# DenseNet201\n",
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "model2 = torchvision.models.densenet201(pretrained=True)\n",
    "num_ftrs = model2.classifier.in_features\n",
    "model2.classifier = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1083 (0%)]\tLoss: 0.732575 \tOP: 0.088608\tOR: 0.097222\tOF1: 0.092715\n",
      "Train Epoch: 0 [32/1083 (3%)]\tLoss: 0.707817 \tOP: 0.136986\tOR: 0.133333\tOF1: 0.135135\n",
      "Train Epoch: 0 [64/1083 (6%)]\tLoss: 0.680108 \tOP: 0.140000\tOR: 0.101449\tOF1: 0.117647\n",
      "Train Epoch: 0 [96/1083 (9%)]\tLoss: 0.667057 \tOP: 0.069767\tOR: 0.048387\tOF1: 0.057143\n",
      "Train Epoch: 0 [128/1083 (12%)]\tLoss: 0.645402 \tOP: 0.027778\tOR: 0.014493\tOF1: 0.019048\n",
      "Train Epoch: 0 [160/1083 (15%)]\tLoss: 0.611889 \tOP: 0.138889\tOR: 0.079365\tOF1: 0.101010\n",
      "Train Epoch: 0 [192/1083 (18%)]\tLoss: 0.600936 \tOP: 0.103448\tOR: 0.042857\tOF1: 0.060606\n",
      "Train Epoch: 0 [224/1083 (21%)]\tLoss: 0.581095 \tOP: 0.040000\tOR: 0.015385\tOF1: 0.022222\n",
      "Train Epoch: 0 [256/1083 (24%)]\tLoss: 0.567764 \tOP: 0.038462\tOR: 0.014085\tOF1: 0.020619\n",
      "Train Epoch: 0 [288/1083 (26%)]\tLoss: 0.546047 \tOP: 0.040000\tOR: 0.015152\tOF1: 0.021978\n",
      "Train Epoch: 0 [320/1083 (29%)]\tLoss: 0.524420 \tOP: 0.148148\tOR: 0.055556\tOF1: 0.080808\n",
      "Train Epoch: 0 [352/1083 (32%)]\tLoss: 0.510825 \tOP: 0.076923\tOR: 0.028169\tOF1: 0.041237\n",
      "Train Epoch: 0 [384/1083 (35%)]\tLoss: 0.498525 \tOP: 0.041667\tOR: 0.014085\tOF1: 0.021053\n",
      "Train Epoch: 0 [416/1083 (38%)]\tLoss: 0.478165 \tOP: 0.083333\tOR: 0.027778\tOF1: 0.041667\n",
      "Train Epoch: 0 [448/1083 (41%)]\tLoss: 0.455891 \tOP: 0.080000\tOR: 0.032787\tOF1: 0.046512\n",
      "Train Epoch: 0 [480/1083 (44%)]\tLoss: 0.464799 \tOP: 0.115385\tOR: 0.041096\tOF1: 0.060606\n",
      "Train Epoch: 0 [512/1083 (47%)]\tLoss: 0.446531 \tOP: 0.076923\tOR: 0.028571\tOF1: 0.041667\n",
      "Train Epoch: 0 [544/1083 (50%)]\tLoss: 0.433952 \tOP: 0.076923\tOR: 0.027027\tOF1: 0.040000\n",
      "Train Epoch: 0 [576/1083 (53%)]\tLoss: 0.417255 \tOP: 0.040000\tOR: 0.015385\tOF1: 0.022222\n",
      "Train Epoch: 0 [608/1083 (56%)]\tLoss: 0.405292 \tOP: 0.074074\tOR: 0.028986\tOF1: 0.041667\n",
      "Train Epoch: 0 [640/1083 (59%)]\tLoss: 0.392836 \tOP: 0.107143\tOR: 0.044776\tOF1: 0.063158\n",
      "Train Epoch: 0 [672/1083 (62%)]\tLoss: 0.384446 \tOP: 0.076923\tOR: 0.031746\tOF1: 0.044944\n",
      "Train Epoch: 0 [704/1083 (65%)]\tLoss: 0.368415 \tOP: 0.041667\tOR: 0.015873\tOF1: 0.022989\n",
      "Train Epoch: 0 [736/1083 (68%)]\tLoss: 0.363796 \tOP: 0.071429\tOR: 0.031250\tOF1: 0.043478\n",
      "Train Epoch: 0 [768/1083 (71%)]\tLoss: 0.363952 \tOP: 0.074074\tOR: 0.028571\tOF1: 0.041237\n",
      "Train Epoch: 0 [800/1083 (74%)]\tLoss: 0.348020 \tOP: 0.076923\tOR: 0.031746\tOF1: 0.044944\n",
      "Train Epoch: 0 [832/1083 (76%)]\tLoss: 0.348457 \tOP: 0.076923\tOR: 0.030769\tOF1: 0.043956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-bbf89c8d258d>:40: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [864/1083 (79%)]\tLoss: 0.342054 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [896/1083 (82%)]\tLoss: 0.334149 \tOP: 0.153846\tOR: 0.054795\tOF1: 0.080808\n",
      "Train Epoch: 0 [928/1083 (85%)]\tLoss: 0.329278 \tOP: 0.080000\tOR: 0.029851\tOF1: 0.043478\n",
      "Train Epoch: 0 [960/1083 (88%)]\tLoss: 0.321470 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [992/1083 (91%)]\tLoss: 0.305416 \tOP: 0.041667\tOR: 0.014706\tOF1: 0.021739\n",
      "Train Epoch: 0 [1024/1083 (94%)]\tLoss: 0.314984 \tOP: 0.080000\tOR: 0.027778\tOF1: 0.041237\n",
      "Train Epoch: 0 [891/1083 (97%)]\tLoss: 0.315591 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-78eea7326411>:45: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.3080 \n",
      "OP: 0.040000\n",
      "OR: 0.028571\n",
      "OF1: 0.033333\n",
      "\n",
      "Train Epoch: 1 [0/1083 (0%)]\tLoss: 0.273257 \tOP: 0.153846\tOR: 0.058824\tOF1: 0.085106\n",
      "Train Epoch: 1 [32/1083 (3%)]\tLoss: 0.261681 \tOP: 0.206897\tOR: 0.090909\tOF1: 0.126316\n",
      "Train Epoch: 1 [64/1083 (6%)]\tLoss: 0.244387 \tOP: 0.333333\tOR: 0.180328\tOF1: 0.234043\n",
      "Train Epoch: 1 [96/1083 (9%)]\tLoss: 0.249878 \tOP: 0.290323\tOR: 0.145161\tOF1: 0.193548\n",
      "Train Epoch: 1 [128/1083 (12%)]\tLoss: 0.238852 \tOP: 0.241379\tOR: 0.106061\tOF1: 0.147368\n",
      "Train Epoch: 1 [160/1083 (15%)]\tLoss: 0.241438 \tOP: 0.153846\tOR: 0.058824\tOF1: 0.085106\n",
      "Train Epoch: 1 [192/1083 (18%)]\tLoss: 0.250312 \tOP: 0.206897\tOR: 0.082192\tOF1: 0.117647\n",
      "Train Epoch: 1 [224/1083 (21%)]\tLoss: 0.224105 \tOP: 0.371429\tOR: 0.213115\tOF1: 0.270833\n",
      "Train Epoch: 1 [256/1083 (24%)]\tLoss: 0.219151 \tOP: 0.266667\tOR: 0.125000\tOF1: 0.170213\n",
      "Train Epoch: 1 [288/1083 (26%)]\tLoss: 0.237909 \tOP: 0.153846\tOR: 0.058824\tOF1: 0.085106\n",
      "Train Epoch: 1 [320/1083 (29%)]\tLoss: 0.234306 \tOP: 0.352941\tOR: 0.160000\tOF1: 0.220183\n",
      "Train Epoch: 1 [352/1083 (32%)]\tLoss: 0.229571 \tOP: 0.178571\tOR: 0.072464\tOF1: 0.103093\n",
      "Train Epoch: 1 [384/1083 (35%)]\tLoss: 0.236794 \tOP: 0.214286\tOR: 0.082192\tOF1: 0.118812\n",
      "Train Epoch: 1 [416/1083 (38%)]\tLoss: 0.226759 \tOP: 0.312500\tOR: 0.142857\tOF1: 0.196078\n",
      "Train Epoch: 1 [448/1083 (41%)]\tLoss: 0.216574 \tOP: 0.333333\tOR: 0.164179\tOF1: 0.220000\n",
      "Train Epoch: 1 [480/1083 (44%)]\tLoss: 0.226856 \tOP: 0.352941\tOR: 0.166667\tOF1: 0.226415\n",
      "Train Epoch: 1 [512/1083 (47%)]\tLoss: 0.212984 \tOP: 0.361111\tOR: 0.180556\tOF1: 0.240741\n",
      "Train Epoch: 1 [544/1083 (50%)]\tLoss: 0.206020 \tOP: 0.352941\tOR: 0.184615\tOF1: 0.242424\n",
      "Train Epoch: 1 [576/1083 (53%)]\tLoss: 0.221879 \tOP: 0.351351\tOR: 0.185714\tOF1: 0.242991\n",
      "Train Epoch: 1 [608/1083 (56%)]\tLoss: 0.219578 \tOP: 0.352941\tOR: 0.171429\tOF1: 0.230769\n",
      "Train Epoch: 1 [640/1083 (59%)]\tLoss: 0.218697 \tOP: 0.250000\tOR: 0.117647\tOF1: 0.160000\n",
      "Train Epoch: 1 [672/1083 (62%)]\tLoss: 0.213043 \tOP: 0.342857\tOR: 0.176471\tOF1: 0.233010\n",
      "Train Epoch: 1 [704/1083 (65%)]\tLoss: 0.216538 \tOP: 0.281250\tOR: 0.126761\tOF1: 0.174757\n",
      "Train Epoch: 1 [736/1083 (68%)]\tLoss: 0.221604 \tOP: 0.361111\tOR: 0.180556\tOF1: 0.240741\n",
      "Train Epoch: 1 [768/1083 (71%)]\tLoss: 0.217474 \tOP: 0.314286\tOR: 0.161765\tOF1: 0.213592\n",
      "Train Epoch: 1 [800/1083 (74%)]\tLoss: 0.199759 \tOP: 0.352941\tOR: 0.196721\tOF1: 0.252632\n",
      "Train Epoch: 1 [832/1083 (76%)]\tLoss: 0.179900 \tOP: 0.410256\tOR: 0.271186\tOF1: 0.326531\n",
      "Train Epoch: 1 [864/1083 (79%)]\tLoss: 0.210547 \tOP: 0.333333\tOR: 0.152778\tOF1: 0.209524\n",
      "Train Epoch: 1 [896/1083 (82%)]\tLoss: 0.195072 \tOP: 0.388889\tOR: 0.225806\tOF1: 0.285714\n",
      "Train Epoch: 1 [928/1083 (85%)]\tLoss: 0.219061 \tOP: 0.342857\tOR: 0.169014\tOF1: 0.226415\n",
      "Train Epoch: 1 [960/1083 (88%)]\tLoss: 0.210470 \tOP: 0.394737\tOR: 0.205479\tOF1: 0.270270\n",
      "Train Epoch: 1 [992/1083 (91%)]\tLoss: 0.214075 \tOP: 0.371429\tOR: 0.173333\tOF1: 0.236364\n",
      "Train Epoch: 1 [1024/1083 (94%)]\tLoss: 0.200622 \tOP: 0.378378\tOR: 0.200000\tOF1: 0.261682\n",
      "Train Epoch: 1 [891/1083 (97%)]\tLoss: 0.202544 \tOP: 0.290323\tOR: 0.155172\tOF1: 0.202247\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2486 \n",
      "OP: 0.172414\n",
      "OR: 0.142857\n",
      "OF1: 0.156250\n",
      "\n",
      "Train Epoch: 2 [0/1083 (0%)]\tLoss: 0.175682 \tOP: 0.378378\tOR: 0.225806\tOF1: 0.282828\n",
      "Train Epoch: 2 [32/1083 (3%)]\tLoss: 0.177330 \tOP: 0.405405\tOR: 0.223881\tOF1: 0.288462\n",
      "Train Epoch: 2 [64/1083 (6%)]\tLoss: 0.169294 \tOP: 0.421053\tOR: 0.228571\tOF1: 0.296296\n",
      "Train Epoch: 2 [96/1083 (9%)]\tLoss: 0.163203 \tOP: 0.371429\tOR: 0.196970\tOF1: 0.257426\n",
      "Train Epoch: 2 [128/1083 (12%)]\tLoss: 0.173892 \tOP: 0.405405\tOR: 0.194805\tOF1: 0.263158\n",
      "Train Epoch: 2 [160/1083 (15%)]\tLoss: 0.169367 \tOP: 0.400000\tOR: 0.218750\tOF1: 0.282828\n",
      "Train Epoch: 2 [192/1083 (18%)]\tLoss: 0.148308 \tOP: 0.461538\tOR: 0.285714\tOF1: 0.352941\n",
      "Train Epoch: 2 [224/1083 (21%)]\tLoss: 0.177295 \tOP: 0.421053\tOR: 0.216216\tOF1: 0.285714\n",
      "Train Epoch: 2 [256/1083 (24%)]\tLoss: 0.158767 \tOP: 0.371429\tOR: 0.183099\tOF1: 0.245283\n",
      "Train Epoch: 2 [288/1083 (26%)]\tLoss: 0.148544 \tOP: 0.432432\tOR: 0.246154\tOF1: 0.313725\n",
      "Train Epoch: 2 [320/1083 (29%)]\tLoss: 0.159693 \tOP: 0.432432\tOR: 0.246154\tOF1: 0.313725\n",
      "Train Epoch: 2 [352/1083 (32%)]\tLoss: 0.154099 \tOP: 0.435897\tOR: 0.257576\tOF1: 0.323810\n",
      "Train Epoch: 2 [384/1083 (35%)]\tLoss: 0.172197 \tOP: 0.421053\tOR: 0.219178\tOF1: 0.288288\n",
      "Train Epoch: 2 [416/1083 (38%)]\tLoss: 0.171346 \tOP: 0.405405\tOR: 0.200000\tOF1: 0.267857\n",
      "Train Epoch: 2 [448/1083 (41%)]\tLoss: 0.160490 \tOP: 0.461538\tOR: 0.257143\tOF1: 0.330275\n",
      "Train Epoch: 2 [480/1083 (44%)]\tLoss: 0.147716 \tOP: 0.421053\tOR: 0.238806\tOF1: 0.304762\n",
      "Train Epoch: 2 [512/1083 (47%)]\tLoss: 0.180109 \tOP: 0.435897\tOR: 0.215190\tOF1: 0.288136\n",
      "Train Epoch: 2 [544/1083 (50%)]\tLoss: 0.150126 \tOP: 0.500000\tOR: 0.338983\tOF1: 0.404040\n",
      "Train Epoch: 2 [576/1083 (53%)]\tLoss: 0.156296 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 2 [608/1083 (56%)]\tLoss: 0.147878 \tOP: 0.523810\tOR: 0.323529\tOF1: 0.400000\n",
      "Train Epoch: 2 [640/1083 (59%)]\tLoss: 0.170116 \tOP: 0.487805\tOR: 0.253165\tOF1: 0.333333\n",
      "Train Epoch: 2 [672/1083 (62%)]\tLoss: 0.148120 \tOP: 0.463415\tOR: 0.292308\tOF1: 0.358491\n",
      "Train Epoch: 2 [704/1083 (65%)]\tLoss: 0.171451 \tOP: 0.447368\tOR: 0.236111\tOF1: 0.309091\n",
      "Train Epoch: 2 [736/1083 (68%)]\tLoss: 0.149093 \tOP: 0.512195\tOR: 0.318182\tOF1: 0.392523\n",
      "Train Epoch: 2 [768/1083 (71%)]\tLoss: 0.165247 \tOP: 0.388889\tOR: 0.197183\tOF1: 0.261682\n",
      "Train Epoch: 2 [800/1083 (74%)]\tLoss: 0.141323 \tOP: 0.534884\tOR: 0.353846\tOF1: 0.425926\n",
      "Train Epoch: 2 [832/1083 (76%)]\tLoss: 0.156201 \tOP: 0.463415\tOR: 0.260274\tOF1: 0.333333\n",
      "Train Epoch: 2 [864/1083 (79%)]\tLoss: 0.146774 \tOP: 0.500000\tOR: 0.308824\tOF1: 0.381818\n",
      "Train Epoch: 2 [896/1083 (82%)]\tLoss: 0.136919 \tOP: 0.522727\tOR: 0.359375\tOF1: 0.425926\n",
      "Train Epoch: 2 [928/1083 (85%)]\tLoss: 0.153322 \tOP: 0.487179\tOR: 0.275362\tOF1: 0.351852\n",
      "Train Epoch: 2 [960/1083 (88%)]\tLoss: 0.164116 \tOP: 0.500000\tOR: 0.295775\tOF1: 0.371681\n",
      "Train Epoch: 2 [992/1083 (91%)]\tLoss: 0.141680 \tOP: 0.432432\tOR: 0.253968\tOF1: 0.320000\n",
      "Train Epoch: 2 [1024/1083 (94%)]\tLoss: 0.136227 \tOP: 0.523810\tOR: 0.372881\tOF1: 0.435644\n",
      "Train Epoch: 2 [891/1083 (97%)]\tLoss: 0.153592 \tOP: 0.432432\tOR: 0.290909\tOF1: 0.347826\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2355 \n",
      "OP: 0.172414\n",
      "OR: 0.142857\n",
      "OF1: 0.156250\n",
      "\n",
      "Train Epoch: 3 [0/1083 (0%)]\tLoss: 0.132695 \tOP: 0.617021\tOR: 0.408451\tOF1: 0.491525\n",
      "Train Epoch: 3 [32/1083 (3%)]\tLoss: 0.112738 \tOP: 0.583333\tOR: 0.417910\tOF1: 0.486957\n",
      "Train Epoch: 3 [64/1083 (6%)]\tLoss: 0.134005 \tOP: 0.581395\tOR: 0.352113\tOF1: 0.438596\n",
      "Train Epoch: 3 [96/1083 (9%)]\tLoss: 0.118200 \tOP: 0.571429\tOR: 0.413793\tOF1: 0.480000\n",
      "Train Epoch: 3 [128/1083 (12%)]\tLoss: 0.141290 \tOP: 0.522727\tOR: 0.306667\tOF1: 0.386555\n",
      "Train Epoch: 3 [160/1083 (15%)]\tLoss: 0.113703 \tOP: 0.617021\tOR: 0.475410\tOF1: 0.537037\n",
      "Train Epoch: 3 [192/1083 (18%)]\tLoss: 0.116651 \tOP: 0.531915\tOR: 0.384615\tOF1: 0.446429\n",
      "Train Epoch: 3 [224/1083 (21%)]\tLoss: 0.131060 \tOP: 0.565217\tOR: 0.366197\tOF1: 0.444444\n",
      "Train Epoch: 3 [256/1083 (24%)]\tLoss: 0.120832 \tOP: 0.653061\tOR: 0.444444\tOF1: 0.528926\n",
      "Train Epoch: 3 [288/1083 (26%)]\tLoss: 0.123267 \tOP: 0.617021\tOR: 0.426471\tOF1: 0.504348\n",
      "Train Epoch: 3 [320/1083 (29%)]\tLoss: 0.122748 \tOP: 0.604167\tOR: 0.460317\tOF1: 0.522523\n",
      "Train Epoch: 3 [352/1083 (32%)]\tLoss: 0.123698 \tOP: 0.638298\tOR: 0.394737\tOF1: 0.487805\n",
      "Train Epoch: 3 [384/1083 (35%)]\tLoss: 0.115486 \tOP: 0.652174\tOR: 0.434783\tOF1: 0.521739\n",
      "Train Epoch: 3 [416/1083 (38%)]\tLoss: 0.115722 \tOP: 0.630435\tOR: 0.439394\tOF1: 0.517857\n",
      "Train Epoch: 3 [448/1083 (41%)]\tLoss: 0.111630 \tOP: 0.583333\tOR: 0.482759\tOF1: 0.528302\n",
      "Train Epoch: 3 [480/1083 (44%)]\tLoss: 0.123144 \tOP: 0.586957\tOR: 0.369863\tOF1: 0.453782\n",
      "Train Epoch: 3 [512/1083 (47%)]\tLoss: 0.122780 \tOP: 0.625000\tOR: 0.434783\tOF1: 0.512821\n",
      "Train Epoch: 3 [544/1083 (50%)]\tLoss: 0.127406 \tOP: 0.595238\tOR: 0.362319\tOF1: 0.450450\n",
      "Train Epoch: 3 [576/1083 (53%)]\tLoss: 0.103628 \tOP: 0.625000\tOR: 0.468750\tOF1: 0.535714\n",
      "Train Epoch: 3 [608/1083 (56%)]\tLoss: 0.113087 \tOP: 0.645833\tOR: 0.442857\tOF1: 0.525424\n",
      "Train Epoch: 3 [640/1083 (59%)]\tLoss: 0.116857 \tOP: 0.608696\tOR: 0.405797\tOF1: 0.486957\n",
      "Train Epoch: 3 [672/1083 (62%)]\tLoss: 0.120616 \tOP: 0.604651\tOR: 0.393939\tOF1: 0.477064\n",
      "Train Epoch: 3 [704/1083 (65%)]\tLoss: 0.118161 \tOP: 0.638298\tOR: 0.428571\tOF1: 0.512821\n",
      "Train Epoch: 3 [736/1083 (68%)]\tLoss: 0.129865 \tOP: 0.666667\tOR: 0.421053\tOF1: 0.516129\n",
      "Train Epoch: 3 [768/1083 (71%)]\tLoss: 0.113921 \tOP: 0.645833\tOR: 0.413333\tOF1: 0.504065\n",
      "Train Epoch: 3 [800/1083 (74%)]\tLoss: 0.106344 \tOP: 0.680851\tOR: 0.477612\tOF1: 0.561404\n",
      "Train Epoch: 3 [832/1083 (76%)]\tLoss: 0.124288 \tOP: 0.608696\tOR: 0.400000\tOF1: 0.482759\n",
      "Train Epoch: 3 [864/1083 (79%)]\tLoss: 0.095657 \tOP: 0.638298\tOR: 0.526316\tOF1: 0.576923\n",
      "Train Epoch: 3 [896/1083 (82%)]\tLoss: 0.113677 \tOP: 0.590909\tOR: 0.382353\tOF1: 0.464286\n",
      "Train Epoch: 3 [928/1083 (85%)]\tLoss: 0.122326 \tOP: 0.608696\tOR: 0.394366\tOF1: 0.478632\n",
      "Train Epoch: 3 [960/1083 (88%)]\tLoss: 0.116187 \tOP: 0.645833\tOR: 0.430556\tOF1: 0.516667\n",
      "Train Epoch: 3 [992/1083 (91%)]\tLoss: 0.133041 \tOP: 0.536585\tOR: 0.328358\tOF1: 0.407407\n",
      "Train Epoch: 3 [1024/1083 (94%)]\tLoss: 0.107490 \tOP: 0.666667\tOR: 0.477612\tOF1: 0.556522\n",
      "Train Epoch: 3 [891/1083 (97%)]\tLoss: 0.117041 \tOP: 0.604651\tOR: 0.456140\tOF1: 0.520000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2322 \n",
      "OP: 0.206897\n",
      "OR: 0.171429\n",
      "OF1: 0.187500\n",
      "\n",
      "Train Epoch: 4 [0/1083 (0%)]\tLoss: 0.099731 \tOP: 0.705882\tOR: 0.553846\tOF1: 0.620690\n",
      "Train Epoch: 4 [32/1083 (3%)]\tLoss: 0.095653 \tOP: 0.719298\tOR: 0.611940\tOF1: 0.661290\n",
      "Train Epoch: 4 [64/1083 (6%)]\tLoss: 0.085385 \tOP: 0.722222\tOR: 0.600000\tOF1: 0.655462\n",
      "Train Epoch: 4 [96/1083 (9%)]\tLoss: 0.102542 \tOP: 0.745763\tOR: 0.602740\tOF1: 0.666667\n",
      "Train Epoch: 4 [128/1083 (12%)]\tLoss: 0.095334 \tOP: 0.750000\tOR: 0.617647\tOF1: 0.677419\n",
      "Train Epoch: 4 [160/1083 (15%)]\tLoss: 0.101554 \tOP: 0.693878\tOR: 0.492754\tOF1: 0.576271\n",
      "Train Epoch: 4 [192/1083 (18%)]\tLoss: 0.092103 \tOP: 0.711538\tOR: 0.569231\tOF1: 0.632479\n",
      "Train Epoch: 4 [224/1083 (21%)]\tLoss: 0.088279 \tOP: 0.716981\tOR: 0.603175\tOF1: 0.655172\n",
      "Train Epoch: 4 [256/1083 (24%)]\tLoss: 0.094628 \tOP: 0.680851\tOR: 0.484848\tOF1: 0.566372\n",
      "Train Epoch: 4 [288/1083 (26%)]\tLoss: 0.098007 \tOP: 0.735849\tOR: 0.513158\tOF1: 0.604651\n",
      "Train Epoch: 4 [320/1083 (29%)]\tLoss: 0.079762 \tOP: 0.719298\tOR: 0.672131\tOF1: 0.694915\n",
      "Train Epoch: 4 [352/1083 (32%)]\tLoss: 0.080211 \tOP: 0.758621\tOR: 0.733333\tOF1: 0.745763\n",
      "Train Epoch: 4 [384/1083 (35%)]\tLoss: 0.089962 \tOP: 0.711538\tOR: 0.544118\tOF1: 0.616667\n",
      "Train Epoch: 4 [416/1083 (38%)]\tLoss: 0.083580 \tOP: 0.732143\tOR: 0.594203\tOF1: 0.656000\n",
      "Train Epoch: 4 [448/1083 (41%)]\tLoss: 0.110343 \tOP: 0.722222\tOR: 0.513158\tOF1: 0.600000\n",
      "Train Epoch: 4 [480/1083 (44%)]\tLoss: 0.102794 \tOP: 0.690909\tOR: 0.506667\tOF1: 0.584615\n",
      "Train Epoch: 4 [512/1083 (47%)]\tLoss: 0.095525 \tOP: 0.727273\tOR: 0.571429\tOF1: 0.640000\n",
      "Train Epoch: 4 [544/1083 (50%)]\tLoss: 0.106231 \tOP: 0.700000\tOR: 0.479452\tOF1: 0.569106\n",
      "Train Epoch: 4 [576/1083 (53%)]\tLoss: 0.093703 \tOP: 0.740741\tOR: 0.571429\tOF1: 0.645161\n",
      "Train Epoch: 4 [608/1083 (56%)]\tLoss: 0.088532 \tOP: 0.730769\tOR: 0.575758\tOF1: 0.644068\n",
      "Train Epoch: 4 [640/1083 (59%)]\tLoss: 0.080960 \tOP: 0.763636\tOR: 0.677419\tOF1: 0.717949\n",
      "Train Epoch: 4 [672/1083 (62%)]\tLoss: 0.095190 \tOP: 0.701754\tOR: 0.588235\tOF1: 0.640000\n",
      "Train Epoch: 4 [704/1083 (65%)]\tLoss: 0.090293 \tOP: 0.750000\tOR: 0.591549\tOF1: 0.661417\n",
      "Train Epoch: 4 [736/1083 (68%)]\tLoss: 0.097539 \tOP: 0.750000\tOR: 0.520000\tOF1: 0.614173\n",
      "Train Epoch: 4 [768/1083 (71%)]\tLoss: 0.088134 \tOP: 0.720000\tOR: 0.590164\tOF1: 0.648649\n",
      "Train Epoch: 4 [800/1083 (74%)]\tLoss: 0.082338 \tOP: 0.716981\tOR: 0.593750\tOF1: 0.649573\n",
      "Train Epoch: 4 [832/1083 (76%)]\tLoss: 0.087864 \tOP: 0.719298\tOR: 0.630769\tOF1: 0.672131\n",
      "Train Epoch: 4 [864/1083 (79%)]\tLoss: 0.086519 \tOP: 0.735849\tOR: 0.590909\tOF1: 0.655462\n",
      "Train Epoch: 4 [896/1083 (82%)]\tLoss: 0.094020 \tOP: 0.700000\tOR: 0.507246\tOF1: 0.588235\n",
      "Train Epoch: 4 [928/1083 (85%)]\tLoss: 0.093642 \tOP: 0.722222\tOR: 0.549296\tOF1: 0.624000\n",
      "Train Epoch: 4 [960/1083 (88%)]\tLoss: 0.085233 \tOP: 0.736842\tOR: 0.617647\tOF1: 0.672000\n",
      "Train Epoch: 4 [992/1083 (91%)]\tLoss: 0.093759 \tOP: 0.736842\tOR: 0.552632\tOF1: 0.631579\n",
      "Train Epoch: 4 [1024/1083 (94%)]\tLoss: 0.086260 \tOP: 0.765625\tOR: 0.680556\tOF1: 0.720588\n",
      "Train Epoch: 4 [891/1083 (97%)]\tLoss: 0.085284 \tOP: 0.702128\tOR: 0.600000\tOF1: 0.647059\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2341 \n",
      "OP: 0.258065\n",
      "OR: 0.228571\n",
      "OF1: 0.242424\n",
      "\n",
      "Train Epoch: 5 [0/1083 (0%)]\tLoss: 0.082723 \tOP: 0.771930\tOR: 0.628571\tOF1: 0.692913\n",
      "Train Epoch: 5 [32/1083 (3%)]\tLoss: 0.071664 \tOP: 0.775862\tOR: 0.714286\tOF1: 0.743802\n",
      "Train Epoch: 5 [64/1083 (6%)]\tLoss: 0.082233 \tOP: 0.754098\tOR: 0.647887\tOF1: 0.696970\n",
      "Train Epoch: 5 [96/1083 (9%)]\tLoss: 0.080942 \tOP: 0.779661\tOR: 0.647887\tOF1: 0.707692\n",
      "Train Epoch: 5 [128/1083 (12%)]\tLoss: 0.073296 \tOP: 0.835616\tOR: 0.802632\tOF1: 0.818792\n",
      "Train Epoch: 5 [160/1083 (15%)]\tLoss: 0.075455 \tOP: 0.774194\tOR: 0.695652\tOF1: 0.732824\n",
      "Train Epoch: 5 [192/1083 (18%)]\tLoss: 0.067902 \tOP: 0.777778\tOR: 0.753846\tOF1: 0.765625\n",
      "Train Epoch: 5 [224/1083 (21%)]\tLoss: 0.079511 \tOP: 0.762712\tOR: 0.608108\tOF1: 0.676692\n",
      "Train Epoch: 5 [256/1083 (24%)]\tLoss: 0.067190 \tOP: 0.766667\tOR: 0.730159\tOF1: 0.747967\n",
      "Train Epoch: 5 [288/1083 (26%)]\tLoss: 0.071800 \tOP: 0.777778\tOR: 0.710145\tOF1: 0.742424\n",
      "Train Epoch: 5 [320/1083 (29%)]\tLoss: 0.066782 \tOP: 0.812500\tOR: 0.800000\tOF1: 0.806202\n",
      "Train Epoch: 5 [352/1083 (32%)]\tLoss: 0.067542 \tOP: 0.803030\tOR: 0.746479\tOF1: 0.773723\n",
      "Train Epoch: 5 [384/1083 (35%)]\tLoss: 0.072402 \tOP: 0.761905\tOR: 0.695652\tOF1: 0.727273\n",
      "Train Epoch: 5 [416/1083 (38%)]\tLoss: 0.061590 \tOP: 0.808824\tOR: 0.859375\tOF1: 0.833333\n",
      "Train Epoch: 5 [448/1083 (41%)]\tLoss: 0.073154 \tOP: 0.774194\tOR: 0.676056\tOF1: 0.721805\n",
      "Train Epoch: 5 [480/1083 (44%)]\tLoss: 0.076737 \tOP: 0.740000\tOR: 0.569231\tOF1: 0.643478\n",
      "Train Epoch: 5 [512/1083 (47%)]\tLoss: 0.078641 \tOP: 0.750000\tOR: 0.617647\tOF1: 0.677419\n",
      "Train Epoch: 5 [544/1083 (50%)]\tLoss: 0.083020 \tOP: 0.716981\tOR: 0.535211\tOF1: 0.612903\n",
      "Train Epoch: 5 [576/1083 (53%)]\tLoss: 0.071396 \tOP: 0.765625\tOR: 0.700000\tOF1: 0.731343\n",
      "Train Epoch: 5 [608/1083 (56%)]\tLoss: 0.067822 \tOP: 0.779661\tOR: 0.730159\tOF1: 0.754098\n",
      "Train Epoch: 5 [640/1083 (59%)]\tLoss: 0.074991 \tOP: 0.800000\tOR: 0.716418\tOF1: 0.755906\n",
      "Train Epoch: 5 [672/1083 (62%)]\tLoss: 0.065673 \tOP: 0.774194\tOR: 0.738462\tOF1: 0.755906\n",
      "Train Epoch: 5 [704/1083 (65%)]\tLoss: 0.069376 \tOP: 0.766667\tOR: 0.696970\tOF1: 0.730159\n",
      "Train Epoch: 5 [736/1083 (68%)]\tLoss: 0.061333 \tOP: 0.787879\tOR: 0.764706\tOF1: 0.776119\n",
      "Train Epoch: 5 [768/1083 (71%)]\tLoss: 0.067837 \tOP: 0.825397\tOR: 0.764706\tOF1: 0.793893\n",
      "Train Epoch: 5 [800/1083 (74%)]\tLoss: 0.070217 \tOP: 0.770492\tOR: 0.681159\tOF1: 0.723077\n",
      "Train Epoch: 5 [832/1083 (76%)]\tLoss: 0.068066 \tOP: 0.761905\tOR: 0.738462\tOF1: 0.750000\n",
      "Train Epoch: 5 [864/1083 (79%)]\tLoss: 0.060645 \tOP: 0.776119\tOR: 0.812500\tOF1: 0.793893\n",
      "Train Epoch: 5 [896/1083 (82%)]\tLoss: 0.080902 \tOP: 0.745098\tOR: 0.535211\tOF1: 0.622951\n",
      "Train Epoch: 5 [928/1083 (85%)]\tLoss: 0.069435 \tOP: 0.779661\tOR: 0.657143\tOF1: 0.713178\n",
      "Train Epoch: 5 [960/1083 (88%)]\tLoss: 0.075073 \tOP: 0.774194\tOR: 0.666667\tOF1: 0.716418\n",
      "Train Epoch: 5 [992/1083 (91%)]\tLoss: 0.062599 \tOP: 0.754098\tOR: 0.730159\tOF1: 0.741935\n",
      "Train Epoch: 5 [1024/1083 (94%)]\tLoss: 0.069378 \tOP: 0.741379\tOR: 0.623188\tOF1: 0.677165\n",
      "Train Epoch: 5 [891/1083 (97%)]\tLoss: 0.074620 \tOP: 0.719298\tOR: 0.650794\tOF1: 0.683333\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2365 \n",
      "OP: 0.218750\n",
      "OR: 0.200000\n",
      "OF1: 0.208955\n",
      "\n",
      "Train Epoch: 6 [0/1083 (0%)]\tLoss: 0.067962 \tOP: 0.796875\tOR: 0.680000\tOF1: 0.733813\n",
      "Train Epoch: 6 [32/1083 (3%)]\tLoss: 0.054926 \tOP: 0.845070\tOR: 0.882353\tOF1: 0.863309\n",
      "Train Epoch: 6 [64/1083 (6%)]\tLoss: 0.061945 \tOP: 0.820896\tOR: 0.743243\tOF1: 0.780142\n",
      "Train Epoch: 6 [96/1083 (9%)]\tLoss: 0.070140 \tOP: 0.825397\tOR: 0.684211\tOF1: 0.748201\n",
      "Train Epoch: 6 [128/1083 (12%)]\tLoss: 0.051726 \tOP: 0.818182\tOR: 0.843750\tOF1: 0.830769\n",
      "Train Epoch: 6 [160/1083 (15%)]\tLoss: 0.069470 \tOP: 0.770492\tOR: 0.681159\tOF1: 0.723077\n",
      "Train Epoch: 6 [192/1083 (18%)]\tLoss: 0.050488 \tOP: 0.820896\tOR: 0.808824\tOF1: 0.814815\n",
      "Train Epoch: 6 [224/1083 (21%)]\tLoss: 0.053827 \tOP: 0.835616\tOR: 0.859155\tOF1: 0.847222\n",
      "Train Epoch: 6 [256/1083 (24%)]\tLoss: 0.068859 \tOP: 0.822581\tOR: 0.708333\tOF1: 0.761194\n",
      "Train Epoch: 6 [288/1083 (26%)]\tLoss: 0.060152 \tOP: 0.784615\tOR: 0.750000\tOF1: 0.766917\n",
      "Train Epoch: 6 [320/1083 (29%)]\tLoss: 0.065166 \tOP: 0.787879\tOR: 0.722222\tOF1: 0.753623\n",
      "Train Epoch: 6 [352/1083 (32%)]\tLoss: 0.057527 \tOP: 0.805970\tOR: 0.750000\tOF1: 0.776978\n",
      "Train Epoch: 6 [384/1083 (35%)]\tLoss: 0.060369 \tOP: 0.774194\tOR: 0.695652\tOF1: 0.732824\n",
      "Train Epoch: 6 [416/1083 (38%)]\tLoss: 0.053007 \tOP: 0.800000\tOR: 0.786885\tOF1: 0.793388\n",
      "Train Epoch: 6 [448/1083 (41%)]\tLoss: 0.065387 \tOP: 0.823529\tOR: 0.756757\tOF1: 0.788732\n",
      "Train Epoch: 6 [480/1083 (44%)]\tLoss: 0.050448 \tOP: 0.800000\tOR: 0.825397\tOF1: 0.812500\n",
      "Train Epoch: 6 [512/1083 (47%)]\tLoss: 0.056344 \tOP: 0.814286\tOR: 0.802817\tOF1: 0.808511\n",
      "Train Epoch: 6 [544/1083 (50%)]\tLoss: 0.053250 \tOP: 0.775862\tOR: 0.737705\tOF1: 0.756303\n",
      "Train Epoch: 6 [576/1083 (53%)]\tLoss: 0.050840 \tOP: 0.825397\tOR: 0.838710\tOF1: 0.832000\n",
      "Train Epoch: 6 [608/1083 (56%)]\tLoss: 0.057378 \tOP: 0.783333\tOR: 0.734375\tOF1: 0.758065\n",
      "Train Epoch: 6 [640/1083 (59%)]\tLoss: 0.055045 \tOP: 0.809524\tOR: 0.809524\tOF1: 0.809524\n",
      "Train Epoch: 6 [672/1083 (62%)]\tLoss: 0.051842 \tOP: 0.826667\tOR: 0.873239\tOF1: 0.849315\n",
      "Train Epoch: 6 [704/1083 (65%)]\tLoss: 0.050247 \tOP: 0.822581\tOR: 0.822581\tOF1: 0.822581\n",
      "Train Epoch: 6 [736/1083 (68%)]\tLoss: 0.057467 \tOP: 0.833333\tOR: 0.800000\tOF1: 0.816327\n",
      "Train Epoch: 6 [768/1083 (71%)]\tLoss: 0.059183 \tOP: 0.800000\tOR: 0.753623\tOF1: 0.776119\n",
      "Train Epoch: 6 [800/1083 (74%)]\tLoss: 0.044475 \tOP: 0.808824\tOR: 0.859375\tOF1: 0.833333\n",
      "Train Epoch: 6 [832/1083 (76%)]\tLoss: 0.052815 \tOP: 0.842857\tOR: 0.830986\tOF1: 0.836879\n",
      "Train Epoch: 6 [864/1083 (79%)]\tLoss: 0.056342 \tOP: 0.787879\tOR: 0.742857\tOF1: 0.764706\n",
      "Train Epoch: 6 [896/1083 (82%)]\tLoss: 0.053518 \tOP: 0.830986\tOR: 0.830986\tOF1: 0.830986\n",
      "Train Epoch: 6 [928/1083 (85%)]\tLoss: 0.051184 \tOP: 0.818182\tOR: 0.794118\tOF1: 0.805970\n",
      "Train Epoch: 6 [960/1083 (88%)]\tLoss: 0.058055 \tOP: 0.791045\tOR: 0.736111\tOF1: 0.762590\n",
      "Train Epoch: 6 [992/1083 (91%)]\tLoss: 0.044126 \tOP: 0.793651\tOR: 0.862069\tOF1: 0.826446\n",
      "Train Epoch: 6 [1024/1083 (94%)]\tLoss: 0.052173 \tOP: 0.796875\tOR: 0.772727\tOF1: 0.784615\n",
      "Train Epoch: 6 [891/1083 (97%)]\tLoss: 0.062056 \tOP: 0.744681\tOR: 0.648148\tOF1: 0.693069\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2391 \n",
      "OP: 0.250000\n",
      "OR: 0.228571\n",
      "OF1: 0.238806\n",
      "\n",
      "Train Epoch: 7 [0/1083 (0%)]\tLoss: 0.038584 \tOP: 0.814286\tOR: 0.934426\tOF1: 0.870229\n",
      "Train Epoch: 7 [32/1083 (3%)]\tLoss: 0.044713 \tOP: 0.840000\tOR: 0.887324\tOF1: 0.863014\n",
      "Train Epoch: 7 [64/1083 (6%)]\tLoss: 0.049316 \tOP: 0.819672\tOR: 0.769231\tOF1: 0.793651\n",
      "Train Epoch: 7 [96/1083 (9%)]\tLoss: 0.065410 \tOP: 0.833333\tOR: 0.671642\tOF1: 0.743802\n",
      "Train Epoch: 7 [128/1083 (12%)]\tLoss: 0.047781 \tOP: 0.840000\tOR: 0.840000\tOF1: 0.840000\n",
      "Train Epoch: 7 [160/1083 (15%)]\tLoss: 0.043038 \tOP: 0.855072\tOR: 0.867647\tOF1: 0.861314\n",
      "Train Epoch: 7 [192/1083 (18%)]\tLoss: 0.046925 \tOP: 0.838235\tOR: 0.850746\tOF1: 0.844444\n",
      "Train Epoch: 7 [224/1083 (21%)]\tLoss: 0.052192 \tOP: 0.838235\tOR: 0.780822\tOF1: 0.808511\n",
      "Train Epoch: 7 [256/1083 (24%)]\tLoss: 0.052769 \tOP: 0.844828\tOR: 0.790323\tOF1: 0.816667\n",
      "Train Epoch: 7 [288/1083 (26%)]\tLoss: 0.048123 \tOP: 0.838710\tOR: 0.800000\tOF1: 0.818898\n",
      "Train Epoch: 7 [320/1083 (29%)]\tLoss: 0.044782 \tOP: 0.826087\tOR: 0.826087\tOF1: 0.826087\n",
      "Train Epoch: 7 [352/1083 (32%)]\tLoss: 0.046685 \tOP: 0.838710\tOR: 0.812500\tOF1: 0.825397\n",
      "Train Epoch: 7 [384/1083 (35%)]\tLoss: 0.038773 \tOP: 0.842857\tOR: 0.907692\tOF1: 0.874074\n",
      "Train Epoch: 7 [416/1083 (38%)]\tLoss: 0.042747 \tOP: 0.830986\tOR: 0.880597\tOF1: 0.855072\n",
      "Train Epoch: 7 [448/1083 (41%)]\tLoss: 0.044607 \tOP: 0.823529\tOR: 0.861538\tOF1: 0.842105\n",
      "Train Epoch: 7 [480/1083 (44%)]\tLoss: 0.047713 \tOP: 0.825397\tOR: 0.764706\tOF1: 0.793893\n",
      "Train Epoch: 7 [512/1083 (47%)]\tLoss: 0.049342 \tOP: 0.840580\tOR: 0.794521\tOF1: 0.816901\n",
      "Train Epoch: 7 [544/1083 (50%)]\tLoss: 0.036610 \tOP: 0.876712\tOR: 0.941176\tOF1: 0.907801\n",
      "Train Epoch: 7 [576/1083 (53%)]\tLoss: 0.044842 \tOP: 0.866667\tOR: 0.878378\tOF1: 0.872483\n",
      "Train Epoch: 7 [608/1083 (56%)]\tLoss: 0.041568 \tOP: 0.842857\tOR: 0.907692\tOF1: 0.874074\n",
      "Train Epoch: 7 [640/1083 (59%)]\tLoss: 0.044950 \tOP: 0.840580\tOR: 0.852941\tOF1: 0.846715\n",
      "Train Epoch: 7 [672/1083 (62%)]\tLoss: 0.042871 \tOP: 0.815385\tOR: 0.803030\tOF1: 0.809160\n",
      "Train Epoch: 7 [704/1083 (65%)]\tLoss: 0.048600 \tOP: 0.848485\tOR: 0.800000\tOF1: 0.823529\n",
      "Train Epoch: 7 [736/1083 (68%)]\tLoss: 0.039781 \tOP: 0.815789\tOR: 0.911765\tOF1: 0.861111\n",
      "Train Epoch: 7 [768/1083 (71%)]\tLoss: 0.046994 \tOP: 0.846154\tOR: 0.785714\tOF1: 0.814815\n",
      "Train Epoch: 7 [800/1083 (74%)]\tLoss: 0.045123 \tOP: 0.855072\tOR: 0.842857\tOF1: 0.848921\n",
      "Train Epoch: 7 [832/1083 (76%)]\tLoss: 0.043061 \tOP: 0.805556\tOR: 0.828571\tOF1: 0.816901\n",
      "Train Epoch: 7 [864/1083 (79%)]\tLoss: 0.044383 \tOP: 0.848485\tOR: 0.835821\tOF1: 0.842105\n",
      "Train Epoch: 7 [896/1083 (82%)]\tLoss: 0.041159 \tOP: 0.838235\tOR: 0.838235\tOF1: 0.838235\n",
      "Train Epoch: 7 [928/1083 (85%)]\tLoss: 0.039251 \tOP: 0.876923\tOR: 0.876923\tOF1: 0.876923\n",
      "Train Epoch: 7 [960/1083 (88%)]\tLoss: 0.040932 \tOP: 0.840580\tOR: 0.865672\tOF1: 0.852941\n",
      "Train Epoch: 7 [992/1083 (91%)]\tLoss: 0.049000 \tOP: 0.859375\tOR: 0.785714\tOF1: 0.820896\n",
      "Train Epoch: 7 [1024/1083 (94%)]\tLoss: 0.048739 \tOP: 0.830986\tOR: 0.786667\tOF1: 0.808219\n",
      "Train Epoch: 7 [891/1083 (97%)]\tLoss: 0.050526 \tOP: 0.813559\tOR: 0.774194\tOF1: 0.793388\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2411 \n",
      "OP: 0.218750\n",
      "OR: 0.200000\n",
      "OF1: 0.208955\n",
      "\n",
      "Train Epoch: 8 [0/1083 (0%)]\tLoss: 0.037616 \tOP: 0.854839\tOR: 0.828125\tOF1: 0.841270\n",
      "Train Epoch: 8 [32/1083 (3%)]\tLoss: 0.045599 \tOP: 0.852941\tOR: 0.828571\tOF1: 0.840580\n",
      "Train Epoch: 8 [64/1083 (6%)]\tLoss: 0.033285 \tOP: 0.842857\tOR: 0.907692\tOF1: 0.874074\n",
      "Train Epoch: 8 [96/1083 (9%)]\tLoss: 0.042026 \tOP: 0.880000\tOR: 0.880000\tOF1: 0.880000\n",
      "Train Epoch: 8 [128/1083 (12%)]\tLoss: 0.040318 \tOP: 0.888889\tOR: 0.914286\tOF1: 0.901408\n",
      "Train Epoch: 8 [160/1083 (15%)]\tLoss: 0.036913 \tOP: 0.871795\tOR: 0.944444\tOF1: 0.906667\n",
      "Train Epoch: 8 [192/1083 (18%)]\tLoss: 0.041085 \tOP: 0.881579\tOR: 0.893333\tOF1: 0.887417\n",
      "Train Epoch: 8 [224/1083 (21%)]\tLoss: 0.037405 \tOP: 0.850746\tOR: 0.890625\tOF1: 0.870229\n",
      "Train Epoch: 8 [256/1083 (24%)]\tLoss: 0.039671 \tOP: 0.866667\tOR: 0.825397\tOF1: 0.845528\n",
      "Train Epoch: 8 [288/1083 (26%)]\tLoss: 0.041470 \tOP: 0.850746\tOR: 0.863636\tOF1: 0.857143\n",
      "Train Epoch: 8 [320/1083 (29%)]\tLoss: 0.033033 \tOP: 0.835616\tOR: 0.924242\tOF1: 0.877698\n",
      "Train Epoch: 8 [352/1083 (32%)]\tLoss: 0.040786 \tOP: 0.833333\tOR: 0.845070\tOF1: 0.839161\n",
      "Train Epoch: 8 [384/1083 (35%)]\tLoss: 0.038116 \tOP: 0.882353\tOR: 0.895522\tOF1: 0.888889\n",
      "Train Epoch: 8 [416/1083 (38%)]\tLoss: 0.035843 \tOP: 0.915493\tOR: 0.942029\tOF1: 0.928571\n",
      "Train Epoch: 8 [448/1083 (41%)]\tLoss: 0.037990 \tOP: 0.828571\tOR: 0.906250\tOF1: 0.865672\n",
      "Train Epoch: 8 [480/1083 (44%)]\tLoss: 0.034924 \tOP: 0.876712\tOR: 0.914286\tOF1: 0.895105\n",
      "Train Epoch: 8 [512/1083 (47%)]\tLoss: 0.037117 \tOP: 0.867647\tOR: 0.893939\tOF1: 0.880597\n",
      "Train Epoch: 8 [544/1083 (50%)]\tLoss: 0.032870 \tOP: 0.837838\tOR: 0.939394\tOF1: 0.885714\n",
      "Train Epoch: 8 [576/1083 (53%)]\tLoss: 0.038776 \tOP: 0.892308\tOR: 0.878788\tOF1: 0.885496\n",
      "Train Epoch: 8 [608/1083 (56%)]\tLoss: 0.038758 \tOP: 0.871429\tOR: 0.897059\tOF1: 0.884058\n",
      "Train Epoch: 8 [640/1083 (59%)]\tLoss: 0.043790 \tOP: 0.861538\tOR: 0.811594\tOF1: 0.835821\n",
      "Train Epoch: 8 [672/1083 (62%)]\tLoss: 0.039511 \tOP: 0.855072\tOR: 0.855072\tOF1: 0.855072\n",
      "Train Epoch: 8 [704/1083 (65%)]\tLoss: 0.043749 \tOP: 0.869565\tOR: 0.845070\tOF1: 0.857143\n",
      "Train Epoch: 8 [736/1083 (68%)]\tLoss: 0.037669 \tOP: 0.871429\tOR: 0.910448\tOF1: 0.890511\n",
      "Train Epoch: 8 [768/1083 (71%)]\tLoss: 0.039136 \tOP: 0.855072\tOR: 0.880597\tOF1: 0.867647\n",
      "Train Epoch: 8 [800/1083 (74%)]\tLoss: 0.034044 \tOP: 0.891892\tOR: 0.942857\tOF1: 0.916667\n",
      "Train Epoch: 8 [832/1083 (76%)]\tLoss: 0.032757 \tOP: 0.902778\tOR: 0.942029\tOF1: 0.921986\n",
      "Train Epoch: 8 [864/1083 (79%)]\tLoss: 0.038988 \tOP: 0.835821\tOR: 0.835821\tOF1: 0.835821\n",
      "Train Epoch: 8 [896/1083 (82%)]\tLoss: 0.038855 \tOP: 0.878378\tOR: 0.878378\tOF1: 0.878378\n",
      "Train Epoch: 8 [928/1083 (85%)]\tLoss: 0.036974 \tOP: 0.863636\tOR: 0.904762\tOF1: 0.883721\n",
      "Train Epoch: 8 [960/1083 (88%)]\tLoss: 0.036606 \tOP: 0.849315\tOR: 0.873239\tOF1: 0.861111\n",
      "Train Epoch: 8 [992/1083 (91%)]\tLoss: 0.036725 \tOP: 0.869565\tOR: 0.882353\tOF1: 0.875912\n",
      "Train Epoch: 8 [1024/1083 (94%)]\tLoss: 0.035476 \tOP: 0.850746\tOR: 0.863636\tOF1: 0.857143\n",
      "Train Epoch: 8 [891/1083 (97%)]\tLoss: 0.040774 \tOP: 0.828125\tOR: 0.883333\tOF1: 0.854839\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2444 \n",
      "OP: 0.225806\n",
      "OR: 0.200000\n",
      "OF1: 0.212121\n",
      "\n",
      "Train Epoch: 9 [0/1083 (0%)]\tLoss: 0.036764 \tOP: 0.910448\tOR: 0.897059\tOF1: 0.903704\n",
      "Train Epoch: 9 [32/1083 (3%)]\tLoss: 0.034969 \tOP: 0.871429\tOR: 0.910448\tOF1: 0.890511\n",
      "Train Epoch: 9 [64/1083 (6%)]\tLoss: 0.028639 \tOP: 0.881579\tOR: 0.957143\tOF1: 0.917808\n",
      "Train Epoch: 9 [96/1083 (9%)]\tLoss: 0.031370 \tOP: 0.867647\tOR: 0.936508\tOF1: 0.900763\n",
      "Train Epoch: 9 [128/1083 (12%)]\tLoss: 0.039058 \tOP: 0.902778\tOR: 0.890411\tOF1: 0.896552\n",
      "Train Epoch: 9 [160/1083 (15%)]\tLoss: 0.034097 \tOP: 0.884058\tOR: 0.910448\tOF1: 0.897059\n",
      "Train Epoch: 9 [192/1083 (18%)]\tLoss: 0.026125 \tOP: 0.893333\tOR: 0.985294\tOF1: 0.937063\n",
      "Train Epoch: 9 [224/1083 (21%)]\tLoss: 0.029832 \tOP: 0.852941\tOR: 0.935484\tOF1: 0.892308\n",
      "Train Epoch: 9 [256/1083 (24%)]\tLoss: 0.032943 \tOP: 0.900000\tOR: 0.940299\tOF1: 0.919708\n",
      "Train Epoch: 9 [288/1083 (26%)]\tLoss: 0.027571 \tOP: 0.880000\tOR: 0.985075\tOF1: 0.929577\n",
      "Train Epoch: 9 [320/1083 (29%)]\tLoss: 0.031710 \tOP: 0.913043\tOR: 0.954545\tOF1: 0.933333\n",
      "Train Epoch: 9 [352/1083 (32%)]\tLoss: 0.037064 \tOP: 0.915493\tOR: 0.890411\tOF1: 0.902778\n",
      "Train Epoch: 9 [384/1083 (35%)]\tLoss: 0.029533 \tOP: 0.861538\tOR: 0.903226\tOF1: 0.881890\n",
      "Train Epoch: 9 [416/1083 (38%)]\tLoss: 0.028270 \tOP: 0.890411\tOR: 0.984848\tOF1: 0.935252\n",
      "Train Epoch: 9 [448/1083 (41%)]\tLoss: 0.031547 \tOP: 0.900000\tOR: 0.972973\tOF1: 0.935065\n",
      "Train Epoch: 9 [480/1083 (44%)]\tLoss: 0.029378 \tOP: 0.916667\tOR: 0.970588\tOF1: 0.942857\n",
      "Train Epoch: 9 [512/1083 (47%)]\tLoss: 0.029911 \tOP: 0.861538\tOR: 0.888889\tOF1: 0.875000\n",
      "Train Epoch: 9 [544/1083 (50%)]\tLoss: 0.026960 \tOP: 0.861111\tOR: 0.953846\tOF1: 0.905109\n",
      "Train Epoch: 9 [576/1083 (53%)]\tLoss: 0.030793 \tOP: 0.878378\tOR: 0.928571\tOF1: 0.902778\n",
      "Train Epoch: 9 [608/1083 (56%)]\tLoss: 0.032946 \tOP: 0.910256\tOR: 0.934211\tOF1: 0.922078\n",
      "Train Epoch: 9 [640/1083 (59%)]\tLoss: 0.034756 \tOP: 0.867647\tOR: 0.907692\tOF1: 0.887218\n",
      "Train Epoch: 9 [672/1083 (62%)]\tLoss: 0.030175 \tOP: 0.864865\tOR: 0.941176\tOF1: 0.901408\n",
      "Train Epoch: 9 [704/1083 (65%)]\tLoss: 0.034109 \tOP: 0.920000\tOR: 0.920000\tOF1: 0.920000\n",
      "Train Epoch: 9 [736/1083 (68%)]\tLoss: 0.035740 \tOP: 0.881579\tOR: 0.881579\tOF1: 0.881579\n",
      "Train Epoch: 9 [768/1083 (71%)]\tLoss: 0.035879 \tOP: 0.918919\tOR: 0.944444\tOF1: 0.931507\n",
      "Train Epoch: 9 [800/1083 (74%)]\tLoss: 0.037418 \tOP: 0.892308\tOR: 0.906250\tOF1: 0.899225\n",
      "Train Epoch: 9 [832/1083 (76%)]\tLoss: 0.033294 \tOP: 0.887324\tOR: 0.887324\tOF1: 0.887324\n",
      "Train Epoch: 9 [864/1083 (79%)]\tLoss: 0.028451 \tOP: 0.871429\tOR: 0.953125\tOF1: 0.910448\n",
      "Train Epoch: 9 [896/1083 (82%)]\tLoss: 0.028338 \tOP: 0.884058\tOR: 0.910448\tOF1: 0.897059\n",
      "Train Epoch: 9 [928/1083 (85%)]\tLoss: 0.030395 \tOP: 0.927536\tOR: 0.955224\tOF1: 0.941176\n",
      "Train Epoch: 9 [960/1083 (88%)]\tLoss: 0.046722 \tOP: 0.859375\tOR: 0.723684\tOF1: 0.785714\n",
      "Train Epoch: 9 [992/1083 (91%)]\tLoss: 0.028275 \tOP: 0.885714\tOR: 0.953846\tOF1: 0.918519\n",
      "Train Epoch: 9 [1024/1083 (94%)]\tLoss: 0.030922 \tOP: 0.869565\tOR: 0.895522\tOF1: 0.882353\n",
      "Train Epoch: 9 [891/1083 (97%)]\tLoss: 0.031736 \tOP: 0.881356\tOR: 0.928571\tOF1: 0.904348\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2468 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n",
      "Train Epoch: 10 [0/1083 (0%)]\tLoss: 0.025685 \tOP: 0.905405\tOR: 0.985294\tOF1: 0.943662\n",
      "Train Epoch: 10 [32/1083 (3%)]\tLoss: 0.027197 \tOP: 0.892308\tOR: 0.935484\tOF1: 0.913386\n",
      "Train Epoch: 10 [64/1083 (6%)]\tLoss: 0.026989 \tOP: 0.853333\tOR: 0.969697\tOF1: 0.907801\n",
      "Train Epoch: 10 [96/1083 (9%)]\tLoss: 0.025924 \tOP: 0.904762\tOR: 0.934426\tOF1: 0.919355\n",
      "Train Epoch: 10 [128/1083 (12%)]\tLoss: 0.028793 \tOP: 0.905405\tOR: 0.957143\tOF1: 0.930556\n",
      "Train Epoch: 10 [160/1083 (15%)]\tLoss: 0.022884 \tOP: 0.847222\tOR: 1.000000\tOF1: 0.917293\n",
      "Train Epoch: 10 [192/1083 (18%)]\tLoss: 0.026331 \tOP: 0.896104\tOR: 0.985714\tOF1: 0.938776\n",
      "Train Epoch: 10 [224/1083 (21%)]\tLoss: 0.028240 \tOP: 0.904110\tOR: 0.956522\tOF1: 0.929577\n",
      "Train Epoch: 10 [256/1083 (24%)]\tLoss: 0.025980 \tOP: 0.894737\tOR: 0.971429\tOF1: 0.931507\n",
      "Train Epoch: 10 [288/1083 (26%)]\tLoss: 0.021137 \tOP: 0.890411\tOR: 1.000000\tOF1: 0.942029\n",
      "Train Epoch: 10 [320/1083 (29%)]\tLoss: 0.026645 \tOP: 0.944444\tOR: 0.971429\tOF1: 0.957746\n",
      "Train Epoch: 10 [352/1083 (32%)]\tLoss: 0.029909 \tOP: 0.904110\tOR: 0.916667\tOF1: 0.910345\n",
      "Train Epoch: 10 [384/1083 (35%)]\tLoss: 0.029566 \tOP: 0.884058\tOR: 0.884058\tOF1: 0.884058\n",
      "Train Epoch: 10 [416/1083 (38%)]\tLoss: 0.021700 \tOP: 0.887324\tOR: 0.969231\tOF1: 0.926471\n",
      "Train Epoch: 10 [448/1083 (41%)]\tLoss: 0.029707 \tOP: 0.859155\tOR: 0.910448\tOF1: 0.884058\n",
      "Train Epoch: 10 [480/1083 (44%)]\tLoss: 0.026892 \tOP: 0.878378\tOR: 0.970149\tOF1: 0.921986\n",
      "Train Epoch: 10 [512/1083 (47%)]\tLoss: 0.031745 \tOP: 0.937500\tOR: 0.909091\tOF1: 0.923077\n",
      "Train Epoch: 10 [544/1083 (50%)]\tLoss: 0.022810 \tOP: 0.866667\tOR: 0.984848\tOF1: 0.921986\n",
      "Train Epoch: 10 [576/1083 (53%)]\tLoss: 0.024863 \tOP: 0.880597\tOR: 0.921875\tOF1: 0.900763\n",
      "Train Epoch: 10 [608/1083 (56%)]\tLoss: 0.022330 \tOP: 0.905405\tOR: 0.985294\tOF1: 0.943662\n",
      "Train Epoch: 10 [640/1083 (59%)]\tLoss: 0.032493 \tOP: 0.890411\tOR: 0.855263\tOF1: 0.872483\n",
      "Train Epoch: 10 [672/1083 (62%)]\tLoss: 0.026985 \tOP: 0.884615\tOR: 0.945205\tOF1: 0.913907\n",
      "Train Epoch: 10 [704/1083 (65%)]\tLoss: 0.027416 \tOP: 0.894737\tOR: 0.918919\tOF1: 0.906667\n",
      "Train Epoch: 10 [736/1083 (68%)]\tLoss: 0.028954 \tOP: 0.898551\tOR: 0.939394\tOF1: 0.918519\n",
      "Train Epoch: 10 [768/1083 (71%)]\tLoss: 0.034651 \tOP: 0.896104\tOR: 0.920000\tOF1: 0.907895\n",
      "Train Epoch: 10 [800/1083 (74%)]\tLoss: 0.023629 \tOP: 0.876923\tOR: 0.934426\tOF1: 0.904762\n",
      "Train Epoch: 10 [832/1083 (76%)]\tLoss: 0.036231 \tOP: 0.915493\tOR: 0.902778\tOF1: 0.909091\n",
      "Train Epoch: 10 [864/1083 (79%)]\tLoss: 0.030374 \tOP: 0.910448\tOR: 0.884058\tOF1: 0.897059\n",
      "Train Epoch: 10 [896/1083 (82%)]\tLoss: 0.024469 \tOP: 0.928571\tOR: 0.970149\tOF1: 0.948905\n",
      "Train Epoch: 10 [928/1083 (85%)]\tLoss: 0.022873 \tOP: 0.927536\tOR: 0.984615\tOF1: 0.955224\n",
      "Train Epoch: 10 [960/1083 (88%)]\tLoss: 0.024830 \tOP: 0.900000\tOR: 0.984375\tOF1: 0.940299\n",
      "Train Epoch: 10 [992/1083 (91%)]\tLoss: 0.035749 \tOP: 0.944444\tOR: 0.871795\tOF1: 0.906667\n",
      "Train Epoch: 10 [1024/1083 (94%)]\tLoss: 0.027621 \tOP: 0.922078\tOR: 0.934211\tOF1: 0.928105\n",
      "Train Epoch: 10 [891/1083 (97%)]\tLoss: 0.026402 \tOP: 0.866667\tOR: 0.928571\tOF1: 0.896552\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2518 \n",
      "OP: 0.187500\n",
      "OR: 0.171429\n",
      "OF1: 0.179104\n",
      "\n",
      "Train Epoch: 11 [0/1083 (0%)]\tLoss: 0.022576 \tOP: 0.906667\tOR: 0.957746\tOF1: 0.931507\n",
      "Train Epoch: 11 [32/1083 (3%)]\tLoss: 0.022676 \tOP: 0.957143\tOR: 0.985294\tOF1: 0.971014\n",
      "Train Epoch: 11 [64/1083 (6%)]\tLoss: 0.025855 \tOP: 0.915493\tOR: 0.942029\tOF1: 0.928571\n",
      "Train Epoch: 11 [96/1083 (9%)]\tLoss: 0.030741 \tOP: 0.884058\tOR: 0.924242\tOF1: 0.903704\n",
      "Train Epoch: 11 [128/1083 (12%)]\tLoss: 0.022222 \tOP: 0.895522\tOR: 0.967742\tOF1: 0.930233\n",
      "Train Epoch: 11 [160/1083 (15%)]\tLoss: 0.023864 \tOP: 0.917808\tOR: 0.985294\tOF1: 0.950355\n",
      "Train Epoch: 11 [192/1083 (18%)]\tLoss: 0.021538 \tOP: 0.876712\tOR: 0.984615\tOF1: 0.927536\n",
      "Train Epoch: 11 [224/1083 (21%)]\tLoss: 0.023629 \tOP: 0.904110\tOR: 0.985075\tOF1: 0.942857\n",
      "Train Epoch: 11 [256/1083 (24%)]\tLoss: 0.023492 \tOP: 0.913043\tOR: 0.969231\tOF1: 0.940299\n",
      "Train Epoch: 11 [288/1083 (26%)]\tLoss: 0.027456 \tOP: 0.907895\tOR: 0.945205\tOF1: 0.926174\n",
      "Train Epoch: 11 [320/1083 (29%)]\tLoss: 0.024482 \tOP: 0.924051\tOR: 0.973333\tOF1: 0.948052\n",
      "Train Epoch: 11 [352/1083 (32%)]\tLoss: 0.024420 \tOP: 0.902778\tOR: 0.942029\tOF1: 0.921986\n",
      "Train Epoch: 11 [384/1083 (35%)]\tLoss: 0.024752 \tOP: 0.948718\tOR: 0.986667\tOF1: 0.967320\n",
      "Train Epoch: 11 [416/1083 (38%)]\tLoss: 0.023081 \tOP: 0.878788\tOR: 0.950820\tOF1: 0.913386\n",
      "Train Epoch: 11 [448/1083 (41%)]\tLoss: 0.025619 \tOP: 0.907895\tOR: 0.985714\tOF1: 0.945205\n",
      "Train Epoch: 11 [480/1083 (44%)]\tLoss: 0.022122 \tOP: 0.891892\tOR: 1.000000\tOF1: 0.942857\n",
      "Train Epoch: 11 [512/1083 (47%)]\tLoss: 0.028416 \tOP: 0.893939\tOR: 0.907692\tOF1: 0.900763\n",
      "Train Epoch: 11 [544/1083 (50%)]\tLoss: 0.024100 \tOP: 0.935065\tOR: 0.986301\tOF1: 0.960000\n",
      "Train Epoch: 11 [576/1083 (53%)]\tLoss: 0.023317 \tOP: 0.861538\tOR: 0.918033\tOF1: 0.888889\n",
      "Train Epoch: 11 [608/1083 (56%)]\tLoss: 0.023491 \tOP: 0.915493\tOR: 0.942029\tOF1: 0.928571\n",
      "Train Epoch: 11 [640/1083 (59%)]\tLoss: 0.021895 \tOP: 0.936709\tOR: 0.986667\tOF1: 0.961039\n",
      "Train Epoch: 11 [672/1083 (62%)]\tLoss: 0.020746 \tOP: 0.895522\tOR: 0.967742\tOF1: 0.930233\n",
      "Train Epoch: 11 [704/1083 (65%)]\tLoss: 0.025480 \tOP: 0.928571\tOR: 0.975000\tOF1: 0.951220\n",
      "Train Epoch: 11 [736/1083 (68%)]\tLoss: 0.018013 \tOP: 0.869565\tOR: 0.983607\tOF1: 0.923077\n",
      "Train Epoch: 11 [768/1083 (71%)]\tLoss: 0.022110 \tOP: 0.932432\tOR: 1.000000\tOF1: 0.965035\n",
      "Train Epoch: 11 [800/1083 (74%)]\tLoss: 0.023264 \tOP: 0.934211\tOR: 1.000000\tOF1: 0.965986\n",
      "Train Epoch: 11 [832/1083 (76%)]\tLoss: 0.019726 \tOP: 0.890411\tOR: 1.000000\tOF1: 0.942029\n",
      "Train Epoch: 11 [864/1083 (79%)]\tLoss: 0.019010 \tOP: 0.900000\tOR: 0.984375\tOF1: 0.940299\n",
      "Train Epoch: 11 [896/1083 (82%)]\tLoss: 0.023074 \tOP: 0.904110\tOR: 0.970588\tOF1: 0.936170\n",
      "Train Epoch: 11 [928/1083 (85%)]\tLoss: 0.024645 \tOP: 0.941176\tOR: 0.941176\tOF1: 0.941176\n",
      "Train Epoch: 11 [960/1083 (88%)]\tLoss: 0.023973 \tOP: 0.900000\tOR: 0.954545\tOF1: 0.926471\n",
      "Train Epoch: 11 [992/1083 (91%)]\tLoss: 0.020962 \tOP: 0.905405\tOR: 0.985294\tOF1: 0.943662\n",
      "Train Epoch: 11 [1024/1083 (94%)]\tLoss: 0.024735 \tOP: 0.933333\tOR: 0.958904\tOF1: 0.945946\n",
      "Train Epoch: 11 [891/1083 (97%)]\tLoss: 0.021627 \tOP: 0.906250\tOR: 0.966667\tOF1: 0.935484\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2534 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n",
      "Train Epoch: 12 [0/1083 (0%)]\tLoss: 0.024058 \tOP: 0.916667\tOR: 0.942857\tOF1: 0.929577\n",
      "Train Epoch: 12 [32/1083 (3%)]\tLoss: 0.019711 \tOP: 0.891892\tOR: 0.985075\tOF1: 0.936170\n",
      "Train Epoch: 12 [64/1083 (6%)]\tLoss: 0.023529 \tOP: 0.931507\tOR: 0.971429\tOF1: 0.951049\n",
      "Train Epoch: 12 [96/1083 (9%)]\tLoss: 0.018631 \tOP: 0.884058\tOR: 1.000000\tOF1: 0.938462\n",
      "Train Epoch: 12 [128/1083 (12%)]\tLoss: 0.021216 \tOP: 0.914286\tOR: 0.984615\tOF1: 0.948148\n",
      "Train Epoch: 12 [160/1083 (15%)]\tLoss: 0.020179 \tOP: 0.916667\tOR: 1.000000\tOF1: 0.956522\n",
      "Train Epoch: 12 [192/1083 (18%)]\tLoss: 0.020677 \tOP: 0.912500\tOR: 0.986486\tOF1: 0.948052\n",
      "Train Epoch: 12 [224/1083 (21%)]\tLoss: 0.017168 \tOP: 0.900000\tOR: 1.000000\tOF1: 0.947368\n",
      "Train Epoch: 12 [256/1083 (24%)]\tLoss: 0.018670 \tOP: 0.921053\tOR: 0.985915\tOF1: 0.952381\n",
      "Train Epoch: 12 [288/1083 (26%)]\tLoss: 0.021337 \tOP: 0.932432\tOR: 0.985714\tOF1: 0.958333\n",
      "Train Epoch: 12 [320/1083 (29%)]\tLoss: 0.017750 \tOP: 0.904110\tOR: 1.000000\tOF1: 0.949640\n",
      "Train Epoch: 12 [352/1083 (32%)]\tLoss: 0.023330 \tOP: 0.897436\tOR: 0.958904\tOF1: 0.927152\n",
      "Train Epoch: 12 [384/1083 (35%)]\tLoss: 0.020616 \tOP: 0.933333\tOR: 0.985915\tOF1: 0.958904\n",
      "Train Epoch: 12 [416/1083 (38%)]\tLoss: 0.018679 \tOP: 0.909091\tOR: 1.000000\tOF1: 0.952381\n",
      "Train Epoch: 12 [448/1083 (41%)]\tLoss: 0.019876 \tOP: 0.898551\tOR: 0.939394\tOF1: 0.918519\n",
      "Train Epoch: 12 [480/1083 (44%)]\tLoss: 0.020281 \tOP: 0.904110\tOR: 0.956522\tOF1: 0.929577\n",
      "Train Epoch: 12 [512/1083 (47%)]\tLoss: 0.017809 \tOP: 0.915493\tOR: 1.000000\tOF1: 0.955882\n",
      "Train Epoch: 12 [544/1083 (50%)]\tLoss: 0.017151 \tOP: 0.884615\tOR: 1.000000\tOF1: 0.938776\n",
      "Train Epoch: 12 [576/1083 (53%)]\tLoss: 0.022285 \tOP: 0.896104\tOR: 0.945205\tOF1: 0.920000\n",
      "Train Epoch: 12 [608/1083 (56%)]\tLoss: 0.021694 \tOP: 0.941176\tOR: 0.969697\tOF1: 0.955224\n",
      "Train Epoch: 12 [640/1083 (59%)]\tLoss: 0.019229 \tOP: 0.901408\tOR: 0.984615\tOF1: 0.941176\n",
      "Train Epoch: 12 [672/1083 (62%)]\tLoss: 0.021028 \tOP: 0.939024\tOR: 0.987179\tOF1: 0.962500\n",
      "Train Epoch: 12 [704/1083 (65%)]\tLoss: 0.022825 \tOP: 0.918919\tOR: 0.944444\tOF1: 0.931507\n",
      "Train Epoch: 12 [736/1083 (68%)]\tLoss: 0.028517 \tOP: 0.893939\tOR: 0.893939\tOF1: 0.893939\n",
      "Train Epoch: 12 [768/1083 (71%)]\tLoss: 0.020562 \tOP: 0.921053\tOR: 0.985915\tOF1: 0.952381\n",
      "Train Epoch: 12 [800/1083 (74%)]\tLoss: 0.020470 \tOP: 0.878378\tOR: 0.970149\tOF1: 0.921986\n",
      "Train Epoch: 12 [832/1083 (76%)]\tLoss: 0.023894 \tOP: 0.940299\tOR: 0.940299\tOF1: 0.940299\n",
      "Train Epoch: 12 [864/1083 (79%)]\tLoss: 0.019329 \tOP: 0.924242\tOR: 0.968254\tOF1: 0.945736\n",
      "Train Epoch: 12 [896/1083 (82%)]\tLoss: 0.018219 \tOP: 0.922078\tOR: 1.000000\tOF1: 0.959459\n",
      "Train Epoch: 12 [928/1083 (85%)]\tLoss: 0.020152 \tOP: 0.928571\tOR: 0.970149\tOF1: 0.948905\n",
      "Train Epoch: 12 [960/1083 (88%)]\tLoss: 0.018203 \tOP: 0.902778\tOR: 0.970149\tOF1: 0.935252\n",
      "Train Epoch: 12 [992/1083 (91%)]\tLoss: 0.018817 \tOP: 0.923077\tOR: 1.000000\tOF1: 0.960000\n",
      "Train Epoch: 12 [1024/1083 (94%)]\tLoss: 0.022129 \tOP: 0.906667\tOR: 0.971429\tOF1: 0.937931\n",
      "Train Epoch: 12 [891/1083 (97%)]\tLoss: 0.023141 \tOP: 0.881356\tOR: 0.912281\tOF1: 0.896552\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2569 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n",
      "Train Epoch: 13 [0/1083 (0%)]\tLoss: 0.016874 \tOP: 0.891892\tOR: 0.985075\tOF1: 0.936170\n",
      "Train Epoch: 13 [32/1083 (3%)]\tLoss: 0.023929 \tOP: 0.917808\tOR: 0.917808\tOF1: 0.917808\n",
      "Train Epoch: 13 [64/1083 (6%)]\tLoss: 0.019002 \tOP: 0.917808\tOR: 0.971014\tOF1: 0.943662\n",
      "Train Epoch: 13 [96/1083 (9%)]\tLoss: 0.014394 \tOP: 0.898551\tOR: 1.000000\tOF1: 0.946565\n",
      "Train Epoch: 13 [128/1083 (12%)]\tLoss: 0.018139 \tOP: 0.913043\tOR: 0.984375\tOF1: 0.947368\n",
      "Train Epoch: 13 [160/1083 (15%)]\tLoss: 0.022038 \tOP: 0.938272\tOR: 0.974359\tOF1: 0.955975\n",
      "Train Epoch: 13 [192/1083 (18%)]\tLoss: 0.015081 \tOP: 0.884058\tOR: 0.983871\tOF1: 0.931298\n",
      "Train Epoch: 13 [224/1083 (21%)]\tLoss: 0.017658 \tOP: 0.935484\tOR: 0.983051\tOF1: 0.958678\n",
      "Train Epoch: 13 [256/1083 (24%)]\tLoss: 0.016407 \tOP: 0.911765\tOR: 1.000000\tOF1: 0.953846\n",
      "Train Epoch: 13 [288/1083 (26%)]\tLoss: 0.018364 \tOP: 0.916667\tOR: 1.000000\tOF1: 0.956522\n",
      "Train Epoch: 13 [320/1083 (29%)]\tLoss: 0.025620 \tOP: 0.906667\tOR: 0.971429\tOF1: 0.937931\n",
      "Train Epoch: 13 [352/1083 (32%)]\tLoss: 0.016227 \tOP: 0.873239\tOR: 1.000000\tOF1: 0.932331\n",
      "Train Epoch: 13 [384/1083 (35%)]\tLoss: 0.018259 \tOP: 0.924242\tOR: 0.953125\tOF1: 0.938462\n",
      "Train Epoch: 13 [416/1083 (38%)]\tLoss: 0.023141 \tOP: 0.928571\tOR: 0.902778\tOF1: 0.915493\n",
      "Train Epoch: 13 [448/1083 (41%)]\tLoss: 0.016665 \tOP: 0.916667\tOR: 0.985075\tOF1: 0.949640\n",
      "Train Epoch: 13 [480/1083 (44%)]\tLoss: 0.019006 \tOP: 0.920000\tOR: 0.971831\tOF1: 0.945205\n",
      "Train Epoch: 13 [512/1083 (47%)]\tLoss: 0.018727 \tOP: 0.898551\tOR: 0.953846\tOF1: 0.925373\n",
      "Train Epoch: 13 [544/1083 (50%)]\tLoss: 0.018097 \tOP: 0.917808\tOR: 0.985294\tOF1: 0.950355\n",
      "Train Epoch: 13 [576/1083 (53%)]\tLoss: 0.016491 \tOP: 0.907895\tOR: 1.000000\tOF1: 0.951724\n",
      "Train Epoch: 13 [608/1083 (56%)]\tLoss: 0.020052 \tOP: 0.943662\tOR: 1.000000\tOF1: 0.971014\n",
      "Train Epoch: 13 [640/1083 (59%)]\tLoss: 0.015625 \tOP: 0.870130\tOR: 1.000000\tOF1: 0.930556\n",
      "Train Epoch: 13 [672/1083 (62%)]\tLoss: 0.017458 \tOP: 0.915663\tOR: 1.000000\tOF1: 0.955975\n",
      "Train Epoch: 13 [704/1083 (65%)]\tLoss: 0.023488 \tOP: 0.931507\tOR: 0.971429\tOF1: 0.951049\n",
      "Train Epoch: 13 [736/1083 (68%)]\tLoss: 0.016464 \tOP: 0.886076\tOR: 0.985915\tOF1: 0.933333\n",
      "Train Epoch: 13 [768/1083 (71%)]\tLoss: 0.019366 \tOP: 0.929577\tOR: 0.956522\tOF1: 0.942857\n",
      "Train Epoch: 13 [800/1083 (74%)]\tLoss: 0.018983 \tOP: 0.876712\tOR: 0.984615\tOF1: 0.927536\n",
      "Train Epoch: 13 [832/1083 (76%)]\tLoss: 0.016003 \tOP: 0.945946\tOR: 1.000000\tOF1: 0.972222\n",
      "Train Epoch: 13 [864/1083 (79%)]\tLoss: 0.018864 \tOP: 0.912500\tOR: 1.000000\tOF1: 0.954248\n",
      "Train Epoch: 13 [896/1083 (82%)]\tLoss: 0.017340 \tOP: 0.918919\tOR: 0.971429\tOF1: 0.944444\n",
      "Train Epoch: 13 [928/1083 (85%)]\tLoss: 0.019895 \tOP: 0.911392\tOR: 0.960000\tOF1: 0.935065\n",
      "Train Epoch: 13 [960/1083 (88%)]\tLoss: 0.016481 \tOP: 0.906667\tOR: 1.000000\tOF1: 0.951049\n",
      "Train Epoch: 13 [992/1083 (91%)]\tLoss: 0.018884 \tOP: 0.939024\tOR: 0.987179\tOF1: 0.962500\n",
      "Train Epoch: 13 [1024/1083 (94%)]\tLoss: 0.015067 \tOP: 0.888889\tOR: 0.984615\tOF1: 0.934307\n",
      "Train Epoch: 13 [891/1083 (97%)]\tLoss: 0.016416 \tOP: 0.854839\tOR: 0.981481\tOF1: 0.913793\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2632 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n",
      "Train Epoch: 14 [0/1083 (0%)]\tLoss: 0.015885 \tOP: 0.895522\tOR: 0.983607\tOF1: 0.937500\n",
      "Train Epoch: 14 [32/1083 (3%)]\tLoss: 0.018002 \tOP: 0.943662\tOR: 0.985294\tOF1: 0.964029\n",
      "Train Epoch: 14 [64/1083 (6%)]\tLoss: 0.015298 \tOP: 0.893333\tOR: 0.985294\tOF1: 0.937063\n",
      "Train Epoch: 14 [96/1083 (9%)]\tLoss: 0.015459 \tOP: 0.924051\tOR: 1.000000\tOF1: 0.960526\n",
      "Train Epoch: 14 [128/1083 (12%)]\tLoss: 0.017620 \tOP: 0.920000\tOR: 1.000000\tOF1: 0.958333\n",
      "Train Epoch: 14 [160/1083 (15%)]\tLoss: 0.017552 \tOP: 0.913580\tOR: 0.986667\tOF1: 0.948718\n",
      "Train Epoch: 14 [192/1083 (18%)]\tLoss: 0.016817 \tOP: 0.904110\tOR: 0.970588\tOF1: 0.936170\n",
      "Train Epoch: 14 [224/1083 (21%)]\tLoss: 0.017531 \tOP: 0.891892\tOR: 0.985075\tOF1: 0.936170\n",
      "Train Epoch: 14 [256/1083 (24%)]\tLoss: 0.018165 \tOP: 0.915493\tOR: 0.970149\tOF1: 0.942029\n",
      "Train Epoch: 14 [288/1083 (26%)]\tLoss: 0.018954 \tOP: 0.884058\tOR: 0.968254\tOF1: 0.924242\n",
      "Train Epoch: 14 [320/1083 (29%)]\tLoss: 0.015280 \tOP: 0.921053\tOR: 0.985915\tOF1: 0.952381\n",
      "Train Epoch: 14 [352/1083 (32%)]\tLoss: 0.017228 \tOP: 0.955882\tOR: 0.984848\tOF1: 0.970149\n",
      "Train Epoch: 14 [384/1083 (35%)]\tLoss: 0.015742 \tOP: 0.910256\tOR: 1.000000\tOF1: 0.953020\n",
      "Train Epoch: 14 [416/1083 (38%)]\tLoss: 0.017653 \tOP: 0.970149\tOR: 0.984848\tOF1: 0.977444\n",
      "Train Epoch: 14 [448/1083 (41%)]\tLoss: 0.014194 \tOP: 0.871429\tOR: 1.000000\tOF1: 0.931298\n",
      "Train Epoch: 14 [480/1083 (44%)]\tLoss: 0.019645 \tOP: 0.918919\tOR: 0.985507\tOF1: 0.951049\n",
      "Train Epoch: 14 [512/1083 (47%)]\tLoss: 0.015064 \tOP: 0.896104\tOR: 1.000000\tOF1: 0.945205\n",
      "Train Epoch: 14 [544/1083 (50%)]\tLoss: 0.021573 \tOP: 0.932432\tOR: 0.971831\tOF1: 0.951724\n",
      "Train Epoch: 14 [576/1083 (53%)]\tLoss: 0.012485 \tOP: 0.885714\tOR: 1.000000\tOF1: 0.939394\n",
      "Train Epoch: 14 [608/1083 (56%)]\tLoss: 0.013954 \tOP: 0.904110\tOR: 0.985075\tOF1: 0.942857\n",
      "Train Epoch: 14 [640/1083 (59%)]\tLoss: 0.015534 \tOP: 0.898551\tOR: 0.984127\tOF1: 0.939394\n",
      "Train Epoch: 14 [672/1083 (62%)]\tLoss: 0.020302 \tOP: 0.891892\tOR: 0.956522\tOF1: 0.923077\n",
      "Train Epoch: 14 [704/1083 (65%)]\tLoss: 0.013661 \tOP: 0.957143\tOR: 1.000000\tOF1: 0.978102\n",
      "Train Epoch: 14 [736/1083 (68%)]\tLoss: 0.014984 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 14 [768/1083 (71%)]\tLoss: 0.013763 \tOP: 0.935897\tOR: 1.000000\tOF1: 0.966887\n",
      "Train Epoch: 14 [800/1083 (74%)]\tLoss: 0.013769 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 14 [832/1083 (76%)]\tLoss: 0.013686 \tOP: 0.905405\tOR: 0.985294\tOF1: 0.943662\n",
      "Train Epoch: 14 [864/1083 (79%)]\tLoss: 0.012301 \tOP: 0.905405\tOR: 1.000000\tOF1: 0.950355\n",
      "Train Epoch: 14 [896/1083 (82%)]\tLoss: 0.016121 \tOP: 0.906667\tOR: 0.985507\tOF1: 0.944444\n",
      "Train Epoch: 14 [928/1083 (85%)]\tLoss: 0.017827 \tOP: 0.895522\tOR: 0.937500\tOF1: 0.916031\n",
      "Train Epoch: 14 [960/1083 (88%)]\tLoss: 0.017830 \tOP: 0.938272\tOR: 0.987013\tOF1: 0.962025\n",
      "Train Epoch: 14 [992/1083 (91%)]\tLoss: 0.016353 \tOP: 0.910256\tOR: 0.972603\tOF1: 0.940397\n",
      "Train Epoch: 14 [1024/1083 (94%)]\tLoss: 0.013744 \tOP: 0.904110\tOR: 0.985075\tOF1: 0.942857\n",
      "Train Epoch: 14 [891/1083 (97%)]\tLoss: 0.020590 \tOP: 0.888889\tOR: 1.000000\tOF1: 0.941176\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2629 \n",
      "OP: 0.242424\n",
      "OR: 0.228571\n",
      "OF1: 0.235294\n",
      "\n",
      "Train Epoch: 15 [0/1083 (0%)]\tLoss: 0.017842 \tOP: 0.960000\tOR: 1.000000\tOF1: 0.979592\n",
      "Train Epoch: 15 [32/1083 (3%)]\tLoss: 0.015421 \tOP: 0.895522\tOR: 0.967742\tOF1: 0.930233\n",
      "Train Epoch: 15 [64/1083 (6%)]\tLoss: 0.011276 \tOP: 0.894737\tOR: 1.000000\tOF1: 0.944444\n",
      "Train Epoch: 15 [96/1083 (9%)]\tLoss: 0.015863 \tOP: 0.913580\tOR: 1.000000\tOF1: 0.954839\n",
      "Train Epoch: 15 [128/1083 (12%)]\tLoss: 0.014710 \tOP: 0.915493\tOR: 1.000000\tOF1: 0.955882\n",
      "Train Epoch: 15 [160/1083 (15%)]\tLoss: 0.012005 \tOP: 0.901408\tOR: 1.000000\tOF1: 0.948148\n",
      "Train Epoch: 15 [192/1083 (18%)]\tLoss: 0.016669 \tOP: 0.947368\tOR: 0.960000\tOF1: 0.953642\n",
      "Train Epoch: 15 [224/1083 (21%)]\tLoss: 0.012512 \tOP: 0.865672\tOR: 1.000000\tOF1: 0.928000\n",
      "Train Epoch: 15 [256/1083 (24%)]\tLoss: 0.012699 \tOP: 0.897059\tOR: 1.000000\tOF1: 0.945736\n",
      "Train Epoch: 15 [288/1083 (26%)]\tLoss: 0.010907 \tOP: 0.864865\tOR: 0.984615\tOF1: 0.920863\n",
      "Train Epoch: 15 [320/1083 (29%)]\tLoss: 0.016968 \tOP: 0.920000\tOR: 0.971831\tOF1: 0.945205\n",
      "Train Epoch: 15 [352/1083 (32%)]\tLoss: 0.016053 \tOP: 0.932432\tOR: 0.985714\tOF1: 0.958333\n",
      "Train Epoch: 15 [384/1083 (35%)]\tLoss: 0.010877 \tOP: 0.875000\tOR: 1.000000\tOF1: 0.933333\n",
      "Train Epoch: 15 [416/1083 (38%)]\tLoss: 0.015618 \tOP: 0.902778\tOR: 0.984848\tOF1: 0.942029\n",
      "Train Epoch: 15 [448/1083 (41%)]\tLoss: 0.017316 \tOP: 0.888889\tOR: 0.955224\tOF1: 0.920863\n",
      "Train Epoch: 15 [480/1083 (44%)]\tLoss: 0.012667 \tOP: 0.940299\tOR: 0.984375\tOF1: 0.961832\n",
      "Train Epoch: 15 [512/1083 (47%)]\tLoss: 0.013125 \tOP: 0.888889\tOR: 0.984615\tOF1: 0.934307\n",
      "Train Epoch: 15 [544/1083 (50%)]\tLoss: 0.015816 \tOP: 0.923077\tOR: 0.986301\tOF1: 0.953642\n",
      "Train Epoch: 15 [576/1083 (53%)]\tLoss: 0.014737 \tOP: 0.893333\tOR: 0.985294\tOF1: 0.937063\n",
      "Train Epoch: 15 [608/1083 (56%)]\tLoss: 0.011996 \tOP: 0.890411\tOR: 1.000000\tOF1: 0.942029\n",
      "Train Epoch: 15 [640/1083 (59%)]\tLoss: 0.011470 \tOP: 0.928571\tOR: 1.000000\tOF1: 0.962963\n",
      "Train Epoch: 15 [672/1083 (62%)]\tLoss: 0.012682 \tOP: 0.900000\tOR: 1.000000\tOF1: 0.947368\n",
      "Train Epoch: 15 [704/1083 (65%)]\tLoss: 0.017008 \tOP: 0.958333\tOR: 0.985714\tOF1: 0.971831\n",
      "Train Epoch: 15 [736/1083 (68%)]\tLoss: 0.018041 \tOP: 0.939759\tOR: 1.000000\tOF1: 0.968944\n",
      "Train Epoch: 15 [768/1083 (71%)]\tLoss: 0.014649 \tOP: 0.905405\tOR: 0.971014\tOF1: 0.937063\n",
      "Train Epoch: 15 [800/1083 (74%)]\tLoss: 0.014974 \tOP: 0.923077\tOR: 0.986301\tOF1: 0.953642\n",
      "Train Epoch: 15 [832/1083 (76%)]\tLoss: 0.012246 \tOP: 0.913043\tOR: 1.000000\tOF1: 0.954545\n",
      "Train Epoch: 15 [864/1083 (79%)]\tLoss: 0.015322 \tOP: 0.935065\tOR: 1.000000\tOF1: 0.966443\n",
      "Train Epoch: 15 [896/1083 (82%)]\tLoss: 0.018763 \tOP: 0.920000\tOR: 0.971831\tOF1: 0.945205\n",
      "Train Epoch: 15 [928/1083 (85%)]\tLoss: 0.011834 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 15 [960/1083 (88%)]\tLoss: 0.017666 \tOP: 0.973684\tOR: 0.986667\tOF1: 0.980132\n",
      "Train Epoch: 15 [992/1083 (91%)]\tLoss: 0.021571 \tOP: 0.900000\tOR: 0.969231\tOF1: 0.933333\n",
      "Train Epoch: 15 [1024/1083 (94%)]\tLoss: 0.013281 \tOP: 0.909091\tOR: 1.000000\tOF1: 0.952381\n",
      "Train Epoch: 15 [891/1083 (97%)]\tLoss: 0.012891 \tOP: 0.925373\tOR: 1.000000\tOF1: 0.961240\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2714 \n",
      "OP: 0.264706\n",
      "OR: 0.257143\n",
      "OF1: 0.260870\n",
      "\n",
      "Train Epoch: 16 [0/1083 (0%)]\tLoss: 0.013753 \tOP: 0.911392\tOR: 1.000000\tOF1: 0.953642\n",
      "Train Epoch: 16 [32/1083 (3%)]\tLoss: 0.014710 \tOP: 0.923077\tOR: 1.000000\tOF1: 0.960000\n",
      "Train Epoch: 16 [64/1083 (6%)]\tLoss: 0.013159 \tOP: 0.913043\tOR: 0.969231\tOF1: 0.940299\n",
      "Train Epoch: 16 [96/1083 (9%)]\tLoss: 0.011798 \tOP: 0.907692\tOR: 1.000000\tOF1: 0.951613\n",
      "Train Epoch: 16 [128/1083 (12%)]\tLoss: 0.011092 \tOP: 0.916667\tOR: 0.985075\tOF1: 0.949640\n",
      "Train Epoch: 16 [160/1083 (15%)]\tLoss: 0.010964 \tOP: 0.902778\tOR: 1.000000\tOF1: 0.948905\n",
      "Train Epoch: 16 [192/1083 (18%)]\tLoss: 0.010662 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 16 [224/1083 (21%)]\tLoss: 0.012729 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 16 [256/1083 (24%)]\tLoss: 0.018311 \tOP: 0.906977\tOR: 0.987342\tOF1: 0.945455\n",
      "Train Epoch: 16 [288/1083 (26%)]\tLoss: 0.015308 \tOP: 0.961538\tOR: 0.974026\tOF1: 0.967742\n",
      "Train Epoch: 16 [320/1083 (29%)]\tLoss: 0.013828 \tOP: 0.933333\tOR: 1.000000\tOF1: 0.965517\n",
      "Train Epoch: 16 [352/1083 (32%)]\tLoss: 0.011717 \tOP: 0.890411\tOR: 0.984848\tOF1: 0.935252\n",
      "Train Epoch: 16 [384/1083 (35%)]\tLoss: 0.011592 \tOP: 0.940299\tOR: 1.000000\tOF1: 0.969231\n",
      "Train Epoch: 16 [416/1083 (38%)]\tLoss: 0.012567 \tOP: 0.893333\tOR: 1.000000\tOF1: 0.943662\n",
      "Train Epoch: 16 [448/1083 (41%)]\tLoss: 0.015039 \tOP: 0.944444\tOR: 0.985507\tOF1: 0.964539\n",
      "Train Epoch: 16 [480/1083 (44%)]\tLoss: 0.013771 \tOP: 0.959459\tOR: 1.000000\tOF1: 0.979310\n",
      "Train Epoch: 16 [512/1083 (47%)]\tLoss: 0.013556 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 16 [544/1083 (50%)]\tLoss: 0.012791 \tOP: 0.900000\tOR: 0.984375\tOF1: 0.940299\n",
      "Train Epoch: 16 [576/1083 (53%)]\tLoss: 0.012955 \tOP: 0.880952\tOR: 0.986667\tOF1: 0.930818\n",
      "Train Epoch: 16 [608/1083 (56%)]\tLoss: 0.012893 \tOP: 0.902778\tOR: 0.984848\tOF1: 0.942029\n",
      "Train Epoch: 16 [640/1083 (59%)]\tLoss: 0.011972 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 16 [672/1083 (62%)]\tLoss: 0.013077 \tOP: 0.958333\tOR: 1.000000\tOF1: 0.978723\n",
      "Train Epoch: 16 [704/1083 (65%)]\tLoss: 0.011665 \tOP: 0.907895\tOR: 0.985714\tOF1: 0.945205\n",
      "Train Epoch: 16 [736/1083 (68%)]\tLoss: 0.010945 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 16 [768/1083 (71%)]\tLoss: 0.012287 \tOP: 0.909091\tOR: 0.985915\tOF1: 0.945946\n",
      "Train Epoch: 16 [800/1083 (74%)]\tLoss: 0.011322 \tOP: 0.898551\tOR: 1.000000\tOF1: 0.946565\n",
      "Train Epoch: 16 [832/1083 (76%)]\tLoss: 0.012352 \tOP: 0.914286\tOR: 1.000000\tOF1: 0.955224\n",
      "Train Epoch: 16 [864/1083 (79%)]\tLoss: 0.013381 \tOP: 0.897436\tOR: 0.985915\tOF1: 0.939597\n",
      "Train Epoch: 16 [896/1083 (82%)]\tLoss: 0.012136 \tOP: 0.932432\tOR: 0.985714\tOF1: 0.958333\n",
      "Train Epoch: 16 [928/1083 (85%)]\tLoss: 0.013132 \tOP: 0.943662\tOR: 1.000000\tOF1: 0.971014\n",
      "Train Epoch: 16 [960/1083 (88%)]\tLoss: 0.015460 \tOP: 0.909091\tOR: 0.958904\tOF1: 0.933333\n",
      "Train Epoch: 16 [992/1083 (91%)]\tLoss: 0.011375 \tOP: 0.893333\tOR: 0.985294\tOF1: 0.937063\n",
      "Train Epoch: 16 [1024/1083 (94%)]\tLoss: 0.011503 \tOP: 0.876712\tOR: 1.000000\tOF1: 0.934307\n",
      "Train Epoch: 16 [891/1083 (97%)]\tLoss: 0.011724 \tOP: 0.879310\tOR: 1.000000\tOF1: 0.935780\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2666 \n",
      "OP: 0.272727\n",
      "OR: 0.257143\n",
      "OF1: 0.264706\n",
      "\n",
      "Train Epoch: 17 [0/1083 (0%)]\tLoss: 0.010451 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 17 [32/1083 (3%)]\tLoss: 0.010876 \tOP: 0.930556\tOR: 1.000000\tOF1: 0.964029\n",
      "Train Epoch: 17 [64/1083 (6%)]\tLoss: 0.012290 \tOP: 0.914286\tOR: 1.000000\tOF1: 0.955224\n",
      "Train Epoch: 17 [96/1083 (9%)]\tLoss: 0.012678 \tOP: 0.910256\tOR: 1.000000\tOF1: 0.953020\n",
      "Train Epoch: 17 [128/1083 (12%)]\tLoss: 0.011994 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 17 [160/1083 (15%)]\tLoss: 0.010217 \tOP: 0.866667\tOR: 1.000000\tOF1: 0.928571\n",
      "Train Epoch: 17 [192/1083 (18%)]\tLoss: 0.010530 \tOP: 0.925000\tOR: 1.000000\tOF1: 0.961039\n",
      "Train Epoch: 17 [224/1083 (21%)]\tLoss: 0.012164 \tOP: 0.918919\tOR: 0.985507\tOF1: 0.951049\n",
      "Train Epoch: 17 [256/1083 (24%)]\tLoss: 0.009771 \tOP: 0.905405\tOR: 1.000000\tOF1: 0.950355\n",
      "Train Epoch: 17 [288/1083 (26%)]\tLoss: 0.011809 \tOP: 0.893333\tOR: 1.000000\tOF1: 0.943662\n",
      "Train Epoch: 17 [320/1083 (29%)]\tLoss: 0.010639 \tOP: 0.906667\tOR: 1.000000\tOF1: 0.951049\n",
      "Train Epoch: 17 [352/1083 (32%)]\tLoss: 0.013910 \tOP: 0.905405\tOR: 1.000000\tOF1: 0.950355\n",
      "Train Epoch: 17 [384/1083 (35%)]\tLoss: 0.009971 \tOP: 0.913043\tOR: 1.000000\tOF1: 0.954545\n",
      "Train Epoch: 17 [416/1083 (38%)]\tLoss: 0.013450 \tOP: 0.937500\tOR: 1.000000\tOF1: 0.967742\n",
      "Train Epoch: 17 [448/1083 (41%)]\tLoss: 0.010784 \tOP: 0.929577\tOR: 1.000000\tOF1: 0.963504\n",
      "Train Epoch: 17 [480/1083 (44%)]\tLoss: 0.010984 \tOP: 0.948718\tOR: 1.000000\tOF1: 0.973684\n",
      "Train Epoch: 17 [512/1083 (47%)]\tLoss: 0.010784 \tOP: 0.918919\tOR: 0.985507\tOF1: 0.951049\n",
      "Train Epoch: 17 [544/1083 (50%)]\tLoss: 0.010640 \tOP: 0.958333\tOR: 1.000000\tOF1: 0.978723\n",
      "Train Epoch: 17 [576/1083 (53%)]\tLoss: 0.009552 \tOP: 0.893939\tOR: 1.000000\tOF1: 0.944000\n",
      "Train Epoch: 17 [608/1083 (56%)]\tLoss: 0.011079 \tOP: 0.910256\tOR: 1.000000\tOF1: 0.953020\n",
      "Train Epoch: 17 [640/1083 (59%)]\tLoss: 0.011348 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 17 [672/1083 (62%)]\tLoss: 0.008553 \tOP: 0.828125\tOR: 1.000000\tOF1: 0.905983\n",
      "Train Epoch: 17 [704/1083 (65%)]\tLoss: 0.010462 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 17 [736/1083 (68%)]\tLoss: 0.010450 \tOP: 0.880000\tOR: 0.985075\tOF1: 0.929577\n",
      "Train Epoch: 17 [768/1083 (71%)]\tLoss: 0.012598 \tOP: 0.905882\tOR: 1.000000\tOF1: 0.950617\n",
      "Train Epoch: 17 [800/1083 (74%)]\tLoss: 0.010848 \tOP: 0.935897\tOR: 1.000000\tOF1: 0.966887\n",
      "Train Epoch: 17 [832/1083 (76%)]\tLoss: 0.011365 \tOP: 0.894737\tOR: 0.985507\tOF1: 0.937931\n",
      "Train Epoch: 17 [864/1083 (79%)]\tLoss: 0.011446 \tOP: 0.925373\tOR: 0.984127\tOF1: 0.953846\n",
      "Train Epoch: 17 [896/1083 (82%)]\tLoss: 0.012768 \tOP: 0.906667\tOR: 1.000000\tOF1: 0.951049\n",
      "Train Epoch: 17 [928/1083 (85%)]\tLoss: 0.010755 \tOP: 0.945205\tOR: 1.000000\tOF1: 0.971831\n",
      "Train Epoch: 17 [960/1083 (88%)]\tLoss: 0.014948 \tOP: 0.948052\tOR: 1.000000\tOF1: 0.973333\n",
      "Train Epoch: 17 [992/1083 (91%)]\tLoss: 0.015779 \tOP: 0.948052\tOR: 0.986486\tOF1: 0.966887\n",
      "Train Epoch: 17 [1024/1083 (94%)]\tLoss: 0.011302 \tOP: 0.931507\tOR: 1.000000\tOF1: 0.964539\n",
      "Train Epoch: 17 [891/1083 (97%)]\tLoss: 0.010803 \tOP: 0.876923\tOR: 1.000000\tOF1: 0.934426\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2724 \n",
      "OP: 0.257143\n",
      "OR: 0.257143\n",
      "OF1: 0.257143\n",
      "\n",
      "Train Epoch: 18 [0/1083 (0%)]\tLoss: 0.010361 \tOP: 0.925926\tOR: 1.000000\tOF1: 0.961538\n",
      "Train Epoch: 18 [32/1083 (3%)]\tLoss: 0.008251 \tOP: 0.845070\tOR: 1.000000\tOF1: 0.916031\n",
      "Train Epoch: 18 [64/1083 (6%)]\tLoss: 0.013793 \tOP: 0.935897\tOR: 0.986486\tOF1: 0.960526\n",
      "Train Epoch: 18 [96/1083 (9%)]\tLoss: 0.011314 \tOP: 0.941176\tOR: 1.000000\tOF1: 0.969697\n",
      "Train Epoch: 18 [128/1083 (12%)]\tLoss: 0.010441 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 18 [160/1083 (15%)]\tLoss: 0.009843 \tOP: 0.944444\tOR: 1.000000\tOF1: 0.971429\n",
      "Train Epoch: 18 [192/1083 (18%)]\tLoss: 0.010345 \tOP: 0.916667\tOR: 1.000000\tOF1: 0.956522\n",
      "Train Epoch: 18 [224/1083 (21%)]\tLoss: 0.010108 \tOP: 0.931507\tOR: 1.000000\tOF1: 0.964539\n",
      "Train Epoch: 18 [256/1083 (24%)]\tLoss: 0.008432 \tOP: 0.885714\tOR: 1.000000\tOF1: 0.939394\n",
      "Train Epoch: 18 [288/1083 (26%)]\tLoss: 0.010397 \tOP: 0.929577\tOR: 1.000000\tOF1: 0.963504\n",
      "Train Epoch: 18 [320/1083 (29%)]\tLoss: 0.010540 \tOP: 0.906667\tOR: 0.985507\tOF1: 0.944444\n",
      "Train Epoch: 18 [352/1083 (32%)]\tLoss: 0.010201 \tOP: 0.935065\tOR: 1.000000\tOF1: 0.966443\n",
      "Train Epoch: 18 [384/1083 (35%)]\tLoss: 0.009669 \tOP: 0.935897\tOR: 1.000000\tOF1: 0.966887\n",
      "Train Epoch: 18 [416/1083 (38%)]\tLoss: 0.011925 \tOP: 0.947368\tOR: 1.000000\tOF1: 0.972973\n",
      "Train Epoch: 18 [448/1083 (41%)]\tLoss: 0.010825 \tOP: 0.920000\tOR: 1.000000\tOF1: 0.958333\n",
      "Train Epoch: 18 [480/1083 (44%)]\tLoss: 0.009169 \tOP: 0.914286\tOR: 1.000000\tOF1: 0.955224\n",
      "Train Epoch: 18 [512/1083 (47%)]\tLoss: 0.012997 \tOP: 0.916667\tOR: 0.970588\tOF1: 0.942857\n",
      "Train Epoch: 18 [544/1083 (50%)]\tLoss: 0.011597 \tOP: 0.917808\tOR: 0.971014\tOF1: 0.943662\n",
      "Train Epoch: 18 [576/1083 (53%)]\tLoss: 0.011373 \tOP: 0.930556\tOR: 0.985294\tOF1: 0.957143\n",
      "Train Epoch: 18 [608/1083 (56%)]\tLoss: 0.010911 \tOP: 0.905405\tOR: 1.000000\tOF1: 0.950355\n",
      "Train Epoch: 18 [640/1083 (59%)]\tLoss: 0.011322 \tOP: 0.857143\tOR: 1.000000\tOF1: 0.923077\n",
      "Train Epoch: 18 [672/1083 (62%)]\tLoss: 0.009839 \tOP: 0.925000\tOR: 1.000000\tOF1: 0.961039\n",
      "Train Epoch: 18 [704/1083 (65%)]\tLoss: 0.009073 \tOP: 0.902778\tOR: 1.000000\tOF1: 0.948905\n",
      "Train Epoch: 18 [736/1083 (68%)]\tLoss: 0.009223 \tOP: 0.931507\tOR: 1.000000\tOF1: 0.964539\n",
      "Train Epoch: 18 [768/1083 (71%)]\tLoss: 0.011881 \tOP: 0.960000\tOR: 0.986301\tOF1: 0.972973\n",
      "Train Epoch: 18 [800/1083 (74%)]\tLoss: 0.009359 \tOP: 0.845070\tOR: 1.000000\tOF1: 0.916031\n",
      "Train Epoch: 18 [832/1083 (76%)]\tLoss: 0.014386 \tOP: 0.944444\tOR: 0.957746\tOF1: 0.951049\n",
      "Train Epoch: 18 [864/1083 (79%)]\tLoss: 0.011832 \tOP: 0.924051\tOR: 1.000000\tOF1: 0.960526\n",
      "Train Epoch: 18 [896/1083 (82%)]\tLoss: 0.007071 \tOP: 0.859375\tOR: 1.000000\tOF1: 0.924370\n",
      "Train Epoch: 18 [928/1083 (85%)]\tLoss: 0.012605 \tOP: 0.944444\tOR: 1.000000\tOF1: 0.971429\n",
      "Train Epoch: 18 [960/1083 (88%)]\tLoss: 0.009403 \tOP: 0.897436\tOR: 1.000000\tOF1: 0.945946\n",
      "Train Epoch: 18 [992/1083 (91%)]\tLoss: 0.010215 \tOP: 0.903614\tOR: 1.000000\tOF1: 0.949367\n",
      "Train Epoch: 18 [1024/1083 (94%)]\tLoss: 0.010426 \tOP: 0.914286\tOR: 1.000000\tOF1: 0.955224\n",
      "Train Epoch: 18 [891/1083 (97%)]\tLoss: 0.009434 \tOP: 0.873239\tOR: 1.000000\tOF1: 0.932331\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2798 \n",
      "OP: 0.257143\n",
      "OR: 0.257143\n",
      "OF1: 0.257143\n",
      "\n",
      "Train Epoch: 19 [0/1083 (0%)]\tLoss: 0.011848 \tOP: 0.948052\tOR: 1.000000\tOF1: 0.973333\n",
      "Train Epoch: 19 [32/1083 (3%)]\tLoss: 0.007097 \tOP: 0.885714\tOR: 1.000000\tOF1: 0.939394\n",
      "Train Epoch: 19 [64/1083 (6%)]\tLoss: 0.010086 \tOP: 0.900000\tOR: 1.000000\tOF1: 0.947368\n",
      "Train Epoch: 19 [96/1083 (9%)]\tLoss: 0.009395 \tOP: 0.947368\tOR: 1.000000\tOF1: 0.972973\n",
      "Train Epoch: 19 [128/1083 (12%)]\tLoss: 0.010589 \tOP: 0.934211\tOR: 1.000000\tOF1: 0.965986\n",
      "Train Epoch: 19 [160/1083 (15%)]\tLoss: 0.009001 \tOP: 0.901408\tOR: 1.000000\tOF1: 0.948148\n",
      "Train Epoch: 19 [192/1083 (18%)]\tLoss: 0.009972 \tOP: 0.922078\tOR: 1.000000\tOF1: 0.959459\n",
      "Train Epoch: 19 [224/1083 (21%)]\tLoss: 0.011656 \tOP: 0.923077\tOR: 0.986301\tOF1: 0.953642\n",
      "Train Epoch: 19 [256/1083 (24%)]\tLoss: 0.006877 \tOP: 0.876712\tOR: 1.000000\tOF1: 0.934307\n",
      "Train Epoch: 19 [288/1083 (26%)]\tLoss: 0.010903 \tOP: 0.913043\tOR: 1.000000\tOF1: 0.954545\n",
      "Train Epoch: 19 [320/1083 (29%)]\tLoss: 0.008541 \tOP: 0.886076\tOR: 1.000000\tOF1: 0.939597\n",
      "Train Epoch: 19 [352/1083 (32%)]\tLoss: 0.009393 \tOP: 0.906667\tOR: 1.000000\tOF1: 0.951049\n",
      "Train Epoch: 19 [384/1083 (35%)]\tLoss: 0.009389 \tOP: 0.947368\tOR: 1.000000\tOF1: 0.972973\n",
      "Train Epoch: 19 [416/1083 (38%)]\tLoss: 0.020783 \tOP: 0.974684\tOR: 0.962500\tOF1: 0.968553\n",
      "Train Epoch: 19 [448/1083 (41%)]\tLoss: 0.010788 \tOP: 0.924051\tOR: 1.000000\tOF1: 0.960526\n",
      "Train Epoch: 19 [480/1083 (44%)]\tLoss: 0.011174 \tOP: 0.916667\tOR: 1.000000\tOF1: 0.956522\n",
      "Train Epoch: 19 [512/1083 (47%)]\tLoss: 0.009014 \tOP: 0.898551\tOR: 1.000000\tOF1: 0.946565\n",
      "Train Epoch: 19 [544/1083 (50%)]\tLoss: 0.009460 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 19 [576/1083 (53%)]\tLoss: 0.010140 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 19 [608/1083 (56%)]\tLoss: 0.008119 \tOP: 0.867647\tOR: 1.000000\tOF1: 0.929134\n",
      "Train Epoch: 19 [640/1083 (59%)]\tLoss: 0.007930 \tOP: 0.942029\tOR: 1.000000\tOF1: 0.970149\n",
      "Train Epoch: 19 [672/1083 (62%)]\tLoss: 0.008600 \tOP: 0.894737\tOR: 1.000000\tOF1: 0.944444\n",
      "Train Epoch: 19 [704/1083 (65%)]\tLoss: 0.007538 \tOP: 0.890411\tOR: 1.000000\tOF1: 0.942029\n",
      "Train Epoch: 19 [736/1083 (68%)]\tLoss: 0.009549 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 19 [768/1083 (71%)]\tLoss: 0.009539 \tOP: 0.904110\tOR: 1.000000\tOF1: 0.949640\n",
      "Train Epoch: 19 [800/1083 (74%)]\tLoss: 0.009238 \tOP: 0.957143\tOR: 1.000000\tOF1: 0.978102\n",
      "Train Epoch: 19 [832/1083 (76%)]\tLoss: 0.009505 \tOP: 0.922078\tOR: 1.000000\tOF1: 0.959459\n",
      "Train Epoch: 19 [864/1083 (79%)]\tLoss: 0.007877 \tOP: 0.892308\tOR: 1.000000\tOF1: 0.943089\n",
      "Train Epoch: 19 [896/1083 (82%)]\tLoss: 0.008917 \tOP: 0.909091\tOR: 0.985915\tOF1: 0.945946\n",
      "Train Epoch: 19 [928/1083 (85%)]\tLoss: 0.009004 \tOP: 0.894737\tOR: 0.985507\tOF1: 0.937931\n",
      "Train Epoch: 19 [960/1083 (88%)]\tLoss: 0.010999 \tOP: 0.943662\tOR: 1.000000\tOF1: 0.971014\n",
      "Train Epoch: 19 [992/1083 (91%)]\tLoss: 0.009886 \tOP: 0.900000\tOR: 1.000000\tOF1: 0.947368\n",
      "Train Epoch: 19 [1024/1083 (94%)]\tLoss: 0.012401 \tOP: 0.932432\tOR: 1.000000\tOF1: 0.965035\n",
      "Train Epoch: 19 [891/1083 (97%)]\tLoss: 0.011761 \tOP: 0.873016\tOR: 0.982143\tOF1: 0.924370\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2768 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model2.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model2(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'wider')\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model2(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "        \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for n in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for m in range(len(classes)):\n",
    "        #        if target[n][m] == 1 or preds[n][m] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[n][m] == target[n][m]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'wider')\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet152 model\n",
    "from torchvision import models\n",
    "\n",
    "class ResNet152(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet152, self).__init__()\n",
    "        self.resnet = models.resnet152(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(2048, len(classes))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model3 = ResNet152()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1083 (0%)]\tLoss: 0.924930 \tOP: 0.744186\tOR: 0.477612\tOF1: 0.581818\n",
      "Train Epoch: 0 [32/1083 (3%)]\tLoss: 0.914003 \tOP: 0.675000\tOR: 0.421875\tOF1: 0.519231\n",
      "Train Epoch: 0 [64/1083 (6%)]\tLoss: 0.897528 \tOP: 0.560976\tOR: 0.333333\tOF1: 0.418182\n",
      "Train Epoch: 0 [96/1083 (9%)]\tLoss: 0.885361 \tOP: 0.538462\tOR: 0.318182\tOF1: 0.400000\n",
      "Train Epoch: 0 [128/1083 (12%)]\tLoss: 0.872881 \tOP: 0.333333\tOR: 0.149254\tOF1: 0.206186\n",
      "Train Epoch: 0 [160/1083 (15%)]\tLoss: 0.862220 \tOP: 0.354839\tOR: 0.171875\tOF1: 0.231579\n",
      "Train Epoch: 0 [192/1083 (18%)]\tLoss: 0.847336 \tOP: 0.178571\tOR: 0.072464\tOF1: 0.103093\n",
      "Train Epoch: 0 [224/1083 (21%)]\tLoss: 0.839043 \tOP: 0.185185\tOR: 0.078125\tOF1: 0.109890\n",
      "Train Epoch: 0 [256/1083 (24%)]\tLoss: 0.826515 \tOP: 0.120000\tOR: 0.044118\tOF1: 0.064516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-bbf89c8d258d>:40: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [288/1083 (26%)]\tLoss: 0.816072 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [320/1083 (29%)]\tLoss: 0.808090 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [352/1083 (32%)]\tLoss: 0.795985 \tOP: 0.185185\tOR: 0.074627\tOF1: 0.106383\n",
      "Train Epoch: 0 [384/1083 (35%)]\tLoss: 0.786944 \tOP: 0.153846\tOR: 0.054795\tOF1: 0.080808\n",
      "Train Epoch: 0 [416/1083 (38%)]\tLoss: 0.781203 \tOP: 0.153846\tOR: 0.060606\tOF1: 0.086957\n",
      "Train Epoch: 0 [448/1083 (41%)]\tLoss: 0.769401 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [480/1083 (44%)]\tLoss: 0.765841 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [512/1083 (47%)]\tLoss: 0.756202 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [544/1083 (50%)]\tLoss: 0.752000 \tOP: 0.041667\tOR: 0.014085\tOF1: 0.021053\n",
      "Train Epoch: 0 [576/1083 (53%)]\tLoss: 0.747602 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [608/1083 (56%)]\tLoss: 0.743030 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [640/1083 (59%)]\tLoss: 0.738278 \tOP: 0.041667\tOR: 0.015873\tOF1: 0.022989\n",
      "Train Epoch: 0 [672/1083 (62%)]\tLoss: 0.735059 \tOP: 0.083333\tOR: 0.028986\tOF1: 0.043011\n",
      "Train Epoch: 0 [704/1083 (65%)]\tLoss: 0.729598 \tOP: 0.041667\tOR: 0.013514\tOF1: 0.020408\n",
      "Train Epoch: 0 [736/1083 (68%)]\tLoss: 0.729817 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [768/1083 (71%)]\tLoss: 0.725650 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [800/1083 (74%)]\tLoss: 0.722392 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [832/1083 (76%)]\tLoss: 0.718147 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [864/1083 (79%)]\tLoss: 0.715926 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [896/1083 (82%)]\tLoss: 0.714491 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [928/1083 (85%)]\tLoss: 0.712209 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [960/1083 (88%)]\tLoss: 0.710794 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [992/1083 (91%)]\tLoss: 0.708570 \tOP: 0.041667\tOR: 0.013889\tOF1: 0.020833\n",
      "Train Epoch: 0 [1024/1083 (94%)]\tLoss: 0.708928 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [891/1083 (97%)]\tLoss: 0.704310 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-59783cd7f140>:45: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.7064 \n",
      "OP: 0.000000\n",
      "OR: 0.000000\n",
      "OF1: nan\n",
      "\n",
      "Train Epoch: 1 [0/1083 (0%)]\tLoss: 0.705251 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [32/1083 (3%)]\tLoss: 0.703731 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [64/1083 (6%)]\tLoss: 0.701759 \tOP: 0.083333\tOR: 0.029851\tOF1: 0.043956\n",
      "Train Epoch: 1 [96/1083 (9%)]\tLoss: 0.700672 \tOP: 0.041667\tOR: 0.016129\tOF1: 0.023256\n",
      "Train Epoch: 1 [128/1083 (12%)]\tLoss: 0.701930 \tOP: 0.083333\tOR: 0.026316\tOF1: 0.040000\n",
      "Train Epoch: 1 [160/1083 (15%)]\tLoss: 0.700172 \tOP: 0.041667\tOR: 0.013699\tOF1: 0.020619\n",
      "Train Epoch: 1 [192/1083 (18%)]\tLoss: 0.699418 \tOP: 0.083333\tOR: 0.027778\tOF1: 0.041667\n",
      "Train Epoch: 1 [224/1083 (21%)]\tLoss: 0.700266 \tOP: 0.185185\tOR: 0.076923\tOF1: 0.108696\n",
      "Train Epoch: 1 [256/1083 (24%)]\tLoss: 0.699635 \tOP: 0.120000\tOR: 0.046154\tOF1: 0.066667\n",
      "Train Epoch: 1 [288/1083 (26%)]\tLoss: 0.698465 \tOP: 0.185185\tOR: 0.076923\tOF1: 0.108696\n",
      "Train Epoch: 1 [320/1083 (29%)]\tLoss: 0.696940 \tOP: 0.241379\tOR: 0.100000\tOF1: 0.141414\n",
      "Train Epoch: 1 [352/1083 (32%)]\tLoss: 0.698158 \tOP: 0.214286\tOR: 0.092308\tOF1: 0.129032\n",
      "Train Epoch: 1 [384/1083 (35%)]\tLoss: 0.698070 \tOP: 0.266667\tOR: 0.117647\tOF1: 0.163265\n",
      "Train Epoch: 1 [416/1083 (38%)]\tLoss: 0.695764 \tOP: 0.290323\tOR: 0.132353\tOF1: 0.181818\n",
      "Train Epoch: 1 [448/1083 (41%)]\tLoss: 0.695140 \tOP: 0.266667\tOR: 0.117647\tOF1: 0.163265\n",
      "Train Epoch: 1 [480/1083 (44%)]\tLoss: 0.697183 \tOP: 0.290323\tOR: 0.126761\tOF1: 0.176471\n",
      "Train Epoch: 1 [512/1083 (47%)]\tLoss: 0.695044 \tOP: 0.333333\tOR: 0.154930\tOF1: 0.211538\n",
      "Train Epoch: 1 [544/1083 (50%)]\tLoss: 0.695582 \tOP: 0.312500\tOR: 0.158730\tOF1: 0.210526\n",
      "Train Epoch: 1 [576/1083 (53%)]\tLoss: 0.698472 \tOP: 0.185185\tOR: 0.074627\tOF1: 0.106383\n",
      "Train Epoch: 1 [608/1083 (56%)]\tLoss: 0.695571 \tOP: 0.312500\tOR: 0.144928\tOF1: 0.198020\n",
      "Train Epoch: 1 [640/1083 (59%)]\tLoss: 0.697217 \tOP: 0.290323\tOR: 0.132353\tOF1: 0.181818\n",
      "Train Epoch: 1 [672/1083 (62%)]\tLoss: 0.694012 \tOP: 0.333333\tOR: 0.177419\tOF1: 0.231579\n",
      "Train Epoch: 1 [704/1083 (65%)]\tLoss: 0.692899 \tOP: 0.352941\tOR: 0.164384\tOF1: 0.224299\n",
      "Train Epoch: 1 [736/1083 (68%)]\tLoss: 0.696760 \tOP: 0.241379\tOR: 0.102941\tOF1: 0.144330\n",
      "Train Epoch: 1 [768/1083 (71%)]\tLoss: 0.696129 \tOP: 0.241379\tOR: 0.097222\tOF1: 0.138614\n",
      "Train Epoch: 1 [800/1083 (74%)]\tLoss: 0.695802 \tOP: 0.312500\tOR: 0.151515\tOF1: 0.204082\n",
      "Train Epoch: 1 [832/1083 (76%)]\tLoss: 0.693889 \tOP: 0.312500\tOR: 0.151515\tOF1: 0.204082\n",
      "Train Epoch: 1 [864/1083 (79%)]\tLoss: 0.692381 \tOP: 0.371429\tOR: 0.183099\tOF1: 0.245283\n",
      "Train Epoch: 1 [896/1083 (82%)]\tLoss: 0.691758 \tOP: 0.333333\tOR: 0.152778\tOF1: 0.209524\n",
      "Train Epoch: 1 [928/1083 (85%)]\tLoss: 0.695874 \tOP: 0.214286\tOR: 0.086957\tOF1: 0.123711\n",
      "Train Epoch: 1 [960/1083 (88%)]\tLoss: 0.694102 \tOP: 0.333333\tOR: 0.159420\tOF1: 0.215686\n",
      "Train Epoch: 1 [992/1083 (91%)]\tLoss: 0.691754 \tOP: 0.352941\tOR: 0.187500\tOF1: 0.244898\n",
      "Train Epoch: 1 [1024/1083 (94%)]\tLoss: 0.690953 \tOP: 0.388889\tOR: 0.189189\tOF1: 0.254545\n",
      "Train Epoch: 1 [891/1083 (97%)]\tLoss: 0.697694 \tOP: 0.214286\tOR: 0.109091\tOF1: 0.144578\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6953 \n",
      "OP: 0.241379\n",
      "OR: 0.200000\n",
      "OF1: 0.218750\n",
      "\n",
      "Train Epoch: 2 [0/1083 (0%)]\tLoss: 0.688572 \tOP: 0.450000\tOR: 0.257143\tOF1: 0.327273\n",
      "Train Epoch: 2 [32/1083 (3%)]\tLoss: 0.690742 \tOP: 0.388889\tOR: 0.215385\tOF1: 0.277228\n",
      "Train Epoch: 2 [64/1083 (6%)]\tLoss: 0.689665 \tOP: 0.388889\tOR: 0.208955\tOF1: 0.271845\n",
      "Train Epoch: 2 [96/1083 (9%)]\tLoss: 0.689631 \tOP: 0.405405\tOR: 0.227273\tOF1: 0.291262\n",
      "Train Epoch: 2 [128/1083 (12%)]\tLoss: 0.689758 \tOP: 0.435897\tOR: 0.232877\tOF1: 0.303571\n",
      "Train Epoch: 2 [160/1083 (15%)]\tLoss: 0.690750 \tOP: 0.421053\tOR: 0.202532\tOF1: 0.273504\n",
      "Train Epoch: 2 [192/1083 (18%)]\tLoss: 0.689715 \tOP: 0.421053\tOR: 0.228571\tOF1: 0.296296\n",
      "Train Epoch: 2 [224/1083 (21%)]\tLoss: 0.689925 \tOP: 0.435897\tOR: 0.250000\tOF1: 0.317757\n",
      "Train Epoch: 2 [256/1083 (24%)]\tLoss: 0.690257 \tOP: 0.388889\tOR: 0.233333\tOF1: 0.291667\n",
      "Train Epoch: 2 [288/1083 (26%)]\tLoss: 0.688093 \tOP: 0.450000\tOR: 0.300000\tOF1: 0.360000\n",
      "Train Epoch: 2 [320/1083 (29%)]\tLoss: 0.687284 \tOP: 0.463415\tOR: 0.316667\tOF1: 0.376238\n",
      "Train Epoch: 2 [352/1083 (32%)]\tLoss: 0.687622 \tOP: 0.463415\tOR: 0.311475\tOF1: 0.372549\n",
      "Train Epoch: 2 [384/1083 (35%)]\tLoss: 0.688823 \tOP: 0.450000\tOR: 0.264706\tOF1: 0.333333\n",
      "Train Epoch: 2 [416/1083 (38%)]\tLoss: 0.687420 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 2 [448/1083 (41%)]\tLoss: 0.686247 \tOP: 0.511111\tOR: 0.323944\tOF1: 0.396552\n",
      "Train Epoch: 2 [480/1083 (44%)]\tLoss: 0.690770 \tOP: 0.421053\tOR: 0.250000\tOF1: 0.313725\n",
      "Train Epoch: 2 [512/1083 (47%)]\tLoss: 0.692407 \tOP: 0.371429\tOR: 0.203125\tOF1: 0.262626\n",
      "Train Epoch: 2 [544/1083 (50%)]\tLoss: 0.687757 \tOP: 0.450000\tOR: 0.290323\tOF1: 0.352941\n",
      "Train Epoch: 2 [576/1083 (53%)]\tLoss: 0.688980 \tOP: 0.435897\tOR: 0.250000\tOF1: 0.317757\n",
      "Train Epoch: 2 [608/1083 (56%)]\tLoss: 0.688500 \tOP: 0.463415\tOR: 0.267606\tOF1: 0.339286\n",
      "Train Epoch: 2 [640/1083 (59%)]\tLoss: 0.688030 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 2 [672/1083 (62%)]\tLoss: 0.688340 \tOP: 0.463415\tOR: 0.275362\tOF1: 0.345455\n",
      "Train Epoch: 2 [704/1083 (65%)]\tLoss: 0.691919 \tOP: 0.388889\tOR: 0.208955\tOF1: 0.271845\n",
      "Train Epoch: 2 [736/1083 (68%)]\tLoss: 0.689013 \tOP: 0.405405\tOR: 0.217391\tOF1: 0.283019\n",
      "Train Epoch: 2 [768/1083 (71%)]\tLoss: 0.689462 \tOP: 0.421053\tOR: 0.235294\tOF1: 0.301887\n",
      "Train Epoch: 2 [800/1083 (74%)]\tLoss: 0.690114 \tOP: 0.421053\tOR: 0.242424\tOF1: 0.307692\n",
      "Train Epoch: 2 [832/1083 (76%)]\tLoss: 0.685617 \tOP: 0.463415\tOR: 0.253333\tOF1: 0.327586\n",
      "Train Epoch: 2 [864/1083 (79%)]\tLoss: 0.686613 \tOP: 0.463415\tOR: 0.250000\tOF1: 0.324786\n",
      "Train Epoch: 2 [896/1083 (82%)]\tLoss: 0.689560 \tOP: 0.435897\tOR: 0.236111\tOF1: 0.306306\n",
      "Train Epoch: 2 [928/1083 (85%)]\tLoss: 0.689653 \tOP: 0.405405\tOR: 0.205479\tOF1: 0.272727\n",
      "Train Epoch: 2 [960/1083 (88%)]\tLoss: 0.688288 \tOP: 0.450000\tOR: 0.233766\tOF1: 0.307692\n",
      "Train Epoch: 2 [992/1083 (91%)]\tLoss: 0.690199 \tOP: 0.388889\tOR: 0.197183\tOF1: 0.261682\n",
      "Train Epoch: 2 [1024/1083 (94%)]\tLoss: 0.688335 \tOP: 0.435897\tOR: 0.250000\tOF1: 0.317757\n",
      "Train Epoch: 2 [891/1083 (97%)]\tLoss: 0.689787 \tOP: 0.352941\tOR: 0.214286\tOF1: 0.266667\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6937 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 3 [0/1083 (0%)]\tLoss: 0.687882 \tOP: 0.435897\tOR: 0.293103\tOF1: 0.350515\n",
      "Train Epoch: 3 [32/1083 (3%)]\tLoss: 0.686880 \tOP: 0.463415\tOR: 0.296875\tOF1: 0.361905\n",
      "Train Epoch: 3 [64/1083 (6%)]\tLoss: 0.688531 \tOP: 0.421053\tOR: 0.253968\tOF1: 0.316832\n",
      "Train Epoch: 3 [96/1083 (9%)]\tLoss: 0.684585 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 3 [128/1083 (12%)]\tLoss: 0.688476 \tOP: 0.450000\tOR: 0.305085\tOF1: 0.363636\n",
      "Train Epoch: 3 [160/1083 (15%)]\tLoss: 0.687602 \tOP: 0.463415\tOR: 0.279412\tOF1: 0.348624\n",
      "Train Epoch: 3 [192/1083 (18%)]\tLoss: 0.685652 \tOP: 0.488372\tOR: 0.280000\tOF1: 0.355932\n",
      "Train Epoch: 3 [224/1083 (21%)]\tLoss: 0.685042 \tOP: 0.500000\tOR: 0.309859\tOF1: 0.382609\n",
      "Train Epoch: 3 [256/1083 (24%)]\tLoss: 0.688014 \tOP: 0.435897\tOR: 0.274194\tOF1: 0.336634\n",
      "Train Epoch: 3 [288/1083 (26%)]\tLoss: 0.686063 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 3 [320/1083 (29%)]\tLoss: 0.683998 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 3 [352/1083 (32%)]\tLoss: 0.687503 \tOP: 0.463415\tOR: 0.306452\tOF1: 0.368932\n",
      "Train Epoch: 3 [384/1083 (35%)]\tLoss: 0.685510 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 3 [416/1083 (38%)]\tLoss: 0.688863 \tOP: 0.405405\tOR: 0.227273\tOF1: 0.291262\n",
      "Train Epoch: 3 [448/1083 (41%)]\tLoss: 0.686134 \tOP: 0.463415\tOR: 0.275362\tOF1: 0.345455\n",
      "Train Epoch: 3 [480/1083 (44%)]\tLoss: 0.686303 \tOP: 0.463415\tOR: 0.292308\tOF1: 0.358491\n",
      "Train Epoch: 3 [512/1083 (47%)]\tLoss: 0.684984 \tOP: 0.500000\tOR: 0.338462\tOF1: 0.403670\n",
      "Train Epoch: 3 [544/1083 (50%)]\tLoss: 0.685188 \tOP: 0.488372\tOR: 0.313433\tOF1: 0.381818\n",
      "Train Epoch: 3 [576/1083 (53%)]\tLoss: 0.685218 \tOP: 0.488372\tOR: 0.291667\tOF1: 0.365217\n",
      "Train Epoch: 3 [608/1083 (56%)]\tLoss: 0.684966 \tOP: 0.488372\tOR: 0.287671\tOF1: 0.362069\n",
      "Train Epoch: 3 [640/1083 (59%)]\tLoss: 0.686606 \tOP: 0.450000\tOR: 0.236842\tOF1: 0.310345\n",
      "Train Epoch: 3 [672/1083 (62%)]\tLoss: 0.686691 \tOP: 0.450000\tOR: 0.240000\tOF1: 0.313043\n",
      "Train Epoch: 3 [704/1083 (65%)]\tLoss: 0.684202 \tOP: 0.511111\tOR: 0.333333\tOF1: 0.403509\n",
      "Train Epoch: 3 [736/1083 (68%)]\tLoss: 0.686010 \tOP: 0.463415\tOR: 0.287879\tOF1: 0.355140\n",
      "Train Epoch: 3 [768/1083 (71%)]\tLoss: 0.685745 \tOP: 0.463415\tOR: 0.267606\tOF1: 0.339286\n",
      "Train Epoch: 3 [800/1083 (74%)]\tLoss: 0.687088 \tOP: 0.435897\tOR: 0.242857\tOF1: 0.311927\n",
      "Train Epoch: 3 [832/1083 (76%)]\tLoss: 0.688156 \tOP: 0.450000\tOR: 0.272727\tOF1: 0.339623\n",
      "Train Epoch: 3 [864/1083 (79%)]\tLoss: 0.688495 \tOP: 0.421053\tOR: 0.242424\tOF1: 0.307692\n",
      "Train Epoch: 3 [896/1083 (82%)]\tLoss: 0.687100 \tOP: 0.450000\tOR: 0.243243\tOF1: 0.315789\n",
      "Train Epoch: 3 [928/1083 (85%)]\tLoss: 0.686027 \tOP: 0.463415\tOR: 0.287879\tOF1: 0.355140\n",
      "Train Epoch: 3 [960/1083 (88%)]\tLoss: 0.684003 \tOP: 0.488372\tOR: 0.272727\tOF1: 0.350000\n",
      "Train Epoch: 3 [992/1083 (91%)]\tLoss: 0.688589 \tOP: 0.421053\tOR: 0.210526\tOF1: 0.280702\n",
      "Train Epoch: 3 [1024/1083 (94%)]\tLoss: 0.685744 \tOP: 0.450000\tOR: 0.276923\tOF1: 0.342857\n",
      "Train Epoch: 3 [891/1083 (97%)]\tLoss: 0.684022 \tOP: 0.476190\tOR: 0.312500\tOF1: 0.377358\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6929 \n",
      "OP: 0.241379\n",
      "OR: 0.200000\n",
      "OF1: 0.218750\n",
      "\n",
      "Train Epoch: 4 [0/1083 (0%)]\tLoss: 0.685653 \tOP: 0.450000\tOR: 0.243243\tOF1: 0.315789\n",
      "Train Epoch: 4 [32/1083 (3%)]\tLoss: 0.686586 \tOP: 0.450000\tOR: 0.295082\tOF1: 0.356436\n",
      "Train Epoch: 4 [64/1083 (6%)]\tLoss: 0.682909 \tOP: 0.521739\tOR: 0.380952\tOF1: 0.440367\n",
      "Train Epoch: 4 [96/1083 (9%)]\tLoss: 0.688762 \tOP: 0.435897\tOR: 0.265625\tOF1: 0.330097\n",
      "Train Epoch: 4 [128/1083 (12%)]\tLoss: 0.684521 \tOP: 0.521739\tOR: 0.380952\tOF1: 0.440367\n",
      "Train Epoch: 4 [160/1083 (15%)]\tLoss: 0.684686 \tOP: 0.488372\tOR: 0.318182\tOF1: 0.385321\n",
      "Train Epoch: 4 [192/1083 (18%)]\tLoss: 0.683276 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 4 [224/1083 (21%)]\tLoss: 0.686063 \tOP: 0.476190\tOR: 0.273973\tOF1: 0.347826\n",
      "Train Epoch: 4 [256/1083 (24%)]\tLoss: 0.685376 \tOP: 0.488372\tOR: 0.300000\tOF1: 0.371681\n",
      "Train Epoch: 4 [288/1083 (26%)]\tLoss: 0.684869 \tOP: 0.488372\tOR: 0.333333\tOF1: 0.396226\n",
      "Train Epoch: 4 [320/1083 (29%)]\tLoss: 0.685748 \tOP: 0.476190\tOR: 0.312500\tOF1: 0.377358\n",
      "Train Epoch: 4 [352/1083 (32%)]\tLoss: 0.686379 \tOP: 0.476190\tOR: 0.303030\tOF1: 0.370370\n",
      "Train Epoch: 4 [384/1083 (35%)]\tLoss: 0.683754 \tOP: 0.500000\tOR: 0.318841\tOF1: 0.389381\n",
      "Train Epoch: 4 [416/1083 (38%)]\tLoss: 0.686282 \tOP: 0.463415\tOR: 0.271429\tOF1: 0.342342\n",
      "Train Epoch: 4 [448/1083 (41%)]\tLoss: 0.683265 \tOP: 0.511111\tOR: 0.328571\tOF1: 0.400000\n",
      "Train Epoch: 4 [480/1083 (44%)]\tLoss: 0.683747 \tOP: 0.511111\tOR: 0.338235\tOF1: 0.407080\n",
      "Train Epoch: 4 [512/1083 (47%)]\tLoss: 0.684308 \tOP: 0.521739\tOR: 0.333333\tOF1: 0.406780\n",
      "Train Epoch: 4 [544/1083 (50%)]\tLoss: 0.684595 \tOP: 0.488372\tOR: 0.300000\tOF1: 0.371681\n",
      "Train Epoch: 4 [576/1083 (53%)]\tLoss: 0.686459 \tOP: 0.450000\tOR: 0.250000\tOF1: 0.321429\n",
      "Train Epoch: 4 [608/1083 (56%)]\tLoss: 0.684835 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 4 [640/1083 (59%)]\tLoss: 0.684099 \tOP: 0.500000\tOR: 0.293333\tOF1: 0.369748\n",
      "Train Epoch: 4 [672/1083 (62%)]\tLoss: 0.683372 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 4 [704/1083 (65%)]\tLoss: 0.685255 \tOP: 0.500000\tOR: 0.343750\tOF1: 0.407407\n",
      "Train Epoch: 4 [736/1083 (68%)]\tLoss: 0.683696 \tOP: 0.500000\tOR: 0.309859\tOF1: 0.382609\n",
      "Train Epoch: 4 [768/1083 (71%)]\tLoss: 0.686570 \tOP: 0.488372\tOR: 0.304348\tOF1: 0.375000\n",
      "Train Epoch: 4 [800/1083 (74%)]\tLoss: 0.684314 \tOP: 0.511111\tOR: 0.315068\tOF1: 0.389831\n",
      "Train Epoch: 4 [832/1083 (76%)]\tLoss: 0.683117 \tOP: 0.511111\tOR: 0.302632\tOF1: 0.380165\n",
      "Train Epoch: 4 [864/1083 (79%)]\tLoss: 0.690328 \tOP: 0.388889\tOR: 0.218750\tOF1: 0.280000\n",
      "Train Epoch: 4 [896/1083 (82%)]\tLoss: 0.684156 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 4 [928/1083 (85%)]\tLoss: 0.688196 \tOP: 0.435897\tOR: 0.250000\tOF1: 0.317757\n",
      "Train Epoch: 4 [960/1083 (88%)]\tLoss: 0.684344 \tOP: 0.500000\tOR: 0.354839\tOF1: 0.415094\n",
      "Train Epoch: 4 [992/1083 (91%)]\tLoss: 0.685142 \tOP: 0.476190\tOR: 0.281690\tOF1: 0.353982\n",
      "Train Epoch: 4 [1024/1083 (94%)]\tLoss: 0.686201 \tOP: 0.476190\tOR: 0.270270\tOF1: 0.344828\n",
      "Train Epoch: 4 [891/1083 (97%)]\tLoss: 0.683713 \tOP: 0.463415\tOR: 0.365385\tOF1: 0.408602\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6947 \n",
      "OP: 0.290323\n",
      "OR: 0.257143\n",
      "OF1: 0.272727\n",
      "\n",
      "Train Epoch: 5 [0/1083 (0%)]\tLoss: 0.684514 \tOP: 0.488372\tOR: 0.287671\tOF1: 0.362069\n",
      "Train Epoch: 5 [32/1083 (3%)]\tLoss: 0.682529 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 5 [64/1083 (6%)]\tLoss: 0.682828 \tOP: 0.521739\tOR: 0.347826\tOF1: 0.417391\n",
      "Train Epoch: 5 [96/1083 (9%)]\tLoss: 0.684889 \tOP: 0.488372\tOR: 0.350000\tOF1: 0.407767\n",
      "Train Epoch: 5 [128/1083 (12%)]\tLoss: 0.682432 \tOP: 0.521739\tOR: 0.347826\tOF1: 0.417391\n",
      "Train Epoch: 5 [160/1083 (15%)]\tLoss: 0.681508 \tOP: 0.551020\tOR: 0.364865\tOF1: 0.439024\n",
      "Train Epoch: 5 [192/1083 (18%)]\tLoss: 0.684436 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 5 [224/1083 (21%)]\tLoss: 0.687912 \tOP: 0.435897\tOR: 0.246377\tOF1: 0.314815\n",
      "Train Epoch: 5 [256/1083 (24%)]\tLoss: 0.687256 \tOP: 0.463415\tOR: 0.306452\tOF1: 0.368932\n",
      "Train Epoch: 5 [288/1083 (26%)]\tLoss: 0.685996 \tOP: 0.450000\tOR: 0.264706\tOF1: 0.333333\n",
      "Train Epoch: 5 [320/1083 (29%)]\tLoss: 0.683934 \tOP: 0.488372\tOR: 0.280000\tOF1: 0.355932\n",
      "Train Epoch: 5 [352/1083 (32%)]\tLoss: 0.687623 \tOP: 0.450000\tOR: 0.253521\tOF1: 0.324324\n",
      "Train Epoch: 5 [384/1083 (35%)]\tLoss: 0.685193 \tOP: 0.476190\tOR: 0.285714\tOF1: 0.357143\n",
      "Train Epoch: 5 [416/1083 (38%)]\tLoss: 0.681648 \tOP: 0.551020\tOR: 0.409091\tOF1: 0.469565\n",
      "Train Epoch: 5 [448/1083 (41%)]\tLoss: 0.684105 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 5 [480/1083 (44%)]\tLoss: 0.682220 \tOP: 0.531915\tOR: 0.347222\tOF1: 0.420168\n",
      "Train Epoch: 5 [512/1083 (47%)]\tLoss: 0.683948 \tOP: 0.500000\tOR: 0.314286\tOF1: 0.385965\n",
      "Train Epoch: 5 [544/1083 (50%)]\tLoss: 0.683091 \tOP: 0.511111\tOR: 0.333333\tOF1: 0.403509\n",
      "Train Epoch: 5 [576/1083 (53%)]\tLoss: 0.681418 \tOP: 0.541667\tOR: 0.388060\tOF1: 0.452174\n",
      "Train Epoch: 5 [608/1083 (56%)]\tLoss: 0.682500 \tOP: 0.521739\tOR: 0.347826\tOF1: 0.417391\n",
      "Train Epoch: 5 [640/1083 (59%)]\tLoss: 0.685708 \tOP: 0.463415\tOR: 0.287879\tOF1: 0.355140\n",
      "Train Epoch: 5 [672/1083 (62%)]\tLoss: 0.682426 \tOP: 0.521739\tOR: 0.333333\tOF1: 0.406780\n",
      "Train Epoch: 5 [704/1083 (65%)]\tLoss: 0.684136 \tOP: 0.511111\tOR: 0.310811\tOF1: 0.386555\n",
      "Train Epoch: 5 [736/1083 (68%)]\tLoss: 0.685132 \tOP: 0.476190\tOR: 0.303030\tOF1: 0.370370\n",
      "Train Epoch: 5 [768/1083 (71%)]\tLoss: 0.681967 \tOP: 0.531915\tOR: 0.403226\tOF1: 0.458716\n",
      "Train Epoch: 5 [800/1083 (74%)]\tLoss: 0.686057 \tOP: 0.476190\tOR: 0.317460\tOF1: 0.380952\n",
      "Train Epoch: 5 [832/1083 (76%)]\tLoss: 0.682754 \tOP: 0.521739\tOR: 0.352941\tOF1: 0.421053\n",
      "Train Epoch: 5 [864/1083 (79%)]\tLoss: 0.684635 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 5 [896/1083 (82%)]\tLoss: 0.686211 \tOP: 0.450000\tOR: 0.276923\tOF1: 0.342857\n",
      "Train Epoch: 5 [928/1083 (85%)]\tLoss: 0.682318 \tOP: 0.531915\tOR: 0.367647\tOF1: 0.434783\n",
      "Train Epoch: 5 [960/1083 (88%)]\tLoss: 0.685131 \tOP: 0.476190\tOR: 0.322581\tOF1: 0.384615\n",
      "Train Epoch: 5 [992/1083 (91%)]\tLoss: 0.683480 \tOP: 0.488372\tOR: 0.291667\tOF1: 0.365217\n",
      "Train Epoch: 5 [1024/1083 (94%)]\tLoss: 0.682730 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 5 [891/1083 (97%)]\tLoss: 0.682188 \tOP: 0.500000\tOR: 0.392857\tOF1: 0.440000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6929 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 6 [0/1083 (0%)]\tLoss: 0.687641 \tOP: 0.447368\tOR: 0.246377\tOF1: 0.317757\n",
      "Train Epoch: 6 [32/1083 (3%)]\tLoss: 0.684880 \tOP: 0.488372\tOR: 0.350000\tOF1: 0.407767\n",
      "Train Epoch: 6 [64/1083 (6%)]\tLoss: 0.681722 \tOP: 0.531915\tOR: 0.357143\tOF1: 0.427350\n",
      "Train Epoch: 6 [96/1083 (9%)]\tLoss: 0.682487 \tOP: 0.521739\tOR: 0.375000\tOF1: 0.436364\n",
      "Train Epoch: 6 [128/1083 (12%)]\tLoss: 0.681432 \tOP: 0.531915\tOR: 0.357143\tOF1: 0.427350\n",
      "Train Epoch: 6 [160/1083 (15%)]\tLoss: 0.685485 \tOP: 0.450000\tOR: 0.310345\tOF1: 0.367347\n",
      "Train Epoch: 6 [192/1083 (18%)]\tLoss: 0.681823 \tOP: 0.543478\tOR: 0.403226\tOF1: 0.462963\n",
      "Train Epoch: 6 [224/1083 (21%)]\tLoss: 0.682936 \tOP: 0.500000\tOR: 0.309859\tOF1: 0.382609\n",
      "Train Epoch: 6 [256/1083 (24%)]\tLoss: 0.685344 \tOP: 0.476190\tOR: 0.294118\tOF1: 0.363636\n",
      "Train Epoch: 6 [288/1083 (26%)]\tLoss: 0.682050 \tOP: 0.521739\tOR: 0.352941\tOF1: 0.421053\n",
      "Train Epoch: 6 [320/1083 (29%)]\tLoss: 0.684946 \tOP: 0.476190\tOR: 0.285714\tOF1: 0.357143\n",
      "Train Epoch: 6 [352/1083 (32%)]\tLoss: 0.682402 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 6 [384/1083 (35%)]\tLoss: 0.686100 \tOP: 0.463415\tOR: 0.243590\tOF1: 0.319328\n",
      "Train Epoch: 6 [416/1083 (38%)]\tLoss: 0.684075 \tOP: 0.522727\tOR: 0.338235\tOF1: 0.410714\n",
      "Train Epoch: 6 [448/1083 (41%)]\tLoss: 0.681409 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 6 [480/1083 (44%)]\tLoss: 0.683700 \tOP: 0.488372\tOR: 0.323077\tOF1: 0.388889\n",
      "Train Epoch: 6 [512/1083 (47%)]\tLoss: 0.685097 \tOP: 0.500000\tOR: 0.308824\tOF1: 0.381818\n",
      "Train Epoch: 6 [544/1083 (50%)]\tLoss: 0.681387 \tOP: 0.541667\tOR: 0.351351\tOF1: 0.426230\n",
      "Train Epoch: 6 [576/1083 (53%)]\tLoss: 0.682629 \tOP: 0.511111\tOR: 0.328571\tOF1: 0.400000\n",
      "Train Epoch: 6 [608/1083 (56%)]\tLoss: 0.682431 \tOP: 0.521739\tOR: 0.369231\tOF1: 0.432432\n",
      "Train Epoch: 6 [640/1083 (59%)]\tLoss: 0.685585 \tOP: 0.500000\tOR: 0.318182\tOF1: 0.388889\n",
      "Train Epoch: 6 [672/1083 (62%)]\tLoss: 0.681143 \tOP: 0.562500\tOR: 0.415385\tOF1: 0.477876\n",
      "Train Epoch: 6 [704/1083 (65%)]\tLoss: 0.682653 \tOP: 0.521739\tOR: 0.363636\tOF1: 0.428571\n",
      "Train Epoch: 6 [736/1083 (68%)]\tLoss: 0.682551 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 6 [768/1083 (71%)]\tLoss: 0.683730 \tOP: 0.511111\tOR: 0.359375\tOF1: 0.422018\n",
      "Train Epoch: 6 [800/1083 (74%)]\tLoss: 0.685452 \tOP: 0.522727\tOR: 0.338235\tOF1: 0.410714\n",
      "Train Epoch: 6 [832/1083 (76%)]\tLoss: 0.687349 \tOP: 0.487805\tOR: 0.277778\tOF1: 0.353982\n",
      "Train Epoch: 6 [864/1083 (79%)]\tLoss: 0.684768 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 6 [896/1083 (82%)]\tLoss: 0.681277 \tOP: 0.531915\tOR: 0.396825\tOF1: 0.454545\n",
      "Train Epoch: 6 [928/1083 (85%)]\tLoss: 0.685887 \tOP: 0.463415\tOR: 0.263889\tOF1: 0.336283\n",
      "Train Epoch: 6 [960/1083 (88%)]\tLoss: 0.682145 \tOP: 0.543478\tOR: 0.342466\tOF1: 0.420168\n",
      "Train Epoch: 6 [992/1083 (91%)]\tLoss: 0.684911 \tOP: 0.488372\tOR: 0.280000\tOF1: 0.355932\n",
      "Train Epoch: 6 [1024/1083 (94%)]\tLoss: 0.682701 \tOP: 0.500000\tOR: 0.309859\tOF1: 0.382609\n",
      "Train Epoch: 6 [891/1083 (97%)]\tLoss: 0.680804 \tOP: 0.500000\tOR: 0.354839\tOF1: 0.415094\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6931 \n",
      "OP: 0.185185\n",
      "OR: 0.142857\n",
      "OF1: 0.161290\n",
      "\n",
      "Train Epoch: 7 [0/1083 (0%)]\tLoss: 0.681840 \tOP: 0.543478\tOR: 0.390625\tOF1: 0.454545\n",
      "Train Epoch: 7 [32/1083 (3%)]\tLoss: 0.682080 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 7 [64/1083 (6%)]\tLoss: 0.686401 \tOP: 0.475000\tOR: 0.271429\tOF1: 0.345455\n",
      "Train Epoch: 7 [96/1083 (9%)]\tLoss: 0.681346 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 7 [128/1083 (12%)]\tLoss: 0.688280 \tOP: 0.421053\tOR: 0.246154\tOF1: 0.310680\n",
      "Train Epoch: 7 [160/1083 (15%)]\tLoss: 0.680498 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 7 [192/1083 (18%)]\tLoss: 0.685275 \tOP: 0.487805\tOR: 0.303030\tOF1: 0.373832\n",
      "Train Epoch: 7 [224/1083 (21%)]\tLoss: 0.683235 \tOP: 0.511628\tOR: 0.318841\tOF1: 0.392857\n",
      "Train Epoch: 7 [256/1083 (24%)]\tLoss: 0.681265 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 7 [288/1083 (26%)]\tLoss: 0.685324 \tOP: 0.500000\tOR: 0.344262\tOF1: 0.407767\n",
      "Train Epoch: 7 [320/1083 (29%)]\tLoss: 0.680657 \tOP: 0.562500\tOR: 0.364865\tOF1: 0.442623\n",
      "Train Epoch: 7 [352/1083 (32%)]\tLoss: 0.682941 \tOP: 0.511111\tOR: 0.338235\tOF1: 0.407080\n",
      "Train Epoch: 7 [384/1083 (35%)]\tLoss: 0.682753 \tOP: 0.521739\tOR: 0.369231\tOF1: 0.432432\n",
      "Train Epoch: 7 [416/1083 (38%)]\tLoss: 0.681182 \tOP: 0.541667\tOR: 0.346667\tOF1: 0.422764\n",
      "Train Epoch: 7 [448/1083 (41%)]\tLoss: 0.686554 \tOP: 0.435897\tOR: 0.257576\tOF1: 0.323810\n",
      "Train Epoch: 7 [480/1083 (44%)]\tLoss: 0.682981 \tOP: 0.522727\tOR: 0.302632\tOF1: 0.383333\n",
      "Train Epoch: 7 [512/1083 (47%)]\tLoss: 0.686307 \tOP: 0.450000\tOR: 0.253521\tOF1: 0.324324\n",
      "Train Epoch: 7 [544/1083 (50%)]\tLoss: 0.684583 \tOP: 0.488372\tOR: 0.328125\tOF1: 0.392523\n",
      "Train Epoch: 7 [576/1083 (53%)]\tLoss: 0.680809 \tOP: 0.551020\tOR: 0.397059\tOF1: 0.461538\n",
      "Train Epoch: 7 [608/1083 (56%)]\tLoss: 0.681967 \tOP: 0.553191\tOR: 0.412698\tOF1: 0.472727\n",
      "Train Epoch: 7 [640/1083 (59%)]\tLoss: 0.682298 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 7 [672/1083 (62%)]\tLoss: 0.682937 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 7 [704/1083 (65%)]\tLoss: 0.681704 \tOP: 0.531915\tOR: 0.367647\tOF1: 0.434783\n",
      "Train Epoch: 7 [736/1083 (68%)]\tLoss: 0.683062 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 7 [768/1083 (71%)]\tLoss: 0.683384 \tOP: 0.533333\tOR: 0.369231\tOF1: 0.436364\n",
      "Train Epoch: 7 [800/1083 (74%)]\tLoss: 0.682410 \tOP: 0.521739\tOR: 0.363636\tOF1: 0.428571\n",
      "Train Epoch: 7 [832/1083 (76%)]\tLoss: 0.682438 \tOP: 0.522727\tOR: 0.310811\tOF1: 0.389831\n",
      "Train Epoch: 7 [864/1083 (79%)]\tLoss: 0.682490 \tOP: 0.543478\tOR: 0.357143\tOF1: 0.431034\n",
      "Train Epoch: 7 [896/1083 (82%)]\tLoss: 0.680656 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 7 [928/1083 (85%)]\tLoss: 0.684032 \tOP: 0.500000\tOR: 0.304348\tOF1: 0.378378\n",
      "Train Epoch: 7 [960/1083 (88%)]\tLoss: 0.683192 \tOP: 0.511111\tOR: 0.365079\tOF1: 0.425926\n",
      "Train Epoch: 7 [992/1083 (91%)]\tLoss: 0.680396 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 7 [1024/1083 (94%)]\tLoss: 0.681321 \tOP: 0.541667\tOR: 0.376812\tOF1: 0.444444\n",
      "Train Epoch: 7 [891/1083 (97%)]\tLoss: 0.682214 \tOP: 0.487805\tOR: 0.392157\tOF1: 0.434783\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6922 \n",
      "OP: 0.266667\n",
      "OR: 0.228571\n",
      "OF1: 0.246154\n",
      "\n",
      "Train Epoch: 8 [0/1083 (0%)]\tLoss: 0.681192 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 8 [32/1083 (3%)]\tLoss: 0.680689 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 8 [64/1083 (6%)]\tLoss: 0.680383 \tOP: 0.571429\tOR: 0.417910\tOF1: 0.482759\n",
      "Train Epoch: 8 [96/1083 (9%)]\tLoss: 0.682003 \tOP: 0.543478\tOR: 0.367647\tOF1: 0.438596\n",
      "Train Epoch: 8 [128/1083 (12%)]\tLoss: 0.685589 \tOP: 0.475000\tOR: 0.283582\tOF1: 0.355140\n",
      "Train Epoch: 8 [160/1083 (15%)]\tLoss: 0.681396 \tOP: 0.543478\tOR: 0.396825\tOF1: 0.458716\n",
      "Train Epoch: 8 [192/1083 (18%)]\tLoss: 0.682806 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 8 [224/1083 (21%)]\tLoss: 0.682565 \tOP: 0.533333\tOR: 0.338028\tOF1: 0.413793\n",
      "Train Epoch: 8 [256/1083 (24%)]\tLoss: 0.683282 \tOP: 0.522727\tOR: 0.377049\tOF1: 0.438095\n",
      "Train Epoch: 8 [288/1083 (26%)]\tLoss: 0.685311 \tOP: 0.476190\tOR: 0.294118\tOF1: 0.363636\n",
      "Train Epoch: 8 [320/1083 (29%)]\tLoss: 0.685539 \tOP: 0.463415\tOR: 0.311475\tOF1: 0.372549\n",
      "Train Epoch: 8 [352/1083 (32%)]\tLoss: 0.682979 \tOP: 0.511628\tOR: 0.333333\tOF1: 0.403670\n",
      "Train Epoch: 8 [384/1083 (35%)]\tLoss: 0.684235 \tOP: 0.511628\tOR: 0.314286\tOF1: 0.389381\n",
      "Train Epoch: 8 [416/1083 (38%)]\tLoss: 0.684997 \tOP: 0.488372\tOR: 0.304348\tOF1: 0.375000\n",
      "Train Epoch: 8 [448/1083 (41%)]\tLoss: 0.681350 \tOP: 0.541667\tOR: 0.412698\tOF1: 0.468468\n",
      "Train Epoch: 8 [480/1083 (44%)]\tLoss: 0.681896 \tOP: 0.531915\tOR: 0.396825\tOF1: 0.454545\n",
      "Train Epoch: 8 [512/1083 (47%)]\tLoss: 0.683758 \tOP: 0.500000\tOR: 0.300000\tOF1: 0.375000\n",
      "Train Epoch: 8 [544/1083 (50%)]\tLoss: 0.682739 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 8 [576/1083 (53%)]\tLoss: 0.682657 \tOP: 0.521739\tOR: 0.328767\tOF1: 0.403361\n",
      "Train Epoch: 8 [608/1083 (56%)]\tLoss: 0.682200 \tOP: 0.533333\tOR: 0.324324\tOF1: 0.403361\n",
      "Train Epoch: 8 [640/1083 (59%)]\tLoss: 0.682453 \tOP: 0.511111\tOR: 0.306667\tOF1: 0.383333\n",
      "Train Epoch: 8 [672/1083 (62%)]\tLoss: 0.681138 \tOP: 0.541667\tOR: 0.400000\tOF1: 0.460177\n",
      "Train Epoch: 8 [704/1083 (65%)]\tLoss: 0.682108 \tOP: 0.522727\tOR: 0.319444\tOF1: 0.396552\n",
      "Train Epoch: 8 [736/1083 (68%)]\tLoss: 0.681666 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 8 [768/1083 (71%)]\tLoss: 0.686429 \tOP: 0.461538\tOR: 0.281250\tOF1: 0.349515\n",
      "Train Epoch: 8 [800/1083 (74%)]\tLoss: 0.681869 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 8 [832/1083 (76%)]\tLoss: 0.679768 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 8 [864/1083 (79%)]\tLoss: 0.682232 \tOP: 0.521739\tOR: 0.358209\tOF1: 0.424779\n",
      "Train Epoch: 8 [896/1083 (82%)]\tLoss: 0.681416 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 8 [928/1083 (85%)]\tLoss: 0.680946 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 8 [960/1083 (88%)]\tLoss: 0.680551 \tOP: 0.562500\tOR: 0.409091\tOF1: 0.473684\n",
      "Train Epoch: 8 [992/1083 (91%)]\tLoss: 0.682664 \tOP: 0.521739\tOR: 0.315789\tOF1: 0.393443\n",
      "Train Epoch: 8 [1024/1083 (94%)]\tLoss: 0.681863 \tOP: 0.533333\tOR: 0.333333\tOF1: 0.410256\n",
      "Train Epoch: 8 [891/1083 (97%)]\tLoss: 0.681145 \tOP: 0.511628\tOR: 0.440000\tOF1: 0.473118\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6935 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 9 [0/1083 (0%)]\tLoss: 0.681169 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 9 [32/1083 (3%)]\tLoss: 0.680984 \tOP: 0.541667\tOR: 0.361111\tOF1: 0.433333\n",
      "Train Epoch: 9 [64/1083 (6%)]\tLoss: 0.680182 \tOP: 0.560000\tOR: 0.444444\tOF1: 0.495575\n",
      "Train Epoch: 9 [96/1083 (9%)]\tLoss: 0.680563 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 9 [128/1083 (12%)]\tLoss: 0.680932 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 9 [160/1083 (15%)]\tLoss: 0.682958 \tOP: 0.533333\tOR: 0.347826\tOF1: 0.421053\n",
      "Train Epoch: 9 [192/1083 (18%)]\tLoss: 0.683561 \tOP: 0.500000\tOR: 0.360656\tOF1: 0.419048\n",
      "Train Epoch: 9 [224/1083 (21%)]\tLoss: 0.681141 \tOP: 0.553191\tOR: 0.351351\tOF1: 0.429752\n",
      "Train Epoch: 9 [256/1083 (24%)]\tLoss: 0.684031 \tOP: 0.522727\tOR: 0.359375\tOF1: 0.425926\n",
      "Train Epoch: 9 [288/1083 (26%)]\tLoss: 0.683953 \tOP: 0.522727\tOR: 0.343284\tOF1: 0.414414\n",
      "Train Epoch: 9 [320/1083 (29%)]\tLoss: 0.686148 \tOP: 0.475000\tOR: 0.275362\tOF1: 0.348624\n",
      "Train Epoch: 9 [352/1083 (32%)]\tLoss: 0.681477 \tOP: 0.543478\tOR: 0.367647\tOF1: 0.438596\n",
      "Train Epoch: 9 [384/1083 (35%)]\tLoss: 0.682411 \tOP: 0.522727\tOR: 0.323944\tOF1: 0.400000\n",
      "Train Epoch: 9 [416/1083 (38%)]\tLoss: 0.681656 \tOP: 0.541667\tOR: 0.393939\tOF1: 0.456140\n",
      "Train Epoch: 9 [448/1083 (41%)]\tLoss: 0.679550 \tOP: 0.580000\tOR: 0.381579\tOF1: 0.460317\n",
      "Train Epoch: 9 [480/1083 (44%)]\tLoss: 0.681952 \tOP: 0.533333\tOR: 0.380952\tOF1: 0.444444\n",
      "Train Epoch: 9 [512/1083 (47%)]\tLoss: 0.682503 \tOP: 0.522727\tOR: 0.348485\tOF1: 0.418182\n",
      "Train Epoch: 9 [544/1083 (50%)]\tLoss: 0.679303 \tOP: 0.588235\tOR: 0.434783\tOF1: 0.500000\n",
      "Train Epoch: 9 [576/1083 (53%)]\tLoss: 0.682294 \tOP: 0.531915\tOR: 0.347222\tOF1: 0.420168\n",
      "Train Epoch: 9 [608/1083 (56%)]\tLoss: 0.680767 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 9 [640/1083 (59%)]\tLoss: 0.682041 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 9 [672/1083 (62%)]\tLoss: 0.681436 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 9 [704/1083 (65%)]\tLoss: 0.684959 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 9 [736/1083 (68%)]\tLoss: 0.680933 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 9 [768/1083 (71%)]\tLoss: 0.682967 \tOP: 0.500000\tOR: 0.318841\tOF1: 0.389381\n",
      "Train Epoch: 9 [800/1083 (74%)]\tLoss: 0.680924 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 9 [832/1083 (76%)]\tLoss: 0.681841 \tOP: 0.543478\tOR: 0.352113\tOF1: 0.427350\n",
      "Train Epoch: 9 [864/1083 (79%)]\tLoss: 0.682898 \tOP: 0.500000\tOR: 0.314286\tOF1: 0.385965\n",
      "Train Epoch: 9 [896/1083 (82%)]\tLoss: 0.683030 \tOP: 0.511628\tOR: 0.343750\tOF1: 0.411215\n",
      "Train Epoch: 9 [928/1083 (85%)]\tLoss: 0.680357 \tOP: 0.551020\tOR: 0.397059\tOF1: 0.461538\n",
      "Train Epoch: 9 [960/1083 (88%)]\tLoss: 0.682800 \tOP: 0.533333\tOR: 0.363636\tOF1: 0.432432\n",
      "Train Epoch: 9 [992/1083 (91%)]\tLoss: 0.681745 \tOP: 0.533333\tOR: 0.380952\tOF1: 0.444444\n",
      "Train Epoch: 9 [1024/1083 (94%)]\tLoss: 0.682768 \tOP: 0.522727\tOR: 0.338235\tOF1: 0.410714\n",
      "Train Epoch: 9 [891/1083 (97%)]\tLoss: 0.679839 \tOP: 0.533333\tOR: 0.393443\tOF1: 0.452830\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6955 \n",
      "OP: 0.241379\n",
      "OR: 0.200000\n",
      "OF1: 0.218750\n",
      "\n",
      "Train Epoch: 10 [0/1083 (0%)]\tLoss: 0.684221 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 10 [32/1083 (3%)]\tLoss: 0.682265 \tOP: 0.543478\tOR: 0.416667\tOF1: 0.471698\n",
      "Train Epoch: 10 [64/1083 (6%)]\tLoss: 0.679982 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 10 [96/1083 (9%)]\tLoss: 0.679814 \tOP: 0.571429\tOR: 0.451613\tOF1: 0.504505\n",
      "Train Epoch: 10 [128/1083 (12%)]\tLoss: 0.681022 \tOP: 0.553191\tOR: 0.490566\tOF1: 0.520000\n",
      "Train Epoch: 10 [160/1083 (15%)]\tLoss: 0.681911 \tOP: 0.533333\tOR: 0.400000\tOF1: 0.457143\n",
      "Train Epoch: 10 [192/1083 (18%)]\tLoss: 0.683546 \tOP: 0.511628\tOR: 0.354839\tOF1: 0.419048\n",
      "Train Epoch: 10 [224/1083 (21%)]\tLoss: 0.681485 \tOP: 0.531915\tOR: 0.362319\tOF1: 0.431034\n",
      "Train Epoch: 10 [256/1083 (24%)]\tLoss: 0.681400 \tOP: 0.543478\tOR: 0.367647\tOF1: 0.438596\n",
      "Train Epoch: 10 [288/1083 (26%)]\tLoss: 0.683028 \tOP: 0.500000\tOR: 0.323529\tOF1: 0.392857\n",
      "Train Epoch: 10 [320/1083 (29%)]\tLoss: 0.680860 \tOP: 0.553191\tOR: 0.366197\tOF1: 0.440678\n",
      "Train Epoch: 10 [352/1083 (32%)]\tLoss: 0.679962 \tOP: 0.560000\tOR: 0.394366\tOF1: 0.462810\n",
      "Train Epoch: 10 [384/1083 (35%)]\tLoss: 0.683799 \tOP: 0.476190\tOR: 0.307692\tOF1: 0.373832\n",
      "Train Epoch: 10 [416/1083 (38%)]\tLoss: 0.680643 \tOP: 0.560000\tOR: 0.383562\tOF1: 0.455285\n",
      "Train Epoch: 10 [448/1083 (41%)]\tLoss: 0.680676 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 10 [480/1083 (44%)]\tLoss: 0.680468 \tOP: 0.571429\tOR: 0.400000\tOF1: 0.470588\n",
      "Train Epoch: 10 [512/1083 (47%)]\tLoss: 0.679893 \tOP: 0.571429\tOR: 0.459016\tOF1: 0.509091\n",
      "Train Epoch: 10 [544/1083 (50%)]\tLoss: 0.682638 \tOP: 0.522727\tOR: 0.302632\tOF1: 0.383333\n",
      "Train Epoch: 10 [576/1083 (53%)]\tLoss: 0.680330 \tOP: 0.571429\tOR: 0.437500\tOF1: 0.495575\n",
      "Train Epoch: 10 [608/1083 (56%)]\tLoss: 0.680190 \tOP: 0.571429\tOR: 0.363636\tOF1: 0.444444\n",
      "Train Epoch: 10 [640/1083 (59%)]\tLoss: 0.682157 \tOP: 0.533333\tOR: 0.369231\tOF1: 0.436364\n",
      "Train Epoch: 10 [672/1083 (62%)]\tLoss: 0.679988 \tOP: 0.571429\tOR: 0.383562\tOF1: 0.459016\n",
      "Train Epoch: 10 [704/1083 (65%)]\tLoss: 0.683247 \tOP: 0.522727\tOR: 0.306667\tOF1: 0.386555\n",
      "Train Epoch: 10 [736/1083 (68%)]\tLoss: 0.682209 \tOP: 0.521739\tOR: 0.338028\tOF1: 0.410256\n",
      "Train Epoch: 10 [768/1083 (71%)]\tLoss: 0.682243 \tOP: 0.521739\tOR: 0.375000\tOF1: 0.436364\n",
      "Train Epoch: 10 [800/1083 (74%)]\tLoss: 0.682216 \tOP: 0.522727\tOR: 0.319444\tOF1: 0.396552\n",
      "Train Epoch: 10 [832/1083 (76%)]\tLoss: 0.680947 \tOP: 0.553191\tOR: 0.371429\tOF1: 0.444444\n",
      "Train Epoch: 10 [864/1083 (79%)]\tLoss: 0.682639 \tOP: 0.500000\tOR: 0.314286\tOF1: 0.385965\n",
      "Train Epoch: 10 [896/1083 (82%)]\tLoss: 0.678886 \tOP: 0.588235\tOR: 0.422535\tOF1: 0.491803\n",
      "Train Epoch: 10 [928/1083 (85%)]\tLoss: 0.682578 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 10 [960/1083 (88%)]\tLoss: 0.680689 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 10 [992/1083 (91%)]\tLoss: 0.682634 \tOP: 0.522727\tOR: 0.323944\tOF1: 0.400000\n",
      "Train Epoch: 10 [1024/1083 (94%)]\tLoss: 0.680722 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 10 [891/1083 (97%)]\tLoss: 0.680640 \tOP: 0.511628\tOR: 0.360656\tOF1: 0.423077\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6934 \n",
      "OP: 0.266667\n",
      "OR: 0.228571\n",
      "OF1: 0.246154\n",
      "\n",
      "Train Epoch: 11 [0/1083 (0%)]\tLoss: 0.679774 \tOP: 0.568627\tOR: 0.414286\tOF1: 0.479339\n",
      "Train Epoch: 11 [32/1083 (3%)]\tLoss: 0.684321 \tOP: 0.511628\tOR: 0.333333\tOF1: 0.403670\n",
      "Train Epoch: 11 [64/1083 (6%)]\tLoss: 0.683748 \tOP: 0.488372\tOR: 0.328125\tOF1: 0.392523\n",
      "Train Epoch: 11 [96/1083 (9%)]\tLoss: 0.682134 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 11 [128/1083 (12%)]\tLoss: 0.680476 \tOP: 0.551020\tOR: 0.421875\tOF1: 0.477876\n",
      "Train Epoch: 11 [160/1083 (15%)]\tLoss: 0.684461 \tOP: 0.476190\tOR: 0.327869\tOF1: 0.388350\n",
      "Train Epoch: 11 [192/1083 (18%)]\tLoss: 0.681466 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 11 [224/1083 (21%)]\tLoss: 0.681114 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 11 [256/1083 (24%)]\tLoss: 0.679359 \tOP: 0.580000\tOR: 0.408451\tOF1: 0.479339\n",
      "Train Epoch: 11 [288/1083 (26%)]\tLoss: 0.682769 \tOP: 0.521739\tOR: 0.393443\tOF1: 0.448598\n",
      "Train Epoch: 11 [320/1083 (29%)]\tLoss: 0.690340 \tOP: 0.416667\tOR: 0.220588\tOF1: 0.288462\n",
      "Train Epoch: 11 [352/1083 (32%)]\tLoss: 0.680017 \tOP: 0.560000\tOR: 0.405797\tOF1: 0.470588\n",
      "Train Epoch: 11 [384/1083 (35%)]\tLoss: 0.682222 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 11 [416/1083 (38%)]\tLoss: 0.681840 \tOP: 0.533333\tOR: 0.358209\tOF1: 0.428571\n",
      "Train Epoch: 11 [448/1083 (41%)]\tLoss: 0.682337 \tOP: 0.522727\tOR: 0.370968\tOF1: 0.433962\n",
      "Train Epoch: 11 [480/1083 (44%)]\tLoss: 0.679761 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 11 [512/1083 (47%)]\tLoss: 0.678619 \tOP: 0.596154\tOR: 0.397436\tOF1: 0.476923\n",
      "Train Epoch: 11 [544/1083 (50%)]\tLoss: 0.681976 \tOP: 0.533333\tOR: 0.328767\tOF1: 0.406780\n",
      "Train Epoch: 11 [576/1083 (53%)]\tLoss: 0.681091 \tOP: 0.531915\tOR: 0.378788\tOF1: 0.442478\n",
      "Train Epoch: 11 [608/1083 (56%)]\tLoss: 0.682478 \tOP: 0.522727\tOR: 0.348485\tOF1: 0.418182\n",
      "Train Epoch: 11 [640/1083 (59%)]\tLoss: 0.682660 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 11 [672/1083 (62%)]\tLoss: 0.684072 \tOP: 0.511628\tOR: 0.328358\tOF1: 0.400000\n",
      "Train Epoch: 11 [704/1083 (65%)]\tLoss: 0.680253 \tOP: 0.560000\tOR: 0.400000\tOF1: 0.466667\n",
      "Train Epoch: 11 [736/1083 (68%)]\tLoss: 0.681158 \tOP: 0.543478\tOR: 0.357143\tOF1: 0.431034\n",
      "Train Epoch: 11 [768/1083 (71%)]\tLoss: 0.681768 \tOP: 0.533333\tOR: 0.333333\tOF1: 0.410256\n",
      "Train Epoch: 11 [800/1083 (74%)]\tLoss: 0.680159 \tOP: 0.571429\tOR: 0.451613\tOF1: 0.504505\n",
      "Train Epoch: 11 [832/1083 (76%)]\tLoss: 0.678851 \tOP: 0.588235\tOR: 0.422535\tOF1: 0.491803\n",
      "Train Epoch: 11 [864/1083 (79%)]\tLoss: 0.681652 \tOP: 0.543478\tOR: 0.378788\tOF1: 0.446429\n",
      "Train Epoch: 11 [896/1083 (82%)]\tLoss: 0.682451 \tOP: 0.522727\tOR: 0.353846\tOF1: 0.422018\n",
      "Train Epoch: 11 [928/1083 (85%)]\tLoss: 0.679576 \tOP: 0.571429\tOR: 0.437500\tOF1: 0.495575\n",
      "Train Epoch: 11 [960/1083 (88%)]\tLoss: 0.679995 \tOP: 0.562500\tOR: 0.415385\tOF1: 0.477876\n",
      "Train Epoch: 11 [992/1083 (91%)]\tLoss: 0.680574 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 11 [1024/1083 (94%)]\tLoss: 0.680613 \tOP: 0.551020\tOR: 0.375000\tOF1: 0.446281\n",
      "Train Epoch: 11 [891/1083 (97%)]\tLoss: 0.681669 \tOP: 0.488372\tOR: 0.338710\tOF1: 0.400000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6934 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 12 [0/1083 (0%)]\tLoss: 0.680322 \tOP: 0.571429\tOR: 0.474576\tOF1: 0.518519\n",
      "Train Epoch: 12 [32/1083 (3%)]\tLoss: 0.682408 \tOP: 0.533333\tOR: 0.311688\tOF1: 0.393443\n",
      "Train Epoch: 12 [64/1083 (6%)]\tLoss: 0.683311 \tOP: 0.500000\tOR: 0.304348\tOF1: 0.378378\n",
      "Train Epoch: 12 [96/1083 (9%)]\tLoss: 0.681284 \tOP: 0.543478\tOR: 0.403226\tOF1: 0.462963\n",
      "Train Epoch: 12 [128/1083 (12%)]\tLoss: 0.678994 \tOP: 0.588235\tOR: 0.428571\tOF1: 0.495868\n",
      "Train Epoch: 12 [160/1083 (15%)]\tLoss: 0.683450 \tOP: 0.511628\tOR: 0.297297\tOF1: 0.376068\n",
      "Train Epoch: 12 [192/1083 (18%)]\tLoss: 0.680161 \tOP: 0.571429\tOR: 0.411765\tOF1: 0.478632\n",
      "Train Epoch: 12 [224/1083 (21%)]\tLoss: 0.681255 \tOP: 0.541667\tOR: 0.371429\tOF1: 0.440678\n",
      "Train Epoch: 12 [256/1083 (24%)]\tLoss: 0.681884 \tOP: 0.521739\tOR: 0.333333\tOF1: 0.406780\n",
      "Train Epoch: 12 [288/1083 (26%)]\tLoss: 0.680858 \tOP: 0.541667\tOR: 0.433333\tOF1: 0.481481\n",
      "Train Epoch: 12 [320/1083 (29%)]\tLoss: 0.680136 \tOP: 0.580000\tOR: 0.439394\tOF1: 0.500000\n",
      "Train Epoch: 12 [352/1083 (32%)]\tLoss: 0.684556 \tOP: 0.488372\tOR: 0.333333\tOF1: 0.396226\n",
      "Train Epoch: 12 [384/1083 (35%)]\tLoss: 0.682920 \tOP: 0.511628\tOR: 0.338462\tOF1: 0.407407\n",
      "Train Epoch: 12 [416/1083 (38%)]\tLoss: 0.683896 \tOP: 0.500000\tOR: 0.350000\tOF1: 0.411765\n",
      "Train Epoch: 12 [448/1083 (41%)]\tLoss: 0.680236 \tOP: 0.562500\tOR: 0.402985\tOF1: 0.469565\n",
      "Train Epoch: 12 [480/1083 (44%)]\tLoss: 0.679395 \tOP: 0.568627\tOR: 0.397260\tOF1: 0.467742\n",
      "Train Epoch: 12 [512/1083 (47%)]\tLoss: 0.683464 \tOP: 0.511628\tOR: 0.328358\tOF1: 0.400000\n",
      "Train Epoch: 12 [544/1083 (50%)]\tLoss: 0.680285 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 12 [576/1083 (53%)]\tLoss: 0.680278 \tOP: 0.562500\tOR: 0.409091\tOF1: 0.473684\n",
      "Train Epoch: 12 [608/1083 (56%)]\tLoss: 0.681714 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 12 [640/1083 (59%)]\tLoss: 0.679802 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 12 [672/1083 (62%)]\tLoss: 0.683364 \tOP: 0.488372\tOR: 0.308824\tOF1: 0.378378\n",
      "Train Epoch: 12 [704/1083 (65%)]\tLoss: 0.679895 \tOP: 0.560000\tOR: 0.394366\tOF1: 0.462810\n",
      "Train Epoch: 12 [736/1083 (68%)]\tLoss: 0.680850 \tOP: 0.541667\tOR: 0.366197\tOF1: 0.436975\n",
      "Train Epoch: 12 [768/1083 (71%)]\tLoss: 0.682577 \tOP: 0.511628\tOR: 0.305556\tOF1: 0.382609\n",
      "Train Epoch: 12 [800/1083 (74%)]\tLoss: 0.682257 \tOP: 0.543478\tOR: 0.337838\tOF1: 0.416667\n",
      "Train Epoch: 12 [832/1083 (76%)]\tLoss: 0.678630 \tOP: 0.596154\tOR: 0.442857\tOF1: 0.508197\n",
      "Train Epoch: 12 [864/1083 (79%)]\tLoss: 0.680647 \tOP: 0.541667\tOR: 0.412698\tOF1: 0.468468\n",
      "Train Epoch: 12 [896/1083 (82%)]\tLoss: 0.683614 \tOP: 0.500000\tOR: 0.313433\tOF1: 0.385321\n",
      "Train Epoch: 12 [928/1083 (85%)]\tLoss: 0.682079 \tOP: 0.543478\tOR: 0.378788\tOF1: 0.446429\n",
      "Train Epoch: 12 [960/1083 (88%)]\tLoss: 0.679376 \tOP: 0.580000\tOR: 0.483333\tOF1: 0.527273\n",
      "Train Epoch: 12 [992/1083 (91%)]\tLoss: 0.679776 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 12 [1024/1083 (94%)]\tLoss: 0.679686 \tOP: 0.580000\tOR: 0.381579\tOF1: 0.460317\n",
      "Train Epoch: 12 [891/1083 (97%)]\tLoss: 0.678145 \tOP: 0.562500\tOR: 0.421875\tOF1: 0.482143\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6931 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 13 [0/1083 (0%)]\tLoss: 0.681413 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 13 [32/1083 (3%)]\tLoss: 0.681579 \tOP: 0.533333\tOR: 0.320000\tOF1: 0.400000\n",
      "Train Epoch: 13 [64/1083 (6%)]\tLoss: 0.681181 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 13 [96/1083 (9%)]\tLoss: 0.682764 \tOP: 0.511628\tOR: 0.338462\tOF1: 0.407407\n",
      "Train Epoch: 13 [128/1083 (12%)]\tLoss: 0.681871 \tOP: 0.533333\tOR: 0.387097\tOF1: 0.448598\n",
      "Train Epoch: 13 [160/1083 (15%)]\tLoss: 0.680987 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 13 [192/1083 (18%)]\tLoss: 0.681297 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 13 [224/1083 (21%)]\tLoss: 0.679013 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 13 [256/1083 (24%)]\tLoss: 0.686631 \tOP: 0.461538\tOR: 0.285714\tOF1: 0.352941\n",
      "Train Epoch: 13 [288/1083 (26%)]\tLoss: 0.681202 \tOP: 0.543478\tOR: 0.396825\tOF1: 0.458716\n",
      "Train Epoch: 13 [320/1083 (29%)]\tLoss: 0.679857 \tOP: 0.571429\tOR: 0.430769\tOF1: 0.491228\n",
      "Train Epoch: 13 [352/1083 (32%)]\tLoss: 0.680607 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 13 [384/1083 (35%)]\tLoss: 0.681175 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 13 [416/1083 (38%)]\tLoss: 0.679769 \tOP: 0.571429\tOR: 0.368421\tOF1: 0.448000\n",
      "Train Epoch: 13 [448/1083 (41%)]\tLoss: 0.681595 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 13 [480/1083 (44%)]\tLoss: 0.680170 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 13 [512/1083 (47%)]\tLoss: 0.680396 \tOP: 0.562500\tOR: 0.450000\tOF1: 0.500000\n",
      "Train Epoch: 13 [544/1083 (50%)]\tLoss: 0.681805 \tOP: 0.533333\tOR: 0.342857\tOF1: 0.417391\n",
      "Train Epoch: 13 [576/1083 (53%)]\tLoss: 0.681017 \tOP: 0.531915\tOR: 0.324675\tOF1: 0.403226\n",
      "Train Epoch: 13 [608/1083 (56%)]\tLoss: 0.678376 \tOP: 0.596154\tOR: 0.436620\tOF1: 0.504065\n",
      "Train Epoch: 13 [640/1083 (59%)]\tLoss: 0.683138 \tOP: 0.522727\tOR: 0.353846\tOF1: 0.422018\n",
      "Train Epoch: 13 [672/1083 (62%)]\tLoss: 0.680839 \tOP: 0.553191\tOR: 0.412698\tOF1: 0.472727\n",
      "Train Epoch: 13 [704/1083 (65%)]\tLoss: 0.682154 \tOP: 0.522727\tOR: 0.359375\tOF1: 0.425926\n",
      "Train Epoch: 13 [736/1083 (68%)]\tLoss: 0.679472 \tOP: 0.580000\tOR: 0.386667\tOF1: 0.464000\n",
      "Train Epoch: 13 [768/1083 (71%)]\tLoss: 0.680683 \tOP: 0.553191\tOR: 0.356164\tOF1: 0.433333\n",
      "Train Epoch: 13 [800/1083 (74%)]\tLoss: 0.679782 \tOP: 0.560000\tOR: 0.451613\tOF1: 0.500000\n",
      "Train Epoch: 13 [832/1083 (76%)]\tLoss: 0.680642 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 13 [864/1083 (79%)]\tLoss: 0.679879 \tOP: 0.571429\tOR: 0.400000\tOF1: 0.470588\n",
      "Train Epoch: 13 [896/1083 (82%)]\tLoss: 0.684062 \tOP: 0.463415\tOR: 0.271429\tOF1: 0.342342\n",
      "Train Epoch: 13 [928/1083 (85%)]\tLoss: 0.682239 \tOP: 0.511111\tOR: 0.323944\tOF1: 0.396552\n",
      "Train Epoch: 13 [960/1083 (88%)]\tLoss: 0.679794 \tOP: 0.571429\tOR: 0.474576\tOF1: 0.518519\n",
      "Train Epoch: 13 [992/1083 (91%)]\tLoss: 0.679642 \tOP: 0.588235\tOR: 0.416667\tOF1: 0.487805\n",
      "Train Epoch: 13 [1024/1083 (94%)]\tLoss: 0.680716 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 13 [891/1083 (97%)]\tLoss: 0.679789 \tOP: 0.521739\tOR: 0.400000\tOF1: 0.452830\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6918 \n",
      "OP: 0.312500\n",
      "OR: 0.285714\n",
      "OF1: 0.298507\n",
      "\n",
      "Train Epoch: 14 [0/1083 (0%)]\tLoss: 0.681064 \tOP: 0.543478\tOR: 0.352113\tOF1: 0.427350\n",
      "Train Epoch: 14 [32/1083 (3%)]\tLoss: 0.682320 \tOP: 0.511111\tOR: 0.370968\tOF1: 0.429907\n",
      "Train Epoch: 14 [64/1083 (6%)]\tLoss: 0.684336 \tOP: 0.476190\tOR: 0.277778\tOF1: 0.350877\n",
      "Train Epoch: 14 [96/1083 (9%)]\tLoss: 0.680517 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 14 [128/1083 (12%)]\tLoss: 0.679812 \tOP: 0.571429\tOR: 0.430769\tOF1: 0.491228\n",
      "Train Epoch: 14 [160/1083 (15%)]\tLoss: 0.681234 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 14 [192/1083 (18%)]\tLoss: 0.681118 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 14 [224/1083 (21%)]\tLoss: 0.681117 \tOP: 0.543478\tOR: 0.357143\tOF1: 0.431034\n",
      "Train Epoch: 14 [256/1083 (24%)]\tLoss: 0.681755 \tOP: 0.533333\tOR: 0.342857\tOF1: 0.417391\n",
      "Train Epoch: 14 [288/1083 (26%)]\tLoss: 0.679347 \tOP: 0.580000\tOR: 0.362500\tOF1: 0.446154\n",
      "Train Epoch: 14 [320/1083 (29%)]\tLoss: 0.679511 \tOP: 0.580000\tOR: 0.420290\tOF1: 0.487395\n",
      "Train Epoch: 14 [352/1083 (32%)]\tLoss: 0.683176 \tOP: 0.500000\tOR: 0.323077\tOF1: 0.392523\n",
      "Train Epoch: 14 [384/1083 (35%)]\tLoss: 0.682214 \tOP: 0.522727\tOR: 0.348485\tOF1: 0.418182\n",
      "Train Epoch: 14 [416/1083 (38%)]\tLoss: 0.678291 \tOP: 0.596154\tOR: 0.484375\tOF1: 0.534483\n",
      "Train Epoch: 14 [448/1083 (41%)]\tLoss: 0.681126 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 14 [480/1083 (44%)]\tLoss: 0.681975 \tOP: 0.511111\tOR: 0.353846\tOF1: 0.418182\n",
      "Train Epoch: 14 [512/1083 (47%)]\tLoss: 0.685302 \tOP: 0.461538\tOR: 0.260870\tOF1: 0.333333\n",
      "Train Epoch: 14 [544/1083 (50%)]\tLoss: 0.680385 \tOP: 0.562500\tOR: 0.428571\tOF1: 0.486486\n",
      "Train Epoch: 14 [576/1083 (53%)]\tLoss: 0.683999 \tOP: 0.476190\tOR: 0.294118\tOF1: 0.363636\n",
      "Train Epoch: 14 [608/1083 (56%)]\tLoss: 0.678431 \tOP: 0.596154\tOR: 0.455882\tOF1: 0.516667\n",
      "Train Epoch: 14 [640/1083 (59%)]\tLoss: 0.678911 \tOP: 0.580000\tOR: 0.371795\tOF1: 0.453125\n",
      "Train Epoch: 14 [672/1083 (62%)]\tLoss: 0.680594 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 14 [704/1083 (65%)]\tLoss: 0.679049 \tOP: 0.588235\tOR: 0.441176\tOF1: 0.504202\n",
      "Train Epoch: 14 [736/1083 (68%)]\tLoss: 0.680303 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 14 [768/1083 (71%)]\tLoss: 0.679225 \tOP: 0.580000\tOR: 0.453125\tOF1: 0.508772\n",
      "Train Epoch: 14 [800/1083 (74%)]\tLoss: 0.680899 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 14 [832/1083 (76%)]\tLoss: 0.678578 \tOP: 0.588235\tOR: 0.434783\tOF1: 0.500000\n",
      "Train Epoch: 14 [864/1083 (79%)]\tLoss: 0.681925 \tOP: 0.533333\tOR: 0.358209\tOF1: 0.428571\n",
      "Train Epoch: 14 [896/1083 (82%)]\tLoss: 0.680374 \tOP: 0.551020\tOR: 0.442623\tOF1: 0.490909\n",
      "Train Epoch: 14 [928/1083 (85%)]\tLoss: 0.679656 \tOP: 0.568627\tOR: 0.397260\tOF1: 0.467742\n",
      "Train Epoch: 14 [960/1083 (88%)]\tLoss: 0.681105 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 14 [992/1083 (91%)]\tLoss: 0.682288 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 14 [1024/1083 (94%)]\tLoss: 0.678235 \tOP: 0.596154\tOR: 0.436620\tOF1: 0.504065\n",
      "Train Epoch: 14 [891/1083 (97%)]\tLoss: 0.681508 \tOP: 0.487805\tOR: 0.307692\tOF1: 0.377358\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6939 \n",
      "OP: 0.241379\n",
      "OR: 0.200000\n",
      "OF1: 0.218750\n",
      "\n",
      "Train Epoch: 15 [0/1083 (0%)]\tLoss: 0.680587 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 15 [32/1083 (3%)]\tLoss: 0.678432 \tOP: 0.596154\tOR: 0.455882\tOF1: 0.516667\n",
      "Train Epoch: 15 [64/1083 (6%)]\tLoss: 0.680596 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 15 [96/1083 (9%)]\tLoss: 0.679476 \tOP: 0.580000\tOR: 0.432836\tOF1: 0.495726\n",
      "Train Epoch: 15 [128/1083 (12%)]\tLoss: 0.687027 \tOP: 0.435897\tOR: 0.288136\tOF1: 0.346939\n",
      "Train Epoch: 15 [160/1083 (15%)]\tLoss: 0.681202 \tOP: 0.553191\tOR: 0.406250\tOF1: 0.468468\n",
      "Train Epoch: 15 [192/1083 (18%)]\tLoss: 0.681765 \tOP: 0.533333\tOR: 0.393443\tOF1: 0.452830\n",
      "Train Epoch: 15 [224/1083 (21%)]\tLoss: 0.681299 \tOP: 0.553191\tOR: 0.412698\tOF1: 0.472727\n",
      "Train Epoch: 15 [256/1083 (24%)]\tLoss: 0.678575 \tOP: 0.588235\tOR: 0.447761\tOF1: 0.508475\n",
      "Train Epoch: 15 [288/1083 (26%)]\tLoss: 0.680390 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 15 [320/1083 (29%)]\tLoss: 0.679124 \tOP: 0.580000\tOR: 0.386667\tOF1: 0.464000\n",
      "Train Epoch: 15 [352/1083 (32%)]\tLoss: 0.683151 \tOP: 0.500000\tOR: 0.313433\tOF1: 0.385321\n",
      "Train Epoch: 15 [384/1083 (35%)]\tLoss: 0.681225 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 15 [416/1083 (38%)]\tLoss: 0.682257 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 15 [448/1083 (41%)]\tLoss: 0.680080 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 15 [480/1083 (44%)]\tLoss: 0.680086 \tOP: 0.562500\tOR: 0.380282\tOF1: 0.453782\n",
      "Train Epoch: 15 [512/1083 (47%)]\tLoss: 0.678068 \tOP: 0.596154\tOR: 0.455882\tOF1: 0.516667\n",
      "Train Epoch: 15 [544/1083 (50%)]\tLoss: 0.678671 \tOP: 0.588235\tOR: 0.434783\tOF1: 0.500000\n",
      "Train Epoch: 15 [576/1083 (53%)]\tLoss: 0.681594 \tOP: 0.521739\tOR: 0.338028\tOF1: 0.410256\n",
      "Train Epoch: 15 [608/1083 (56%)]\tLoss: 0.681754 \tOP: 0.533333\tOR: 0.347826\tOF1: 0.421053\n",
      "Train Epoch: 15 [640/1083 (59%)]\tLoss: 0.680838 \tOP: 0.541667\tOR: 0.366197\tOF1: 0.436975\n",
      "Train Epoch: 15 [672/1083 (62%)]\tLoss: 0.680327 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 15 [704/1083 (65%)]\tLoss: 0.684475 \tOP: 0.500000\tOR: 0.328358\tOF1: 0.396396\n",
      "Train Epoch: 15 [736/1083 (68%)]\tLoss: 0.679259 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 15 [768/1083 (71%)]\tLoss: 0.682567 \tOP: 0.522727\tOR: 0.315068\tOF1: 0.393162\n",
      "Train Epoch: 15 [800/1083 (74%)]\tLoss: 0.682131 \tOP: 0.511111\tOR: 0.348485\tOF1: 0.414414\n",
      "Train Epoch: 15 [832/1083 (76%)]\tLoss: 0.680199 \tOP: 0.562500\tOR: 0.428571\tOF1: 0.486486\n",
      "Train Epoch: 15 [864/1083 (79%)]\tLoss: 0.680598 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 15 [896/1083 (82%)]\tLoss: 0.680752 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 15 [928/1083 (85%)]\tLoss: 0.680743 \tOP: 0.541667\tOR: 0.361111\tOF1: 0.433333\n",
      "Train Epoch: 15 [960/1083 (88%)]\tLoss: 0.681863 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 15 [992/1083 (91%)]\tLoss: 0.680673 \tOP: 0.553191\tOR: 0.406250\tOF1: 0.468468\n",
      "Train Epoch: 15 [1024/1083 (94%)]\tLoss: 0.679771 \tOP: 0.571429\tOR: 0.405797\tOF1: 0.474576\n",
      "Train Epoch: 15 [891/1083 (97%)]\tLoss: 0.678361 \tOP: 0.553191\tOR: 0.464286\tOF1: 0.504854\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6925 \n",
      "OP: 0.312500\n",
      "OR: 0.285714\n",
      "OF1: 0.298507\n",
      "\n",
      "Train Epoch: 16 [0/1083 (0%)]\tLoss: 0.682872 \tOP: 0.511628\tOR: 0.338462\tOF1: 0.407407\n",
      "Train Epoch: 16 [32/1083 (3%)]\tLoss: 0.679516 \tOP: 0.571429\tOR: 0.405797\tOF1: 0.474576\n",
      "Train Epoch: 16 [64/1083 (6%)]\tLoss: 0.682939 \tOP: 0.511628\tOR: 0.309859\tOF1: 0.385965\n",
      "Train Epoch: 16 [96/1083 (9%)]\tLoss: 0.678819 \tOP: 0.588235\tOR: 0.441176\tOF1: 0.504202\n",
      "Train Epoch: 16 [128/1083 (12%)]\tLoss: 0.681053 \tOP: 0.553191\tOR: 0.371429\tOF1: 0.444444\n",
      "Train Epoch: 16 [160/1083 (15%)]\tLoss: 0.680725 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 16 [192/1083 (18%)]\tLoss: 0.679006 \tOP: 0.588235\tOR: 0.405405\tOF1: 0.480000\n",
      "Train Epoch: 16 [224/1083 (21%)]\tLoss: 0.681108 \tOP: 0.531915\tOR: 0.352113\tOF1: 0.423729\n",
      "Train Epoch: 16 [256/1083 (24%)]\tLoss: 0.679698 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 16 [288/1083 (26%)]\tLoss: 0.682308 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 16 [320/1083 (29%)]\tLoss: 0.682046 \tOP: 0.522727\tOR: 0.323944\tOF1: 0.400000\n",
      "Train Epoch: 16 [352/1083 (32%)]\tLoss: 0.678857 \tOP: 0.588235\tOR: 0.434783\tOF1: 0.500000\n",
      "Train Epoch: 16 [384/1083 (35%)]\tLoss: 0.680831 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 16 [416/1083 (38%)]\tLoss: 0.682605 \tOP: 0.500000\tOR: 0.366667\tOF1: 0.423077\n",
      "Train Epoch: 16 [448/1083 (41%)]\tLoss: 0.678802 \tOP: 0.588235\tOR: 0.416667\tOF1: 0.487805\n",
      "Train Epoch: 16 [480/1083 (44%)]\tLoss: 0.682043 \tOP: 0.511111\tOR: 0.319444\tOF1: 0.393162\n",
      "Train Epoch: 16 [512/1083 (47%)]\tLoss: 0.680054 \tOP: 0.562500\tOR: 0.355263\tOF1: 0.435484\n",
      "Train Epoch: 16 [544/1083 (50%)]\tLoss: 0.680681 \tOP: 0.543478\tOR: 0.367647\tOF1: 0.438596\n",
      "Train Epoch: 16 [576/1083 (53%)]\tLoss: 0.684667 \tOP: 0.475000\tOR: 0.322034\tOF1: 0.383838\n",
      "Train Epoch: 16 [608/1083 (56%)]\tLoss: 0.678659 \tOP: 0.588235\tOR: 0.441176\tOF1: 0.504202\n",
      "Train Epoch: 16 [640/1083 (59%)]\tLoss: 0.682972 \tOP: 0.511628\tOR: 0.328358\tOF1: 0.400000\n",
      "Train Epoch: 16 [672/1083 (62%)]\tLoss: 0.682453 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 16 [704/1083 (65%)]\tLoss: 0.680630 \tOP: 0.541667\tOR: 0.388060\tOF1: 0.452174\n",
      "Train Epoch: 16 [736/1083 (68%)]\tLoss: 0.683989 \tOP: 0.543478\tOR: 0.390625\tOF1: 0.454545\n",
      "Train Epoch: 16 [768/1083 (71%)]\tLoss: 0.677302 \tOP: 0.611111\tOR: 0.578947\tOF1: 0.594595\n",
      "Train Epoch: 16 [800/1083 (74%)]\tLoss: 0.679641 \tOP: 0.571429\tOR: 0.411765\tOF1: 0.478632\n",
      "Train Epoch: 16 [832/1083 (76%)]\tLoss: 0.678441 \tOP: 0.588235\tOR: 0.428571\tOF1: 0.495868\n",
      "Train Epoch: 16 [864/1083 (79%)]\tLoss: 0.682025 \tOP: 0.511111\tOR: 0.370968\tOF1: 0.429907\n",
      "Train Epoch: 16 [896/1083 (82%)]\tLoss: 0.680738 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 16 [928/1083 (85%)]\tLoss: 0.681152 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 16 [960/1083 (88%)]\tLoss: 0.677629 \tOP: 0.603774\tOR: 0.470588\tOF1: 0.528926\n",
      "Train Epoch: 16 [992/1083 (91%)]\tLoss: 0.681230 \tOP: 0.531915\tOR: 0.357143\tOF1: 0.427350\n",
      "Train Epoch: 16 [1024/1083 (94%)]\tLoss: 0.682407 \tOP: 0.521739\tOR: 0.352941\tOF1: 0.421053\n",
      "Train Epoch: 16 [891/1083 (97%)]\tLoss: 0.682534 \tOP: 0.475000\tOR: 0.322034\tOF1: 0.383838\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6936 \n",
      "OP: 0.312500\n",
      "OR: 0.285714\n",
      "OF1: 0.298507\n",
      "\n",
      "Train Epoch: 17 [0/1083 (0%)]\tLoss: 0.682521 \tOP: 0.522727\tOR: 0.315068\tOF1: 0.393162\n",
      "Train Epoch: 17 [32/1083 (3%)]\tLoss: 0.680128 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 17 [64/1083 (6%)]\tLoss: 0.678896 \tOP: 0.580000\tOR: 0.362500\tOF1: 0.446154\n",
      "Train Epoch: 17 [96/1083 (9%)]\tLoss: 0.683583 \tOP: 0.500000\tOR: 0.318182\tOF1: 0.388889\n",
      "Train Epoch: 17 [128/1083 (12%)]\tLoss: 0.685512 \tOP: 0.475000\tOR: 0.267606\tOF1: 0.342342\n",
      "Train Epoch: 17 [160/1083 (15%)]\tLoss: 0.681137 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 17 [192/1083 (18%)]\tLoss: 0.677471 \tOP: 0.611111\tOR: 0.452055\tOF1: 0.519685\n",
      "Train Epoch: 17 [224/1083 (21%)]\tLoss: 0.679698 \tOP: 0.560000\tOR: 0.424242\tOF1: 0.482759\n",
      "Train Epoch: 17 [256/1083 (24%)]\tLoss: 0.683883 \tOP: 0.511628\tOR: 0.360656\tOF1: 0.423077\n",
      "Train Epoch: 17 [288/1083 (26%)]\tLoss: 0.677191 \tOP: 0.618182\tOR: 0.453333\tOF1: 0.523077\n",
      "Train Epoch: 17 [320/1083 (29%)]\tLoss: 0.681659 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 17 [352/1083 (32%)]\tLoss: 0.680421 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 17 [384/1083 (35%)]\tLoss: 0.679259 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 17 [416/1083 (38%)]\tLoss: 0.683930 \tOP: 0.511628\tOR: 0.318841\tOF1: 0.392857\n",
      "Train Epoch: 17 [448/1083 (41%)]\tLoss: 0.681687 \tOP: 0.533333\tOR: 0.393443\tOF1: 0.452830\n",
      "Train Epoch: 17 [480/1083 (44%)]\tLoss: 0.679074 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 17 [512/1083 (47%)]\tLoss: 0.682134 \tOP: 0.531915\tOR: 0.384615\tOF1: 0.446429\n",
      "Train Epoch: 17 [544/1083 (50%)]\tLoss: 0.679947 \tOP: 0.571429\tOR: 0.437500\tOF1: 0.495575\n",
      "Train Epoch: 17 [576/1083 (53%)]\tLoss: 0.680969 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 17 [608/1083 (56%)]\tLoss: 0.683436 \tOP: 0.487805\tOR: 0.333333\tOF1: 0.396040\n",
      "Train Epoch: 17 [640/1083 (59%)]\tLoss: 0.682705 \tOP: 0.500000\tOR: 0.349206\tOF1: 0.411215\n",
      "Train Epoch: 17 [672/1083 (62%)]\tLoss: 0.680649 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 17 [704/1083 (65%)]\tLoss: 0.680292 \tOP: 0.551020\tOR: 0.421875\tOF1: 0.477876\n",
      "Train Epoch: 17 [736/1083 (68%)]\tLoss: 0.681226 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 17 [768/1083 (71%)]\tLoss: 0.683691 \tOP: 0.533333\tOR: 0.352941\tOF1: 0.424779\n",
      "Train Epoch: 17 [800/1083 (74%)]\tLoss: 0.680217 \tOP: 0.562500\tOR: 0.380282\tOF1: 0.453782\n",
      "Train Epoch: 17 [832/1083 (76%)]\tLoss: 0.680131 \tOP: 0.580000\tOR: 0.453125\tOF1: 0.508772\n",
      "Train Epoch: 17 [864/1083 (79%)]\tLoss: 0.678538 \tOP: 0.588235\tOR: 0.447761\tOF1: 0.508475\n",
      "Train Epoch: 17 [896/1083 (82%)]\tLoss: 0.679874 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 17 [928/1083 (85%)]\tLoss: 0.683130 \tOP: 0.500000\tOR: 0.318182\tOF1: 0.388889\n",
      "Train Epoch: 17 [960/1083 (88%)]\tLoss: 0.682391 \tOP: 0.522727\tOR: 0.359375\tOF1: 0.425926\n",
      "Train Epoch: 17 [992/1083 (91%)]\tLoss: 0.680690 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 17 [1024/1083 (94%)]\tLoss: 0.679929 \tOP: 0.571429\tOR: 0.424242\tOF1: 0.486957\n",
      "Train Epoch: 17 [891/1083 (97%)]\tLoss: 0.680913 \tOP: 0.500000\tOR: 0.385965\tOF1: 0.435644\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6935 \n",
      "OP: 0.312500\n",
      "OR: 0.285714\n",
      "OF1: 0.298507\n",
      "\n",
      "Train Epoch: 18 [0/1083 (0%)]\tLoss: 0.679374 \tOP: 0.571429\tOR: 0.430769\tOF1: 0.491228\n",
      "Train Epoch: 18 [32/1083 (3%)]\tLoss: 0.681675 \tOP: 0.533333\tOR: 0.352941\tOF1: 0.424779\n",
      "Train Epoch: 18 [64/1083 (6%)]\tLoss: 0.680336 \tOP: 0.562500\tOR: 0.360000\tOF1: 0.439024\n",
      "Train Epoch: 18 [96/1083 (9%)]\tLoss: 0.680053 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 18 [128/1083 (12%)]\tLoss: 0.682036 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 18 [160/1083 (15%)]\tLoss: 0.683741 \tOP: 0.488372\tOR: 0.350000\tOF1: 0.407767\n",
      "Train Epoch: 18 [192/1083 (18%)]\tLoss: 0.680561 \tOP: 0.541667\tOR: 0.376812\tOF1: 0.444444\n",
      "Train Epoch: 18 [224/1083 (21%)]\tLoss: 0.678702 \tOP: 0.576923\tOR: 0.405405\tOF1: 0.476190\n",
      "Train Epoch: 18 [256/1083 (24%)]\tLoss: 0.680104 \tOP: 0.571429\tOR: 0.417910\tOF1: 0.482759\n",
      "Train Epoch: 18 [288/1083 (26%)]\tLoss: 0.680127 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 18 [320/1083 (29%)]\tLoss: 0.680517 \tOP: 0.541667\tOR: 0.426230\tOF1: 0.477064\n",
      "Train Epoch: 18 [352/1083 (32%)]\tLoss: 0.680049 \tOP: 0.551020\tOR: 0.428571\tOF1: 0.482143\n",
      "Train Epoch: 18 [384/1083 (35%)]\tLoss: 0.679216 \tOP: 0.560000\tOR: 0.383562\tOF1: 0.455285\n",
      "Train Epoch: 18 [416/1083 (38%)]\tLoss: 0.680346 \tOP: 0.562500\tOR: 0.402985\tOF1: 0.469565\n",
      "Train Epoch: 18 [448/1083 (41%)]\tLoss: 0.680698 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 18 [480/1083 (44%)]\tLoss: 0.682702 \tOP: 0.522727\tOR: 0.343284\tOF1: 0.414414\n",
      "Train Epoch: 18 [512/1083 (47%)]\tLoss: 0.682494 \tOP: 0.533333\tOR: 0.352941\tOF1: 0.424779\n",
      "Train Epoch: 18 [544/1083 (50%)]\tLoss: 0.680124 \tOP: 0.560000\tOR: 0.459016\tOF1: 0.504505\n",
      "Train Epoch: 18 [576/1083 (53%)]\tLoss: 0.681063 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 18 [608/1083 (56%)]\tLoss: 0.680562 \tOP: 0.553191\tOR: 0.393939\tOF1: 0.460177\n",
      "Train Epoch: 18 [640/1083 (59%)]\tLoss: 0.678911 \tOP: 0.580000\tOR: 0.460317\tOF1: 0.513274\n",
      "Train Epoch: 18 [672/1083 (62%)]\tLoss: 0.680770 \tOP: 0.541667\tOR: 0.325000\tOF1: 0.406250\n",
      "Train Epoch: 18 [704/1083 (65%)]\tLoss: 0.681207 \tOP: 0.543478\tOR: 0.352113\tOF1: 0.427350\n",
      "Train Epoch: 18 [736/1083 (68%)]\tLoss: 0.680868 \tOP: 0.553191\tOR: 0.342105\tOF1: 0.422764\n",
      "Train Epoch: 18 [768/1083 (71%)]\tLoss: 0.677902 \tOP: 0.603774\tOR: 0.457143\tOF1: 0.520325\n",
      "Train Epoch: 18 [800/1083 (74%)]\tLoss: 0.684432 \tOP: 0.475000\tOR: 0.279412\tOF1: 0.351852\n",
      "Train Epoch: 18 [832/1083 (76%)]\tLoss: 0.682707 \tOP: 0.522727\tOR: 0.343284\tOF1: 0.414414\n",
      "Train Epoch: 18 [864/1083 (79%)]\tLoss: 0.679459 \tOP: 0.580000\tOR: 0.432836\tOF1: 0.495726\n",
      "Train Epoch: 18 [896/1083 (82%)]\tLoss: 0.680815 \tOP: 0.541667\tOR: 0.388060\tOF1: 0.452174\n",
      "Train Epoch: 18 [928/1083 (85%)]\tLoss: 0.682945 \tOP: 0.511111\tOR: 0.348485\tOF1: 0.414414\n",
      "Train Epoch: 18 [960/1083 (88%)]\tLoss: 0.683618 \tOP: 0.500000\tOR: 0.338710\tOF1: 0.403846\n",
      "Train Epoch: 18 [992/1083 (91%)]\tLoss: 0.681540 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 18 [1024/1083 (94%)]\tLoss: 0.679835 \tOP: 0.568627\tOR: 0.432836\tOF1: 0.491525\n",
      "Train Epoch: 18 [891/1083 (97%)]\tLoss: 0.681425 \tOP: 0.500000\tOR: 0.344262\tOF1: 0.407767\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6930 \n",
      "OP: 0.290323\n",
      "OR: 0.257143\n",
      "OF1: 0.272727\n",
      "\n",
      "Train Epoch: 19 [0/1083 (0%)]\tLoss: 0.679281 \tOP: 0.580000\tOR: 0.446154\tOF1: 0.504348\n",
      "Train Epoch: 19 [32/1083 (3%)]\tLoss: 0.679240 \tOP: 0.580000\tOR: 0.408451\tOF1: 0.479339\n",
      "Train Epoch: 19 [64/1083 (6%)]\tLoss: 0.680537 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 19 [96/1083 (9%)]\tLoss: 0.680571 \tOP: 0.553191\tOR: 0.342105\tOF1: 0.422764\n",
      "Train Epoch: 19 [128/1083 (12%)]\tLoss: 0.679777 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 19 [160/1083 (15%)]\tLoss: 0.682959 \tOP: 0.522727\tOR: 0.365079\tOF1: 0.429907\n",
      "Train Epoch: 19 [192/1083 (18%)]\tLoss: 0.680164 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 19 [224/1083 (21%)]\tLoss: 0.681358 \tOP: 0.543478\tOR: 0.403226\tOF1: 0.462963\n",
      "Train Epoch: 19 [256/1083 (24%)]\tLoss: 0.685343 \tOP: 0.461538\tOR: 0.240000\tOF1: 0.315789\n",
      "Train Epoch: 19 [288/1083 (26%)]\tLoss: 0.680184 \tOP: 0.560000\tOR: 0.437500\tOF1: 0.491228\n",
      "Train Epoch: 19 [320/1083 (29%)]\tLoss: 0.679542 \tOP: 0.571429\tOR: 0.378378\tOF1: 0.455285\n",
      "Train Epoch: 19 [352/1083 (32%)]\tLoss: 0.680531 \tOP: 0.553191\tOR: 0.412698\tOF1: 0.472727\n",
      "Train Epoch: 19 [384/1083 (35%)]\tLoss: 0.680732 \tOP: 0.553191\tOR: 0.356164\tOF1: 0.433333\n",
      "Train Epoch: 19 [416/1083 (38%)]\tLoss: 0.679295 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 19 [448/1083 (41%)]\tLoss: 0.679093 \tOP: 0.580000\tOR: 0.414286\tOF1: 0.483333\n",
      "Train Epoch: 19 [480/1083 (44%)]\tLoss: 0.684302 \tOP: 0.500000\tOR: 0.300000\tOF1: 0.375000\n",
      "Train Epoch: 19 [512/1083 (47%)]\tLoss: 0.681747 \tOP: 0.521739\tOR: 0.380952\tOF1: 0.440367\n",
      "Train Epoch: 19 [544/1083 (50%)]\tLoss: 0.679486 \tOP: 0.568627\tOR: 0.386667\tOF1: 0.460317\n",
      "Train Epoch: 19 [576/1083 (53%)]\tLoss: 0.681099 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 19 [608/1083 (56%)]\tLoss: 0.681565 \tOP: 0.533333\tOR: 0.387097\tOF1: 0.448598\n",
      "Train Epoch: 19 [640/1083 (59%)]\tLoss: 0.679077 \tOP: 0.580000\tOR: 0.491525\tOF1: 0.532110\n",
      "Train Epoch: 19 [672/1083 (62%)]\tLoss: 0.681155 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 19 [704/1083 (65%)]\tLoss: 0.679092 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 19 [736/1083 (68%)]\tLoss: 0.680117 \tOP: 0.562500\tOR: 0.355263\tOF1: 0.435484\n",
      "Train Epoch: 19 [768/1083 (71%)]\tLoss: 0.682245 \tOP: 0.522727\tOR: 0.353846\tOF1: 0.422018\n",
      "Train Epoch: 19 [800/1083 (74%)]\tLoss: 0.680888 \tOP: 0.543478\tOR: 0.390625\tOF1: 0.454545\n",
      "Train Epoch: 19 [832/1083 (76%)]\tLoss: 0.677993 \tOP: 0.603774\tOR: 0.410256\tOF1: 0.488550\n",
      "Train Epoch: 19 [864/1083 (79%)]\tLoss: 0.680790 \tOP: 0.562500\tOR: 0.397059\tOF1: 0.465517\n",
      "Train Epoch: 19 [896/1083 (82%)]\tLoss: 0.679951 \tOP: 0.562500\tOR: 0.442623\tOF1: 0.495413\n",
      "Train Epoch: 19 [928/1083 (85%)]\tLoss: 0.682731 \tOP: 0.511628\tOR: 0.323529\tOF1: 0.396396\n",
      "Train Epoch: 19 [960/1083 (88%)]\tLoss: 0.679507 \tOP: 0.571429\tOR: 0.459016\tOF1: 0.509091\n",
      "Train Epoch: 19 [992/1083 (91%)]\tLoss: 0.679456 \tOP: 0.580000\tOR: 0.376623\tOF1: 0.456693\n",
      "Train Epoch: 19 [1024/1083 (94%)]\tLoss: 0.680933 \tOP: 0.553191\tOR: 0.342105\tOF1: 0.422764\n",
      "Train Epoch: 19 [891/1083 (97%)]\tLoss: 0.681256 \tOP: 0.488372\tOR: 0.428571\tOF1: 0.456522\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6927 \n",
      "OP: 0.266667\n",
      "OR: 0.228571\n",
      "OF1: 0.246154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model3.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model3(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'voc07')\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model3.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model3(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "        \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for n in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for m in range(len(classes)):\n",
    "        #        if target[n][m] == 1 or preds[n][m] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[n][m] == target[n][m]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'voc07')\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
