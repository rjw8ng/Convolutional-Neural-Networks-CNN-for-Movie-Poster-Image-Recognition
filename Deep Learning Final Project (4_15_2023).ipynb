{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "from torch.nn.modules import MSELoss, L1Loss, BCELoss\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "from numpy import array, asarray, ndarray, swapaxes\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training controls\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "training_size = 0.7\n",
    "learning_rate = 0.001\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 268, 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data holders\n",
    "x_test = []\n",
    "x_train = []\n",
    "y_test= []\n",
    "y_train= []\n",
    "tempY = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdbId', 'Imdb Link', 'Title', 'IMDB Score', 'Genre', 'Poster']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opening the dataset\n",
    "dataset = csv.reader(open(\"MovieGenre415.csv\",encoding=\"utf8\",errors='replace'), delimiter=\",\")\n",
    "\n",
    "# skipping the header line\n",
    "next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract images from zip folder\n",
    "\n",
    "import zipfile as zf\n",
    "\n",
    "files = zf.ZipFile(\"FinalProject-20230415T194533Z-001.zip\", 'r')\n",
    "files.extractall()\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of image files in SampleMoviePosters folder\n",
    "flist=glob.glob('FinalProject/*.jpg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5347"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = []\n",
    "\n",
    "for path in flist:\n",
    "    start = path.rfind(\"/\")+1\n",
    "    end = len(path)-4\n",
    "    image_ids.append(path[start:end])\n",
    "    \n",
    "#image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114885</td>\n",
       "      <td>http://www.imdb.com/title/tt114885</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112281</td>\n",
       "      <td>http://www.imdb.com/title/tt112281</td>\n",
       "      <td>Ace Ventura: When Nature Calls (1995)</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Adventure|Comedy|Crime</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114168</td>\n",
       "      <td>http://www.imdb.com/title/tt114168</td>\n",
       "      <td>Powder (1995)</td>\n",
       "      <td>6.5</td>\n",
       "      <td>Drama|Fantasy|Mystery</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114011</td>\n",
       "      <td>http://www.imdb.com/title/tt114011</td>\n",
       "      <td>Now and Then (1995)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114117</td>\n",
       "      <td>http://www.imdb.com/title/tt114117</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Drama</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5342</th>\n",
       "      <td>313579</td>\n",
       "      <td>http://www.imdb.com/title/tt313579</td>\n",
       "      <td>Seung joi ngo sam (2001)</td>\n",
       "      <td>6.6</td>\n",
       "      <td>Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5343</th>\n",
       "      <td>4361050</td>\n",
       "      <td>http://www.imdb.com/title/tt4361050</td>\n",
       "      <td>Ouija: Origin of Evil (2016)</td>\n",
       "      <td>6.1</td>\n",
       "      <td>Horror|Thriller</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>1337139</td>\n",
       "      <td>http://www.imdb.com/title/tt1337139</td>\n",
       "      <td>45365 (2009)</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5345</th>\n",
       "      <td>47495</td>\n",
       "      <td>http://www.imdb.com/title/tt47495</td>\n",
       "      <td>Silver Lode (1954)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>2145909</td>\n",
       "      <td>http://www.imdb.com/title/tt2145909</td>\n",
       "      <td>The Forgotten (2014)</td>\n",
       "      <td>5.5</td>\n",
       "      <td>Horror</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5347 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       imdbId                            Imdb Link  \\\n",
       "0      114885   http://www.imdb.com/title/tt114885   \n",
       "1      112281   http://www.imdb.com/title/tt112281   \n",
       "2      114168   http://www.imdb.com/title/tt114168   \n",
       "3      114011   http://www.imdb.com/title/tt114011   \n",
       "4      114117   http://www.imdb.com/title/tt114117   \n",
       "...       ...                                  ...   \n",
       "5342   313579   http://www.imdb.com/title/tt313579   \n",
       "5343  4361050  http://www.imdb.com/title/tt4361050   \n",
       "5344  1337139  http://www.imdb.com/title/tt1337139   \n",
       "5345    47495    http://www.imdb.com/title/tt47495   \n",
       "5346  2145909  http://www.imdb.com/title/tt2145909   \n",
       "\n",
       "                                      Title  IMDB Score  \\\n",
       "0                  Waiting to Exhale (1995)         5.7   \n",
       "1     Ace Ventura: When Nature Calls (1995)         6.3   \n",
       "2                             Powder (1995)         6.5   \n",
       "3                       Now and Then (1995)         6.8   \n",
       "4                                Persuasion         7.7   \n",
       "...                                     ...         ...   \n",
       "5342               Seung joi ngo sam (2001)         6.6   \n",
       "5343           Ouija: Origin of Evil (2016)         6.1   \n",
       "5344                           45365 (2009)         7.3   \n",
       "5345                     Silver Lode (1954)         7.0   \n",
       "5346                   The Forgotten (2014)         5.5   \n",
       "\n",
       "                       Genre  \\\n",
       "0       Comedy|Drama|Romance   \n",
       "1     Adventure|Comedy|Crime   \n",
       "2      Drama|Fantasy|Mystery   \n",
       "3       Comedy|Drama|Romance   \n",
       "4                      Drama   \n",
       "...                      ...   \n",
       "5342                 Romance   \n",
       "5343         Horror|Thriller   \n",
       "5344             Documentary   \n",
       "5345                 Western   \n",
       "5346                  Horror   \n",
       "\n",
       "                                                 Poster  \n",
       "0     https://images-na.ssl-images-amazon.com/images...  \n",
       "1     https://images-na.ssl-images-amazon.com/images...  \n",
       "2     https://images-na.ssl-images-amazon.com/images...  \n",
       "3     https://images-na.ssl-images-amazon.com/images...  \n",
       "4     https://images-na.ssl-images-amazon.com/images...  \n",
       "...                                                 ...  \n",
       "5342  https://images-na.ssl-images-amazon.com/images...  \n",
       "5343  https://images-na.ssl-images-amazon.com/images...  \n",
       "5344  https://images-na.ssl-images-amazon.com/images...  \n",
       "5345  https://images-na.ssl-images-amazon.com/images...  \n",
       "5346  https://images-na.ssl-images-amazon.com/images...  \n",
       "\n",
       "[5347 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset2 = pd.read_csv(\"MovieGenre415.csv\")\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "indexlist = []\n",
    "classes = tuple()\n",
    "ids = dataset2.imdbId.values.tolist()\n",
    "for image_id in image_ids:\n",
    "    #print(dataset2[\"imdbId\"])\n",
    "    genres = tuple((dataset2[dataset2[\"imdbId\"] == int(image_id)][\"Genre\"].values[0]).split(\"|\"))\n",
    "    if int(image_id) in ids:\n",
    "        indexlist.append(image_id)\n",
    "    y.append(genres)\n",
    "    classes = classes + genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y)\n",
    "y = mlb.transform(y)\n",
    "classes = set(classes)\n",
    "classes = list(classes)\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>...</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>News</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Talk-Show</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408839</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240468</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116409</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106317</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331846</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43386</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5777628</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5347 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Action  Adult  Adventure  Animation  Biography  Comedy  Crime  \\\n",
       "408839        0      0          0          0          0       1      0   \n",
       "240468        1      0          0          0          0       1      0   \n",
       "387131        0      0          0          0          0       0      0   \n",
       "116409        0      0          1          0          0       0      0   \n",
       "106317        0      0          0          0          0       1      0   \n",
       "...         ...    ...        ...        ...        ...     ...    ...   \n",
       "3331846       0      0          0          1          0       1      0   \n",
       "108160        0      0          0          0          0       1      0   \n",
       "2247566       0      0          0          0          1       0      0   \n",
       "43386         0      0          0          0          0       0      1   \n",
       "5777628       0      0          0          0          0       0      0   \n",
       "\n",
       "         Documentary  Drama  Family  ...  Mystery  News  Romance  Sci-Fi  \\\n",
       "408839             0      0       0  ...        0     0        1       0   \n",
       "240468             0      0       0  ...        0     0        0       0   \n",
       "387131             0      1       0  ...        1     0        1       0   \n",
       "116409             0      1       0  ...        0     0        0       0   \n",
       "106317             0      0       0  ...        0     0        0       1   \n",
       "...              ...    ...     ...  ...      ...   ...      ...     ...   \n",
       "3331846            0      1       0  ...        0     0        0       0   \n",
       "108160             0      1       0  ...        0     0        1       0   \n",
       "2247566            0      1       0  ...        0     0        0       0   \n",
       "43386              0      1       0  ...        0     0        1       0   \n",
       "5777628            1      0       0  ...        0     0        0       0   \n",
       "\n",
       "         Short  Sport  Talk-Show  Thriller  War  Western  \n",
       "408839       0      0          0         0    0        0  \n",
       "240468       0      0          0         0    0        0  \n",
       "387131       0      0          0         0    0        0  \n",
       "116409       0      0          0         0    0        0  \n",
       "106317       0      0          0         0    0        0  \n",
       "...        ...    ...        ...       ...  ...      ...  \n",
       "3331846      0      0          0         0    0        0  \n",
       "108160       0      0          0         0    0        0  \n",
       "2247566      0      0          0         0    0        0  \n",
       "43386        0      0          0         0    0        0  \n",
       "5777628      0      0          0         0    0        0  \n",
       "\n",
       "[5347 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame(y, columns = classes, index = indexlist)\n",
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "      <th>genrelst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114885</td>\n",
       "      <td>http://www.imdb.com/title/tt114885</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112281</td>\n",
       "      <td>http://www.imdb.com/title/tt112281</td>\n",
       "      <td>Ace Ventura: When Nature Calls (1995)</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Adventure|Comedy|Crime</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114168</td>\n",
       "      <td>http://www.imdb.com/title/tt114168</td>\n",
       "      <td>Powder (1995)</td>\n",
       "      <td>6.5</td>\n",
       "      <td>Drama|Fantasy|Mystery</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114011</td>\n",
       "      <td>http://www.imdb.com/title/tt114011</td>\n",
       "      <td>Now and Then (1995)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114117</td>\n",
       "      <td>http://www.imdb.com/title/tt114117</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Drama</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5342</th>\n",
       "      <td>313579</td>\n",
       "      <td>http://www.imdb.com/title/tt313579</td>\n",
       "      <td>Seung joi ngo sam (2001)</td>\n",
       "      <td>6.6</td>\n",
       "      <td>Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5343</th>\n",
       "      <td>4361050</td>\n",
       "      <td>http://www.imdb.com/title/tt4361050</td>\n",
       "      <td>Ouija: Origin of Evil (2016)</td>\n",
       "      <td>6.1</td>\n",
       "      <td>Horror|Thriller</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>1337139</td>\n",
       "      <td>http://www.imdb.com/title/tt1337139</td>\n",
       "      <td>45365 (2009)</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5345</th>\n",
       "      <td>47495</td>\n",
       "      <td>http://www.imdb.com/title/tt47495</td>\n",
       "      <td>Silver Lode (1954)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>2145909</td>\n",
       "      <td>http://www.imdb.com/title/tt2145909</td>\n",
       "      <td>The Forgotten (2014)</td>\n",
       "      <td>5.5</td>\n",
       "      <td>Horror</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5347 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       imdbId                            Imdb Link  \\\n",
       "0      114885   http://www.imdb.com/title/tt114885   \n",
       "1      112281   http://www.imdb.com/title/tt112281   \n",
       "2      114168   http://www.imdb.com/title/tt114168   \n",
       "3      114011   http://www.imdb.com/title/tt114011   \n",
       "4      114117   http://www.imdb.com/title/tt114117   \n",
       "...       ...                                  ...   \n",
       "5342   313579   http://www.imdb.com/title/tt313579   \n",
       "5343  4361050  http://www.imdb.com/title/tt4361050   \n",
       "5344  1337139  http://www.imdb.com/title/tt1337139   \n",
       "5345    47495    http://www.imdb.com/title/tt47495   \n",
       "5346  2145909  http://www.imdb.com/title/tt2145909   \n",
       "\n",
       "                                      Title  IMDB Score  \\\n",
       "0                  Waiting to Exhale (1995)         5.7   \n",
       "1     Ace Ventura: When Nature Calls (1995)         6.3   \n",
       "2                             Powder (1995)         6.5   \n",
       "3                       Now and Then (1995)         6.8   \n",
       "4                                Persuasion         7.7   \n",
       "...                                     ...         ...   \n",
       "5342               Seung joi ngo sam (2001)         6.6   \n",
       "5343           Ouija: Origin of Evil (2016)         6.1   \n",
       "5344                           45365 (2009)         7.3   \n",
       "5345                     Silver Lode (1954)         7.0   \n",
       "5346                   The Forgotten (2014)         5.5   \n",
       "\n",
       "                       Genre  \\\n",
       "0       Comedy|Drama|Romance   \n",
       "1     Adventure|Comedy|Crime   \n",
       "2      Drama|Fantasy|Mystery   \n",
       "3       Comedy|Drama|Romance   \n",
       "4                      Drama   \n",
       "...                      ...   \n",
       "5342                 Romance   \n",
       "5343         Horror|Thriller   \n",
       "5344             Documentary   \n",
       "5345                 Western   \n",
       "5346                  Horror   \n",
       "\n",
       "                                                 Poster  \\\n",
       "0     https://images-na.ssl-images-amazon.com/images...   \n",
       "1     https://images-na.ssl-images-amazon.com/images...   \n",
       "2     https://images-na.ssl-images-amazon.com/images...   \n",
       "3     https://images-na.ssl-images-amazon.com/images...   \n",
       "4     https://images-na.ssl-images-amazon.com/images...   \n",
       "...                                                 ...   \n",
       "5342  https://images-na.ssl-images-amazon.com/images...   \n",
       "5343  https://images-na.ssl-images-amazon.com/images...   \n",
       "5344  https://images-na.ssl-images-amazon.com/images...   \n",
       "5345  https://images-na.ssl-images-amazon.com/images...   \n",
       "5346  https://images-na.ssl-images-amazon.com/images...   \n",
       "\n",
       "                                               genrelst  \n",
       "0     [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "5342  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5343  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "5344  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5345  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5346  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "\n",
       "[5347 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df_reset = y_df.reset_index()\n",
    "\n",
    "shape = y_df_reset.shape[1]\n",
    "\n",
    "index_value = []\n",
    "genre_lst = []\n",
    "\n",
    "for i in range(len(y_df_reset)):\n",
    "    index_value.append(int(y_df_reset.loc[i,\"index\"]))\n",
    "    temp_list = []\n",
    "    for j in y_df_reset.columns[1:]:\n",
    "        temp_list.append(y_df_reset.loc[i,j])\n",
    "    genre_lst.append(temp_list)\n",
    "\n",
    "df = pd.DataFrame(list(zip(index_value, genre_lst)),\n",
    "               columns =['imdbId', 'genrelst'])\n",
    "\n",
    "result = dataset2.merge(df, on=\"imdbId\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(result)):\n",
    "    tempY.append((int(result['imdbId'].iloc[x]),result['genrelst'].iloc[x]))\n",
    "\n",
    "#tempY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3742"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting the length of training data\n",
    "length=int(len(flist)*training_size)\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the data about the images that are available\n",
    "i=0\n",
    "for filename in flist:\n",
    "    name=int(filename.split('/')[-1][:-4])\n",
    "    for z in tempY:\n",
    "        if(z[0]==name):\n",
    "            \n",
    "            img = array(cv2.imread(filename))\n",
    "            img = swapaxes(img, 2,0)\n",
    "            img = swapaxes(img, 2,1)\n",
    "\n",
    "            if(i<length):\n",
    "                x_train.append(img)\n",
    "                y_train.append(z[1])\n",
    "                i+=1\n",
    "            else:\n",
    "                x_test.append(img)\n",
    "                y_test.append(z[1])\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data from lists to numpy arrays\n",
    "x_train=asarray(x_train,dtype=float)\n",
    "x_test=asarray(x_test,dtype=float)\n",
    "y_train=asarray(y_train,dtype=float)\n",
    "y_test=asarray(y_test,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling down the RGB data\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3742, 3, 268, 182)\n",
      "3742 train samples\n",
      "1605 test samples\n"
     ]
    }
   ],
   "source": [
    "#printing stats about the features\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = x_train.shape[0]\n",
    "\n",
    "x_train=torch.from_numpy(x_train)\n",
    "x_test=torch.from_numpy(x_test)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_test=torch.from_numpy(y_test)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train, y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(x_test, y_test)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric calculation\n",
    "\n",
    "def metric(scores, targets):\n",
    "    \"\"\"\n",
    "    :param scores: the output the model predict\n",
    "    :param targets: the gt label\n",
    "    :return: OP, OR, OF1, CP, CR, CF1\n",
    "    calculate the Precision of every class by: TP/TP+FP i.e. TP/total predict\n",
    "    calculate the Recall by: TP/total GT\n",
    "    \"\"\"\n",
    "    num, num_class = scores.shape\n",
    "    gt_num = np.zeros(num_class)\n",
    "    tp_num = np.zeros(num_class)\n",
    "    predict_num = np.zeros(num_class)\n",
    "\n",
    "\n",
    "    for index in range(num_class):\n",
    "        score = scores[:, index]\n",
    "        target = targets[:, index]\n",
    "\n",
    "        gt_num[index] = np.sum(target == 1)\n",
    "        predict_num[index] = np.sum(score >= 0.5)\n",
    "        tp_num[index] = np.sum(target * (score >= 0.5))\n",
    "\n",
    "    predict_num[predict_num == 0] = 1  # avoid dividing 0\n",
    "    OP = np.sum(tp_num) / np.sum(predict_num) #OP (Overall Precision) is the ratio of the number of correctly predicted positive samples to the total number of positive predictions made by the model\n",
    "    OR = np.sum(tp_num) / np.sum(gt_num) #OR (Overall Recall) is the ratio of the number of correctly predicted positive samples to the total number of positive samples in the ground truth.\n",
    "    OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n",
    "\n",
    "    return OP, OR, OF1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 model\n",
    "from torchvision import models\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(2048, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNet()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/3742 (0%)]\tLoss: 0.697382 \tOP: 0.111111\tOR: 0.066176\tOF1: 0.082949\n",
      "Train Epoch: 0 [64/3742 (2%)]\tLoss: 0.658392 \tOP: 0.022727\tOR: 0.007576\tOF1: 0.011364\n",
      "Train Epoch: 0 [128/3742 (3%)]\tLoss: 0.619709 \tOP: 0.031250\tOR: 0.006897\tOF1: 0.011299\n",
      "Train Epoch: 0 [192/3742 (5%)]\tLoss: 0.582666 \tOP: 0.035714\tOR: 0.006757\tOF1: 0.011364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-447663f93c68>:28: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [256/3742 (7%)]\tLoss: 0.546917 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [320/3742 (8%)]\tLoss: 0.514193 \tOP: 0.038462\tOR: 0.007143\tOF1: 0.012048\n",
      "Train Epoch: 0 [384/3742 (10%)]\tLoss: 0.485512 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [448/3742 (12%)]\tLoss: 0.460225 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [512/3742 (14%)]\tLoss: 0.434832 \tOP: 0.032258\tOR: 0.007463\tOF1: 0.012121\n",
      "Train Epoch: 0 [576/3742 (15%)]\tLoss: 0.408632 \tOP: 0.166667\tOR: 0.035461\tOF1: 0.058480\n",
      "Train Epoch: 0 [640/3742 (17%)]\tLoss: 0.389026 \tOP: 0.068966\tOR: 0.014599\tOF1: 0.024096\n",
      "Train Epoch: 0 [704/3742 (19%)]\tLoss: 0.370935 \tOP: 0.096774\tOR: 0.021583\tOF1: 0.035294\n",
      "Train Epoch: 0 [768/3742 (20%)]\tLoss: 0.346261 \tOP: 0.137931\tOR: 0.031008\tOF1: 0.050633\n",
      "Train Epoch: 0 [832/3742 (22%)]\tLoss: 0.330763 \tOP: 0.133333\tOR: 0.029851\tOF1: 0.048780\n",
      "Train Epoch: 0 [896/3742 (24%)]\tLoss: 0.325184 \tOP: 0.151515\tOR: 0.035461\tOF1: 0.057471\n",
      "Train Epoch: 0 [960/3742 (25%)]\tLoss: 0.295355 \tOP: 0.166667\tOR: 0.038168\tOF1: 0.062112\n",
      "Train Epoch: 0 [1024/3742 (27%)]\tLoss: 0.303605 \tOP: 0.282051\tOR: 0.082090\tOF1: 0.127168\n",
      "Train Epoch: 0 [1088/3742 (29%)]\tLoss: 0.276364 \tOP: 0.216216\tOR: 0.063492\tOF1: 0.098160\n",
      "Train Epoch: 0 [1152/3742 (31%)]\tLoss: 0.268215 \tOP: 0.205128\tOR: 0.061538\tOF1: 0.094675\n",
      "Train Epoch: 0 [1216/3742 (32%)]\tLoss: 0.279769 \tOP: 0.200000\tOR: 0.059701\tOF1: 0.091954\n",
      "Train Epoch: 0 [1280/3742 (34%)]\tLoss: 0.258260 \tOP: 0.285714\tOR: 0.092308\tOF1: 0.139535\n",
      "Train Epoch: 0 [1344/3742 (36%)]\tLoss: 0.256501 \tOP: 0.263158\tOR: 0.074074\tOF1: 0.115607\n",
      "Train Epoch: 0 [1408/3742 (37%)]\tLoss: 0.254971 \tOP: 0.225000\tOR: 0.065217\tOF1: 0.101124\n",
      "Train Epoch: 0 [1472/3742 (39%)]\tLoss: 0.245253 \tOP: 0.325000\tOR: 0.094203\tOF1: 0.146067\n",
      "Train Epoch: 0 [1536/3742 (41%)]\tLoss: 0.246797 \tOP: 0.400000\tOR: 0.125000\tOF1: 0.190476\n",
      "Train Epoch: 0 [1600/3742 (42%)]\tLoss: 0.221058 \tOP: 0.390244\tOR: 0.126984\tOF1: 0.191617\n",
      "Train Epoch: 0 [1664/3742 (44%)]\tLoss: 0.238904 \tOP: 0.317073\tOR: 0.090909\tOF1: 0.141304\n",
      "Train Epoch: 0 [1728/3742 (46%)]\tLoss: 0.236194 \tOP: 0.295455\tOR: 0.095588\tOF1: 0.144444\n",
      "Train Epoch: 0 [1792/3742 (47%)]\tLoss: 0.232711 \tOP: 0.319149\tOR: 0.108696\tOF1: 0.162162\n",
      "Train Epoch: 0 [1856/3742 (49%)]\tLoss: 0.227263 \tOP: 0.407407\tOR: 0.154930\tOF1: 0.224490\n",
      "Train Epoch: 0 [1920/3742 (51%)]\tLoss: 0.237067 \tOP: 0.372549\tOR: 0.131034\tOF1: 0.193878\n",
      "Train Epoch: 0 [1984/3742 (53%)]\tLoss: 0.235698 \tOP: 0.367347\tOR: 0.125000\tOF1: 0.186528\n",
      "Train Epoch: 0 [2048/3742 (54%)]\tLoss: 0.236938 \tOP: 0.282609\tOR: 0.094203\tOF1: 0.141304\n",
      "Train Epoch: 0 [2112/3742 (56%)]\tLoss: 0.226925 \tOP: 0.274510\tOR: 0.100000\tOF1: 0.146597\n",
      "Train Epoch: 0 [2176/3742 (58%)]\tLoss: 0.214833 \tOP: 0.387755\tOR: 0.142857\tOF1: 0.208791\n",
      "Train Epoch: 0 [2240/3742 (59%)]\tLoss: 0.198831 \tOP: 0.384615\tOR: 0.156250\tOF1: 0.222222\n",
      "Train Epoch: 0 [2304/3742 (61%)]\tLoss: 0.227231 \tOP: 0.372549\tOR: 0.134752\tOF1: 0.197917\n",
      "Train Epoch: 0 [2368/3742 (63%)]\tLoss: 0.217858 \tOP: 0.406780\tOR: 0.175182\tOF1: 0.244898\n",
      "Train Epoch: 0 [2432/3742 (64%)]\tLoss: 0.230199 \tOP: 0.396226\tOR: 0.141892\tOF1: 0.208955\n",
      "Train Epoch: 0 [2496/3742 (66%)]\tLoss: 0.214108 \tOP: 0.314815\tOR: 0.125926\tOF1: 0.179894\n",
      "Train Epoch: 0 [2560/3742 (68%)]\tLoss: 0.206694 \tOP: 0.352941\tOR: 0.134328\tOF1: 0.194595\n",
      "Train Epoch: 0 [2624/3742 (69%)]\tLoss: 0.208118 \tOP: 0.367347\tOR: 0.140625\tOF1: 0.203390\n",
      "Train Epoch: 0 [2688/3742 (71%)]\tLoss: 0.215416 \tOP: 0.333333\tOR: 0.110294\tOF1: 0.165746\n",
      "Train Epoch: 0 [2752/3742 (73%)]\tLoss: 0.220867 \tOP: 0.214286\tOR: 0.070866\tOF1: 0.106509\n",
      "Train Epoch: 0 [2816/3742 (75%)]\tLoss: 0.193687 \tOP: 0.339286\tOR: 0.153226\tOF1: 0.211111\n",
      "Train Epoch: 0 [2880/3742 (76%)]\tLoss: 0.224916 \tOP: 0.288889\tOR: 0.091549\tOF1: 0.139037\n",
      "Train Epoch: 0 [2944/3742 (78%)]\tLoss: 0.226316 \tOP: 0.250000\tOR: 0.078014\tOF1: 0.118919\n",
      "Train Epoch: 0 [3008/3742 (80%)]\tLoss: 0.205350 \tOP: 0.333333\tOR: 0.110294\tOF1: 0.165746\n",
      "Train Epoch: 0 [3072/3742 (81%)]\tLoss: 0.218590 \tOP: 0.363636\tOR: 0.109589\tOF1: 0.168421\n",
      "Train Epoch: 0 [3136/3742 (83%)]\tLoss: 0.220503 \tOP: 0.355556\tOR: 0.113475\tOF1: 0.172043\n",
      "Train Epoch: 0 [3200/3742 (85%)]\tLoss: 0.198468 \tOP: 0.420000\tOR: 0.146853\tOF1: 0.217617\n",
      "Train Epoch: 0 [3264/3742 (86%)]\tLoss: 0.199100 \tOP: 0.377778\tOR: 0.122302\tOF1: 0.184783\n",
      "Train Epoch: 0 [3328/3742 (88%)]\tLoss: 0.206210 \tOP: 0.413043\tOR: 0.131034\tOF1: 0.198953\n",
      "Train Epoch: 0 [3392/3742 (90%)]\tLoss: 0.199293 \tOP: 0.372549\tOR: 0.142857\tOF1: 0.206522\n",
      "Train Epoch: 0 [3456/3742 (92%)]\tLoss: 0.211868 \tOP: 0.434783\tOR: 0.134228\tOF1: 0.205128\n",
      "Train Epoch: 0 [3520/3742 (93%)]\tLoss: 0.213757 \tOP: 0.369565\tOR: 0.125000\tOF1: 0.186813\n",
      "Train Epoch: 0 [3584/3742 (95%)]\tLoss: 0.218535 \tOP: 0.416667\tOR: 0.135135\tOF1: 0.204082\n",
      "Train Epoch: 0 [3648/3742 (97%)]\tLoss: 0.193956 \tOP: 0.431373\tOR: 0.166667\tOF1: 0.240437\n",
      "Train Epoch: 0 [1740/3742 (98%)]\tLoss: 0.190529 \tOP: 0.270270\tOR: 0.172414\tOF1: 0.210526\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-ea5e5cd9fa3b>:45: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2092 \n",
      "OP: 0.076923\n",
      "OR: 0.142857\n",
      "OF1: 0.100000\n",
      "\n",
      "Train Epoch: 1 [0/3742 (0%)]\tLoss: 0.170585 \tOP: 0.540000\tOR: 0.201493\tOF1: 0.293478\n",
      "Train Epoch: 1 [64/3742 (2%)]\tLoss: 0.200910 \tOP: 0.500000\tOR: 0.200000\tOF1: 0.285714\n",
      "Train Epoch: 1 [128/3742 (3%)]\tLoss: 0.187900 \tOP: 0.516667\tOR: 0.229630\tOF1: 0.317949\n",
      "Train Epoch: 1 [192/3742 (5%)]\tLoss: 0.193933 \tOP: 0.525424\tOR: 0.210884\tOF1: 0.300971\n",
      "Train Epoch: 1 [256/3742 (7%)]\tLoss: 0.170857 \tOP: 0.587302\tOR: 0.282443\tOF1: 0.381443\n",
      "Train Epoch: 1 [320/3742 (8%)]\tLoss: 0.186951 \tOP: 0.491228\tOR: 0.198582\tOF1: 0.282828\n",
      "Train Epoch: 1 [384/3742 (10%)]\tLoss: 0.167561 \tOP: 0.557377\tOR: 0.263566\tOF1: 0.357895\n",
      "Train Epoch: 1 [448/3742 (12%)]\tLoss: 0.169442 \tOP: 0.491228\tOR: 0.227642\tOF1: 0.311111\n",
      "Train Epoch: 1 [512/3742 (14%)]\tLoss: 0.201028 \tOP: 0.507937\tOR: 0.211921\tOF1: 0.299065\n",
      "Train Epoch: 1 [576/3742 (15%)]\tLoss: 0.156928 \tOP: 0.551724\tOR: 0.258065\tOF1: 0.351648\n",
      "Train Epoch: 1 [640/3742 (17%)]\tLoss: 0.171310 \tOP: 0.551724\tOR: 0.233577\tOF1: 0.328205\n",
      "Train Epoch: 1 [704/3742 (19%)]\tLoss: 0.191787 \tOP: 0.472727\tOR: 0.188406\tOF1: 0.269430\n",
      "Train Epoch: 1 [768/3742 (20%)]\tLoss: 0.161284 \tOP: 0.548387\tOR: 0.274194\tOF1: 0.365591\n",
      "Train Epoch: 1 [832/3742 (22%)]\tLoss: 0.174756 \tOP: 0.566667\tOR: 0.246377\tOF1: 0.343434\n",
      "Train Epoch: 1 [896/3742 (24%)]\tLoss: 0.176042 \tOP: 0.590164\tOR: 0.251748\tOF1: 0.352941\n",
      "Train Epoch: 1 [960/3742 (25%)]\tLoss: 0.191578 \tOP: 0.518519\tOR: 0.208955\tOF1: 0.297872\n",
      "Train Epoch: 1 [1024/3742 (27%)]\tLoss: 0.160455 \tOP: 0.589286\tOR: 0.251908\tOF1: 0.352941\n",
      "Train Epoch: 1 [1088/3742 (29%)]\tLoss: 0.170290 \tOP: 0.573770\tOR: 0.255474\tOF1: 0.353535\n",
      "Train Epoch: 1 [1152/3742 (31%)]\tLoss: 0.182843 \tOP: 0.525424\tOR: 0.236641\tOF1: 0.326316\n",
      "Train Epoch: 1 [1216/3742 (32%)]\tLoss: 0.196507 \tOP: 0.537037\tOR: 0.200000\tOF1: 0.291457\n",
      "Train Epoch: 1 [1280/3742 (34%)]\tLoss: 0.165874 \tOP: 0.603448\tOR: 0.248227\tOF1: 0.351759\n",
      "Train Epoch: 1 [1344/3742 (36%)]\tLoss: 0.176770 \tOP: 0.551724\tOR: 0.228571\tOF1: 0.323232\n",
      "Train Epoch: 1 [1408/3742 (37%)]\tLoss: 0.167325 \tOP: 0.571429\tOR: 0.260870\tOF1: 0.358209\n",
      "Train Epoch: 1 [1472/3742 (39%)]\tLoss: 0.178648 \tOP: 0.542373\tOR: 0.244275\tOF1: 0.336842\n",
      "Train Epoch: 1 [1536/3742 (41%)]\tLoss: 0.173062 \tOP: 0.587302\tOR: 0.278195\tOF1: 0.377551\n",
      "Train Epoch: 1 [1600/3742 (42%)]\tLoss: 0.177983 \tOP: 0.562500\tOR: 0.250000\tOF1: 0.346154\n",
      "Train Epoch: 1 [1664/3742 (44%)]\tLoss: 0.166075 \tOP: 0.606557\tOR: 0.278195\tOF1: 0.381443\n",
      "Train Epoch: 1 [1728/3742 (46%)]\tLoss: 0.189434 \tOP: 0.590164\tOR: 0.238411\tOF1: 0.339623\n",
      "Train Epoch: 1 [1792/3742 (47%)]\tLoss: 0.190125 \tOP: 0.523077\tOR: 0.237762\tOF1: 0.326923\n",
      "Train Epoch: 1 [1856/3742 (49%)]\tLoss: 0.168066 \tOP: 0.641791\tOR: 0.304965\tOF1: 0.413462\n",
      "Train Epoch: 1 [1920/3742 (51%)]\tLoss: 0.197927 \tOP: 0.530303\tOR: 0.241379\tOF1: 0.331754\n",
      "Train Epoch: 1 [1984/3742 (53%)]\tLoss: 0.171531 \tOP: 0.530303\tOR: 0.277778\tOF1: 0.364583\n",
      "Train Epoch: 1 [2048/3742 (54%)]\tLoss: 0.178119 \tOP: 0.629032\tOR: 0.267123\tOF1: 0.375000\n",
      "Train Epoch: 1 [2112/3742 (56%)]\tLoss: 0.153787 \tOP: 0.647059\tOR: 0.325926\tOF1: 0.433498\n",
      "Train Epoch: 1 [2176/3742 (58%)]\tLoss: 0.171589 \tOP: 0.619048\tOR: 0.272727\tOF1: 0.378641\n",
      "Train Epoch: 1 [2240/3742 (59%)]\tLoss: 0.161477 \tOP: 0.584615\tOR: 0.279412\tOF1: 0.378109\n",
      "Train Epoch: 1 [2304/3742 (61%)]\tLoss: 0.173226 \tOP: 0.516667\tOR: 0.234848\tOF1: 0.322917\n",
      "Train Epoch: 1 [2368/3742 (63%)]\tLoss: 0.149843 \tOP: 0.625000\tOR: 0.305344\tOF1: 0.410256\n",
      "Train Epoch: 1 [2432/3742 (64%)]\tLoss: 0.166863 \tOP: 0.593750\tOR: 0.283582\tOF1: 0.383838\n",
      "Train Epoch: 1 [2496/3742 (66%)]\tLoss: 0.174265 \tOP: 0.546875\tOR: 0.253623\tOF1: 0.346535\n",
      "Train Epoch: 1 [2560/3742 (68%)]\tLoss: 0.175378 \tOP: 0.650794\tOR: 0.284722\tOF1: 0.396135\n",
      "Train Epoch: 1 [2624/3742 (69%)]\tLoss: 0.159546 \tOP: 0.620690\tOR: 0.272727\tOF1: 0.378947\n",
      "Train Epoch: 1 [2688/3742 (71%)]\tLoss: 0.171283 \tOP: 0.593750\tOR: 0.271429\tOF1: 0.372549\n",
      "Train Epoch: 1 [2752/3742 (73%)]\tLoss: 0.170750 \tOP: 0.605634\tOR: 0.311594\tOF1: 0.411483\n",
      "Train Epoch: 1 [2816/3742 (75%)]\tLoss: 0.169385 \tOP: 0.593750\tOR: 0.271429\tOF1: 0.372549\n",
      "Train Epoch: 1 [2880/3742 (76%)]\tLoss: 0.168974 \tOP: 0.581081\tOR: 0.318519\tOF1: 0.411483\n",
      "Train Epoch: 1 [2944/3742 (78%)]\tLoss: 0.165489 \tOP: 0.639344\tOR: 0.284672\tOF1: 0.393939\n",
      "Train Epoch: 1 [3008/3742 (80%)]\tLoss: 0.172110 \tOP: 0.575758\tOR: 0.296875\tOF1: 0.391753\n",
      "Train Epoch: 1 [3072/3742 (81%)]\tLoss: 0.184379 \tOP: 0.485294\tOR: 0.239130\tOF1: 0.320388\n",
      "Train Epoch: 1 [3136/3742 (83%)]\tLoss: 0.170725 \tOP: 0.612903\tOR: 0.277372\tOF1: 0.381910\n",
      "Train Epoch: 1 [3200/3742 (85%)]\tLoss: 0.176032 \tOP: 0.555556\tOR: 0.261194\tOF1: 0.355330\n",
      "Train Epoch: 1 [3264/3742 (86%)]\tLoss: 0.165156 \tOP: 0.571429\tOR: 0.281250\tOF1: 0.376963\n",
      "Train Epoch: 1 [3328/3742 (88%)]\tLoss: 0.188002 \tOP: 0.562500\tOR: 0.248276\tOF1: 0.344498\n",
      "Train Epoch: 1 [3392/3742 (90%)]\tLoss: 0.180469 \tOP: 0.566667\tOR: 0.253731\tOF1: 0.350515\n",
      "Train Epoch: 1 [3456/3742 (92%)]\tLoss: 0.163422 \tOP: 0.596774\tOR: 0.274074\tOF1: 0.375635\n",
      "Train Epoch: 1 [3520/3742 (93%)]\tLoss: 0.176802 \tOP: 0.603175\tOR: 0.285714\tOF1: 0.387755\n",
      "Train Epoch: 1 [3584/3742 (95%)]\tLoss: 0.175993 \tOP: 0.576271\tOR: 0.248175\tOF1: 0.346939\n",
      "Train Epoch: 1 [3648/3742 (97%)]\tLoss: 0.185954 \tOP: 0.609375\tOR: 0.263514\tOF1: 0.367925\n",
      "Train Epoch: 1 [1740/3742 (98%)]\tLoss: 0.182783 \tOP: 0.400000\tOR: 0.238806\tOF1: 0.299065\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2121 \n",
      "OP: 0.037037\n",
      "OR: 0.071429\n",
      "OF1: 0.048780\n",
      "\n",
      "Train Epoch: 2 [0/3742 (0%)]\tLoss: 0.125515 \tOP: 0.681818\tOR: 0.360000\tOF1: 0.471204\n",
      "Train Epoch: 2 [64/3742 (2%)]\tLoss: 0.143811 \tOP: 0.685714\tOR: 0.350365\tOF1: 0.463768\n",
      "Train Epoch: 2 [128/3742 (3%)]\tLoss: 0.136973 \tOP: 0.655738\tOR: 0.285714\tOF1: 0.398010\n",
      "Train Epoch: 2 [192/3742 (5%)]\tLoss: 0.128962 \tOP: 0.732394\tOR: 0.396947\tOF1: 0.514851\n",
      "Train Epoch: 2 [256/3742 (7%)]\tLoss: 0.120875 \tOP: 0.746667\tOR: 0.437500\tOF1: 0.551724\n",
      "Train Epoch: 2 [320/3742 (8%)]\tLoss: 0.136998 \tOP: 0.722222\tOR: 0.363636\tOF1: 0.483721\n",
      "Train Epoch: 2 [384/3742 (10%)]\tLoss: 0.138020 \tOP: 0.753425\tOR: 0.371622\tOF1: 0.497738\n",
      "Train Epoch: 2 [448/3742 (12%)]\tLoss: 0.151950 \tOP: 0.756757\tOR: 0.363636\tOF1: 0.491228\n",
      "Train Epoch: 2 [512/3742 (14%)]\tLoss: 0.144303 \tOP: 0.746835\tOR: 0.409722\tOF1: 0.529148\n",
      "Train Epoch: 2 [576/3742 (15%)]\tLoss: 0.138589 \tOP: 0.717949\tOR: 0.397163\tOF1: 0.511416\n",
      "Train Epoch: 2 [640/3742 (17%)]\tLoss: 0.130964 \tOP: 0.775000\tOR: 0.427586\tOF1: 0.551111\n",
      "Train Epoch: 2 [704/3742 (19%)]\tLoss: 0.128914 \tOP: 0.746667\tOR: 0.427481\tOF1: 0.543689\n",
      "Train Epoch: 2 [768/3742 (20%)]\tLoss: 0.124664 \tOP: 0.790123\tOR: 0.450704\tOF1: 0.573991\n",
      "Train Epoch: 2 [832/3742 (22%)]\tLoss: 0.143846 \tOP: 0.727273\tOR: 0.400000\tOF1: 0.516129\n",
      "Train Epoch: 2 [896/3742 (24%)]\tLoss: 0.135858 \tOP: 0.759494\tOR: 0.408163\tOF1: 0.530973\n",
      "Train Epoch: 2 [960/3742 (25%)]\tLoss: 0.125221 \tOP: 0.765432\tOR: 0.469697\tOF1: 0.582160\n",
      "Train Epoch: 2 [1024/3742 (27%)]\tLoss: 0.110680 \tOP: 0.792683\tOR: 0.488722\tOF1: 0.604651\n",
      "Train Epoch: 2 [1088/3742 (29%)]\tLoss: 0.126524 \tOP: 0.717949\tOR: 0.411765\tOF1: 0.523364\n",
      "Train Epoch: 2 [1152/3742 (31%)]\tLoss: 0.120616 \tOP: 0.797753\tOR: 0.496503\tOF1: 0.612069\n",
      "Train Epoch: 2 [1216/3742 (32%)]\tLoss: 0.111473 \tOP: 0.746988\tOR: 0.484375\tOF1: 0.587678\n",
      "Train Epoch: 2 [1280/3742 (34%)]\tLoss: 0.121266 \tOP: 0.760000\tOR: 0.445312\tOF1: 0.561576\n",
      "Train Epoch: 2 [1344/3742 (36%)]\tLoss: 0.144119 \tOP: 0.666667\tOR: 0.372414\tOF1: 0.477876\n",
      "Train Epoch: 2 [1408/3742 (37%)]\tLoss: 0.130055 \tOP: 0.790123\tOR: 0.441379\tOF1: 0.566372\n",
      "Train Epoch: 2 [1472/3742 (39%)]\tLoss: 0.121685 \tOP: 0.756098\tOR: 0.466165\tOF1: 0.576744\n",
      "Train Epoch: 2 [1536/3742 (41%)]\tLoss: 0.120557 \tOP: 0.792208\tOR: 0.476562\tOF1: 0.595122\n",
      "Train Epoch: 2 [1600/3742 (42%)]\tLoss: 0.113616 \tOP: 0.779221\tOR: 0.454545\tOF1: 0.574163\n",
      "Train Epoch: 2 [1664/3742 (44%)]\tLoss: 0.126961 \tOP: 0.761364\tOR: 0.492647\tOF1: 0.598214\n",
      "Train Epoch: 2 [1728/3742 (46%)]\tLoss: 0.138610 \tOP: 0.756757\tOR: 0.400000\tOF1: 0.523364\n",
      "Train Epoch: 2 [1792/3742 (47%)]\tLoss: 0.123047 \tOP: 0.761364\tOR: 0.465278\tOF1: 0.577586\n",
      "Train Epoch: 2 [1856/3742 (49%)]\tLoss: 0.120832 \tOP: 0.762500\tOR: 0.438849\tOF1: 0.557078\n",
      "Train Epoch: 2 [1920/3742 (51%)]\tLoss: 0.123828 \tOP: 0.811111\tOR: 0.506944\tOF1: 0.623932\n",
      "Train Epoch: 2 [1984/3742 (53%)]\tLoss: 0.127727 \tOP: 0.759036\tOR: 0.428571\tOF1: 0.547826\n",
      "Train Epoch: 2 [2048/3742 (54%)]\tLoss: 0.117521 \tOP: 0.759494\tOR: 0.454545\tOF1: 0.568720\n",
      "Train Epoch: 2 [2112/3742 (56%)]\tLoss: 0.112780 \tOP: 0.802198\tOR: 0.544776\tOF1: 0.648889\n",
      "Train Epoch: 2 [2176/3742 (58%)]\tLoss: 0.117524 \tOP: 0.790123\tOR: 0.467153\tOF1: 0.587156\n",
      "Train Epoch: 2 [2240/3742 (59%)]\tLoss: 0.128184 \tOP: 0.777778\tOR: 0.446809\tOF1: 0.567568\n",
      "Train Epoch: 2 [2304/3742 (61%)]\tLoss: 0.118672 \tOP: 0.759036\tOR: 0.456522\tOF1: 0.570136\n",
      "Train Epoch: 2 [2368/3742 (63%)]\tLoss: 0.113179 \tOP: 0.758621\tOR: 0.496241\tOF1: 0.600000\n",
      "Train Epoch: 2 [2432/3742 (64%)]\tLoss: 0.123061 \tOP: 0.766667\tOR: 0.500000\tOF1: 0.605263\n",
      "Train Epoch: 2 [2496/3742 (66%)]\tLoss: 0.125051 \tOP: 0.743902\tOR: 0.458647\tOF1: 0.567442\n",
      "Train Epoch: 2 [2560/3742 (68%)]\tLoss: 0.108290 \tOP: 0.788235\tOR: 0.507576\tOF1: 0.617512\n",
      "Train Epoch: 2 [2624/3742 (69%)]\tLoss: 0.115014 \tOP: 0.785714\tOR: 0.523810\tOF1: 0.628571\n",
      "Train Epoch: 2 [2688/3742 (71%)]\tLoss: 0.113094 \tOP: 0.759036\tOR: 0.512195\tOF1: 0.611650\n",
      "Train Epoch: 2 [2752/3742 (73%)]\tLoss: 0.122165 \tOP: 0.752809\tOR: 0.482014\tOF1: 0.587719\n",
      "Train Epoch: 2 [2816/3742 (75%)]\tLoss: 0.118970 \tOP: 0.741573\tOR: 0.515625\tOF1: 0.608295\n",
      "Train Epoch: 2 [2880/3742 (76%)]\tLoss: 0.124787 \tOP: 0.741176\tOR: 0.470149\tOF1: 0.575342\n",
      "Train Epoch: 2 [2944/3742 (78%)]\tLoss: 0.108988 \tOP: 0.760417\tOR: 0.561538\tOF1: 0.646018\n",
      "Train Epoch: 2 [3008/3742 (80%)]\tLoss: 0.120989 \tOP: 0.761905\tOR: 0.477612\tOF1: 0.587156\n",
      "Train Epoch: 2 [3072/3742 (81%)]\tLoss: 0.118290 \tOP: 0.724138\tOR: 0.484615\tOF1: 0.580645\n",
      "Train Epoch: 2 [3136/3742 (83%)]\tLoss: 0.115255 \tOP: 0.767442\tOR: 0.503817\tOF1: 0.608295\n",
      "Train Epoch: 2 [3200/3742 (85%)]\tLoss: 0.144855 \tOP: 0.802469\tOR: 0.419355\tOF1: 0.550847\n",
      "Train Epoch: 2 [3264/3742 (86%)]\tLoss: 0.127675 \tOP: 0.735632\tOR: 0.463768\tOF1: 0.568889\n",
      "Train Epoch: 2 [3328/3742 (88%)]\tLoss: 0.122349 \tOP: 0.725000\tOR: 0.449612\tOF1: 0.555024\n",
      "Train Epoch: 2 [3392/3742 (90%)]\tLoss: 0.121932 \tOP: 0.750000\tOR: 0.463235\tOF1: 0.572727\n",
      "Train Epoch: 2 [3456/3742 (92%)]\tLoss: 0.125415 \tOP: 0.772727\tOR: 0.503704\tOF1: 0.609865\n",
      "Train Epoch: 2 [3520/3742 (93%)]\tLoss: 0.128537 \tOP: 0.728395\tOR: 0.443609\tOF1: 0.551402\n",
      "Train Epoch: 2 [3584/3742 (95%)]\tLoss: 0.134924 \tOP: 0.752941\tOR: 0.467153\tOF1: 0.576577\n",
      "Train Epoch: 2 [3648/3742 (97%)]\tLoss: 0.131922 \tOP: 0.750000\tOR: 0.448980\tOF1: 0.561702\n",
      "Train Epoch: 2 [1740/3742 (98%)]\tLoss: 0.135841 \tOP: 0.660377\tOR: 0.486111\tOF1: 0.560000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2305 \n",
      "OP: 0.107143\n",
      "OR: 0.214286\n",
      "OF1: 0.142857\n",
      "\n",
      "Train Epoch: 3 [0/3742 (0%)]\tLoss: 0.095839 \tOP: 0.835052\tOR: 0.586957\tOF1: 0.689362\n",
      "Train Epoch: 3 [64/3742 (2%)]\tLoss: 0.083977 \tOP: 0.844660\tOR: 0.644444\tOF1: 0.731092\n",
      "Train Epoch: 3 [128/3742 (3%)]\tLoss: 0.078768 \tOP: 0.844037\tOR: 0.696970\tOF1: 0.763485\n",
      "Train Epoch: 3 [192/3742 (5%)]\tLoss: 0.101472 \tOP: 0.822917\tOR: 0.612403\tOF1: 0.702222\n",
      "Train Epoch: 3 [256/3742 (7%)]\tLoss: 0.100996 \tOP: 0.795699\tOR: 0.564885\tOF1: 0.660714\n",
      "Train Epoch: 3 [320/3742 (8%)]\tLoss: 0.094871 \tOP: 0.835052\tOR: 0.582734\tOF1: 0.686441\n",
      "Train Epoch: 3 [384/3742 (10%)]\tLoss: 0.101750 \tOP: 0.835052\tOR: 0.604478\tOF1: 0.701299\n",
      "Train Epoch: 3 [448/3742 (12%)]\tLoss: 0.085936 \tOP: 0.855769\tOR: 0.635714\tOF1: 0.729508\n",
      "Train Epoch: 3 [512/3742 (14%)]\tLoss: 0.102387 \tOP: 0.843750\tOR: 0.551020\tOF1: 0.666667\n",
      "Train Epoch: 3 [576/3742 (15%)]\tLoss: 0.096935 \tOP: 0.851485\tOR: 0.614286\tOF1: 0.713693\n",
      "Train Epoch: 3 [640/3742 (17%)]\tLoss: 0.086221 \tOP: 0.836735\tOR: 0.625954\tOF1: 0.716157\n",
      "Train Epoch: 3 [704/3742 (19%)]\tLoss: 0.091437 \tOP: 0.828571\tOR: 0.621429\tOF1: 0.710204\n",
      "Train Epoch: 3 [768/3742 (20%)]\tLoss: 0.094003 \tOP: 0.824742\tOR: 0.583942\tOF1: 0.683761\n",
      "Train Epoch: 3 [832/3742 (22%)]\tLoss: 0.091707 \tOP: 0.814433\tOR: 0.617188\tOF1: 0.702222\n",
      "Train Epoch: 3 [896/3742 (24%)]\tLoss: 0.095117 \tOP: 0.820000\tOR: 0.616541\tOF1: 0.703863\n",
      "Train Epoch: 3 [960/3742 (25%)]\tLoss: 0.095966 \tOP: 0.827957\tOR: 0.546099\tOF1: 0.658120\n",
      "Train Epoch: 3 [1024/3742 (27%)]\tLoss: 0.081573 \tOP: 0.878505\tOR: 0.676259\tOF1: 0.764228\n",
      "Train Epoch: 3 [1088/3742 (29%)]\tLoss: 0.094086 \tOP: 0.830189\tOR: 0.611111\tOF1: 0.704000\n",
      "Train Epoch: 3 [1152/3742 (31%)]\tLoss: 0.089078 \tOP: 0.844660\tOR: 0.625899\tOF1: 0.719008\n",
      "Train Epoch: 3 [1216/3742 (32%)]\tLoss: 0.092593 \tOP: 0.831776\tOR: 0.597315\tOF1: 0.695313\n",
      "Train Epoch: 3 [1280/3742 (34%)]\tLoss: 0.105645 \tOP: 0.836957\tOR: 0.520270\tOF1: 0.641667\n",
      "Train Epoch: 3 [1344/3742 (36%)]\tLoss: 0.083517 \tOP: 0.814433\tOR: 0.663866\tOF1: 0.731481\n",
      "Train Epoch: 3 [1408/3742 (37%)]\tLoss: 0.079151 \tOP: 0.836735\tOR: 0.630769\tOF1: 0.719298\n",
      "Train Epoch: 3 [1472/3742 (39%)]\tLoss: 0.076983 \tOP: 0.842593\tOR: 0.679104\tOF1: 0.752066\n",
      "Train Epoch: 3 [1536/3742 (41%)]\tLoss: 0.072338 \tOP: 0.846154\tOR: 0.651852\tOF1: 0.736402\n",
      "Train Epoch: 3 [1600/3742 (42%)]\tLoss: 0.084447 \tOP: 0.841121\tOR: 0.633803\tOF1: 0.722892\n",
      "Train Epoch: 3 [1664/3742 (44%)]\tLoss: 0.089622 \tOP: 0.838384\tOR: 0.614815\tOF1: 0.709402\n",
      "Train Epoch: 3 [1728/3742 (46%)]\tLoss: 0.081802 \tOP: 0.858491\tOR: 0.679104\tOF1: 0.758333\n",
      "Train Epoch: 3 [1792/3742 (47%)]\tLoss: 0.083708 \tOP: 0.818182\tOR: 0.623077\tOF1: 0.707424\n",
      "Train Epoch: 3 [1856/3742 (49%)]\tLoss: 0.076195 \tOP: 0.861386\tOR: 0.630435\tOF1: 0.728033\n",
      "Train Epoch: 3 [1920/3742 (51%)]\tLoss: 0.082586 \tOP: 0.867257\tOR: 0.671233\tOF1: 0.756757\n",
      "Train Epoch: 3 [1984/3742 (53%)]\tLoss: 0.087344 \tOP: 0.828283\tOR: 0.581560\tOF1: 0.683333\n",
      "Train Epoch: 3 [2048/3742 (54%)]\tLoss: 0.078758 \tOP: 0.864078\tOR: 0.706349\tOF1: 0.777293\n",
      "Train Epoch: 3 [2112/3742 (56%)]\tLoss: 0.090241 \tOP: 0.852941\tOR: 0.591837\tOF1: 0.698795\n",
      "Train Epoch: 3 [2176/3742 (58%)]\tLoss: 0.084194 \tOP: 0.861386\tOR: 0.649254\tOF1: 0.740426\n",
      "Train Epoch: 3 [2240/3742 (59%)]\tLoss: 0.089754 \tOP: 0.860000\tOR: 0.597222\tOF1: 0.704918\n",
      "Train Epoch: 3 [2304/3742 (61%)]\tLoss: 0.079161 \tOP: 0.854369\tOR: 0.647059\tOF1: 0.736402\n",
      "Train Epoch: 3 [2368/3742 (63%)]\tLoss: 0.075441 \tOP: 0.820000\tOR: 0.672131\tOF1: 0.738739\n",
      "Train Epoch: 3 [2432/3742 (64%)]\tLoss: 0.087511 \tOP: 0.839623\tOR: 0.631206\tOF1: 0.720648\n",
      "Train Epoch: 3 [2496/3742 (66%)]\tLoss: 0.076226 \tOP: 0.865385\tOR: 0.671642\tOF1: 0.756303\n",
      "Train Epoch: 3 [2560/3742 (68%)]\tLoss: 0.073387 \tOP: 0.862385\tOR: 0.696296\tOF1: 0.770492\n",
      "Train Epoch: 3 [2624/3742 (69%)]\tLoss: 0.076428 \tOP: 0.827273\tOR: 0.700000\tOF1: 0.758333\n",
      "Train Epoch: 3 [2688/3742 (71%)]\tLoss: 0.072103 \tOP: 0.866667\tOR: 0.664234\tOF1: 0.752066\n",
      "Train Epoch: 3 [2752/3742 (73%)]\tLoss: 0.081298 \tOP: 0.855769\tOR: 0.640288\tOF1: 0.732510\n",
      "Train Epoch: 3 [2816/3742 (75%)]\tLoss: 0.087897 \tOP: 0.826923\tOR: 0.637037\tOF1: 0.719665\n",
      "Train Epoch: 3 [2880/3742 (76%)]\tLoss: 0.087603 \tOP: 0.852941\tOR: 0.608392\tOF1: 0.710204\n",
      "Train Epoch: 3 [2944/3742 (78%)]\tLoss: 0.070013 \tOP: 0.849558\tOR: 0.705882\tOF1: 0.771084\n",
      "Train Epoch: 3 [3008/3742 (80%)]\tLoss: 0.089932 \tOP: 0.840708\tOR: 0.655172\tOF1: 0.736434\n",
      "Train Epoch: 3 [3072/3742 (81%)]\tLoss: 0.079792 \tOP: 0.836735\tOR: 0.616541\tOF1: 0.709957\n",
      "Train Epoch: 3 [3136/3742 (83%)]\tLoss: 0.094476 \tOP: 0.815534\tOR: 0.604317\tOF1: 0.694215\n",
      "Train Epoch: 3 [3200/3742 (85%)]\tLoss: 0.082609 \tOP: 0.847619\tOR: 0.649635\tOF1: 0.735537\n",
      "Train Epoch: 3 [3264/3742 (86%)]\tLoss: 0.092775 \tOP: 0.865979\tOR: 0.604317\tOF1: 0.711864\n",
      "Train Epoch: 3 [3328/3742 (88%)]\tLoss: 0.082866 \tOP: 0.866667\tOR: 0.659420\tOF1: 0.748971\n",
      "Train Epoch: 3 [3392/3742 (90%)]\tLoss: 0.081624 \tOP: 0.867925\tOR: 0.671533\tOF1: 0.757202\n",
      "Train Epoch: 3 [3456/3742 (92%)]\tLoss: 0.073489 \tOP: 0.875000\tOR: 0.710145\tOF1: 0.784000\n",
      "Train Epoch: 3 [3520/3742 (93%)]\tLoss: 0.067085 \tOP: 0.870370\tOR: 0.717557\tOF1: 0.786611\n",
      "Train Epoch: 3 [3584/3742 (95%)]\tLoss: 0.080841 \tOP: 0.845361\tOR: 0.594203\tOF1: 0.697872\n",
      "Train Epoch: 3 [3648/3742 (97%)]\tLoss: 0.094194 \tOP: 0.852941\tOR: 0.591837\tOF1: 0.698795\n",
      "Train Epoch: 3 [1740/3742 (98%)]\tLoss: 0.104354 \tOP: 0.634615\tOR: 0.507692\tOF1: 0.564103\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2277 \n",
      "OP: 0.103448\n",
      "OR: 0.214286\n",
      "OF1: 0.139535\n",
      "\n",
      "Train Epoch: 4 [0/3742 (0%)]\tLoss: 0.073529 \tOP: 0.899083\tOR: 0.695035\tOF1: 0.784000\n",
      "Train Epoch: 4 [64/3742 (2%)]\tLoss: 0.057168 \tOP: 0.890909\tOR: 0.748092\tOF1: 0.813278\n",
      "Train Epoch: 4 [128/3742 (3%)]\tLoss: 0.059638 \tOP: 0.886957\tOR: 0.733813\tOF1: 0.803150\n",
      "Train Epoch: 4 [192/3742 (5%)]\tLoss: 0.059614 \tOP: 0.886957\tOR: 0.778626\tOF1: 0.829268\n",
      "Train Epoch: 4 [256/3742 (7%)]\tLoss: 0.069531 \tOP: 0.912000\tOR: 0.750000\tOF1: 0.823105\n",
      "Train Epoch: 4 [320/3742 (8%)]\tLoss: 0.062122 \tOP: 0.880734\tOR: 0.727273\tOF1: 0.796680\n",
      "Train Epoch: 4 [384/3742 (10%)]\tLoss: 0.059667 \tOP: 0.879310\tOR: 0.744526\tOF1: 0.806324\n",
      "Train Epoch: 4 [448/3742 (12%)]\tLoss: 0.063630 \tOP: 0.889908\tOR: 0.729323\tOF1: 0.801653\n",
      "Train Epoch: 4 [512/3742 (14%)]\tLoss: 0.070387 \tOP: 0.905172\tOR: 0.714286\tOF1: 0.798479\n",
      "Train Epoch: 4 [576/3742 (15%)]\tLoss: 0.052692 \tOP: 0.903509\tOR: 0.780303\tOF1: 0.837398\n",
      "Train Epoch: 4 [640/3742 (17%)]\tLoss: 0.061781 \tOP: 0.893443\tOR: 0.784173\tOF1: 0.835249\n",
      "Train Epoch: 4 [704/3742 (19%)]\tLoss: 0.064761 \tOP: 0.897436\tOR: 0.755396\tOF1: 0.820312\n",
      "Train Epoch: 4 [768/3742 (20%)]\tLoss: 0.060017 \tOP: 0.893805\tOR: 0.748148\tOF1: 0.814516\n",
      "Train Epoch: 4 [832/3742 (22%)]\tLoss: 0.058227 \tOP: 0.872727\tOR: 0.786885\tOF1: 0.827586\n",
      "Train Epoch: 4 [896/3742 (24%)]\tLoss: 0.061438 \tOP: 0.905983\tOR: 0.741259\tOF1: 0.815385\n",
      "Train Epoch: 4 [960/3742 (25%)]\tLoss: 0.058883 \tOP: 0.884956\tOR: 0.719424\tOF1: 0.793651\n",
      "Train Epoch: 4 [1024/3742 (27%)]\tLoss: 0.048256 \tOP: 0.893443\tOR: 0.838462\tOF1: 0.865079\n",
      "Train Epoch: 4 [1088/3742 (29%)]\tLoss: 0.064836 \tOP: 0.891892\tOR: 0.722628\tOF1: 0.798387\n",
      "Train Epoch: 4 [1152/3742 (31%)]\tLoss: 0.060946 \tOP: 0.880342\tOR: 0.746377\tOF1: 0.807843\n",
      "Train Epoch: 4 [1216/3742 (32%)]\tLoss: 0.050287 \tOP: 0.890909\tOR: 0.784000\tOF1: 0.834043\n",
      "Train Epoch: 4 [1280/3742 (34%)]\tLoss: 0.052866 \tOP: 0.893443\tOR: 0.819549\tOF1: 0.854902\n",
      "Train Epoch: 4 [1344/3742 (36%)]\tLoss: 0.062123 \tOP: 0.887931\tOR: 0.725352\tOF1: 0.798450\n",
      "Train Epoch: 4 [1408/3742 (37%)]\tLoss: 0.061219 \tOP: 0.876190\tOR: 0.718750\tOF1: 0.789700\n",
      "Train Epoch: 4 [1472/3742 (39%)]\tLoss: 0.055859 \tOP: 0.892857\tOR: 0.763359\tOF1: 0.823045\n",
      "Train Epoch: 4 [1536/3742 (41%)]\tLoss: 0.044146 \tOP: 0.907563\tOR: 0.850394\tOF1: 0.878049\n",
      "Train Epoch: 4 [1600/3742 (42%)]\tLoss: 0.065304 \tOP: 0.898305\tOR: 0.701987\tOF1: 0.788104\n",
      "Train Epoch: 4 [1664/3742 (44%)]\tLoss: 0.062893 \tOP: 0.891667\tOR: 0.753521\tOF1: 0.816794\n",
      "Train Epoch: 4 [1728/3742 (46%)]\tLoss: 0.048413 \tOP: 0.903226\tOR: 0.805755\tOF1: 0.851711\n",
      "Train Epoch: 4 [1792/3742 (47%)]\tLoss: 0.048272 \tOP: 0.900826\tOR: 0.807407\tOF1: 0.851562\n",
      "Train Epoch: 4 [1856/3742 (49%)]\tLoss: 0.045184 \tOP: 0.897436\tOR: 0.807692\tOF1: 0.850202\n",
      "Train Epoch: 4 [1920/3742 (51%)]\tLoss: 0.060580 \tOP: 0.899160\tOR: 0.732877\tOF1: 0.807547\n",
      "Train Epoch: 4 [1984/3742 (53%)]\tLoss: 0.063842 \tOP: 0.886957\tOR: 0.689189\tOF1: 0.775665\n",
      "Train Epoch: 4 [2048/3742 (54%)]\tLoss: 0.058389 \tOP: 0.885965\tOR: 0.721429\tOF1: 0.795276\n",
      "Train Epoch: 4 [2112/3742 (56%)]\tLoss: 0.059881 \tOP: 0.879310\tOR: 0.733813\tOF1: 0.800000\n",
      "Train Epoch: 4 [2176/3742 (58%)]\tLoss: 0.064652 \tOP: 0.901961\tOR: 0.696970\tOF1: 0.786325\n",
      "Train Epoch: 4 [2240/3742 (59%)]\tLoss: 0.057642 \tOP: 0.914062\tOR: 0.818182\tOF1: 0.863469\n",
      "Train Epoch: 4 [2304/3742 (61%)]\tLoss: 0.074291 \tOP: 0.909091\tOR: 0.661765\tOF1: 0.765957\n",
      "Train Epoch: 4 [2368/3742 (63%)]\tLoss: 0.058193 \tOP: 0.891892\tOR: 0.738806\tOF1: 0.808163\n",
      "Train Epoch: 4 [2432/3742 (64%)]\tLoss: 0.061271 \tOP: 0.887931\tOR: 0.741007\tOF1: 0.807843\n",
      "Train Epoch: 4 [2496/3742 (66%)]\tLoss: 0.051262 \tOP: 0.897436\tOR: 0.777778\tOF1: 0.833333\n",
      "Train Epoch: 4 [2560/3742 (68%)]\tLoss: 0.049687 \tOP: 0.875000\tOR: 0.771654\tOF1: 0.820084\n",
      "Train Epoch: 4 [2624/3742 (69%)]\tLoss: 0.063595 \tOP: 0.884956\tOR: 0.709220\tOF1: 0.787402\n",
      "Train Epoch: 4 [2688/3742 (71%)]\tLoss: 0.058997 \tOP: 0.896552\tOR: 0.748201\tOF1: 0.815686\n",
      "Train Epoch: 4 [2752/3742 (73%)]\tLoss: 0.053362 \tOP: 0.933884\tOR: 0.801418\tOF1: 0.862595\n",
      "Train Epoch: 4 [2816/3742 (75%)]\tLoss: 0.055208 \tOP: 0.900000\tOR: 0.776978\tOF1: 0.833977\n",
      "Train Epoch: 4 [2880/3742 (76%)]\tLoss: 0.053399 \tOP: 0.908333\tOR: 0.807407\tOF1: 0.854902\n",
      "Train Epoch: 4 [2944/3742 (78%)]\tLoss: 0.057732 \tOP: 0.900826\tOR: 0.756944\tOF1: 0.822642\n",
      "Train Epoch: 4 [3008/3742 (80%)]\tLoss: 0.063739 \tOP: 0.892857\tOR: 0.724638\tOF1: 0.800000\n",
      "Train Epoch: 4 [3072/3742 (81%)]\tLoss: 0.049359 \tOP: 0.911290\tOR: 0.818841\tOF1: 0.862595\n",
      "Train Epoch: 4 [3136/3742 (83%)]\tLoss: 0.046175 \tOP: 0.902655\tOR: 0.790698\tOF1: 0.842975\n",
      "Train Epoch: 4 [3200/3742 (85%)]\tLoss: 0.055206 \tOP: 0.916667\tOR: 0.769231\tOF1: 0.836502\n",
      "Train Epoch: 4 [3264/3742 (86%)]\tLoss: 0.060404 \tOP: 0.887850\tOR: 0.736434\tOF1: 0.805085\n",
      "Train Epoch: 4 [3328/3742 (88%)]\tLoss: 0.064391 \tOP: 0.873874\tOR: 0.687943\tOF1: 0.769841\n",
      "Train Epoch: 4 [3392/3742 (90%)]\tLoss: 0.047951 \tOP: 0.902655\tOR: 0.803150\tOF1: 0.850000\n",
      "Train Epoch: 4 [3456/3742 (92%)]\tLoss: 0.063917 \tOP: 0.905983\tOR: 0.773723\tOF1: 0.834646\n",
      "Train Epoch: 4 [3520/3742 (93%)]\tLoss: 0.058218 \tOP: 0.905983\tOR: 0.768116\tOF1: 0.831373\n",
      "Train Epoch: 4 [3584/3742 (95%)]\tLoss: 0.047903 \tOP: 0.933333\tOR: 0.811594\tOF1: 0.868217\n",
      "Train Epoch: 4 [3648/3742 (97%)]\tLoss: 0.061719 \tOP: 0.902655\tOR: 0.723404\tOF1: 0.803150\n",
      "Train Epoch: 4 [1740/3742 (98%)]\tLoss: 0.068310 \tOP: 0.765625\tOR: 0.731343\tOF1: 0.748092\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2379 \n",
      "OP: 0.071429\n",
      "OR: 0.142857\n",
      "OF1: 0.095238\n",
      "\n",
      "Train Epoch: 5 [0/3742 (0%)]\tLoss: 0.040297 \tOP: 0.916667\tOR: 0.827068\tOF1: 0.869565\n",
      "Train Epoch: 5 [64/3742 (2%)]\tLoss: 0.038845 \tOP: 0.915966\tOR: 0.858268\tOF1: 0.886179\n",
      "Train Epoch: 5 [128/3742 (3%)]\tLoss: 0.043705 \tOP: 0.939850\tOR: 0.833333\tOF1: 0.883392\n",
      "Train Epoch: 5 [192/3742 (5%)]\tLoss: 0.043630 \tOP: 0.918699\tOR: 0.830882\tOF1: 0.872587\n",
      "Train Epoch: 5 [256/3742 (7%)]\tLoss: 0.049456 \tOP: 0.922535\tOR: 0.850649\tOF1: 0.885135\n",
      "Train Epoch: 5 [320/3742 (8%)]\tLoss: 0.043006 \tOP: 0.900826\tOR: 0.832061\tOF1: 0.865079\n",
      "Train Epoch: 5 [384/3742 (10%)]\tLoss: 0.039516 \tOP: 0.925620\tOR: 0.842105\tOF1: 0.881890\n",
      "Train Epoch: 5 [448/3742 (12%)]\tLoss: 0.042764 \tOP: 0.912281\tOR: 0.793893\tOF1: 0.848980\n",
      "Train Epoch: 5 [512/3742 (14%)]\tLoss: 0.045990 \tOP: 0.931034\tOR: 0.782609\tOF1: 0.850394\n",
      "Train Epoch: 5 [576/3742 (15%)]\tLoss: 0.037806 \tOP: 0.925926\tOR: 0.886525\tOF1: 0.905797\n",
      "Train Epoch: 5 [640/3742 (17%)]\tLoss: 0.039981 \tOP: 0.928571\tOR: 0.847826\tOF1: 0.886364\n",
      "Train Epoch: 5 [704/3742 (19%)]\tLoss: 0.041628 \tOP: 0.935484\tOR: 0.840580\tOF1: 0.885496\n",
      "Train Epoch: 5 [768/3742 (20%)]\tLoss: 0.037096 \tOP: 0.909836\tOR: 0.860465\tOF1: 0.884462\n",
      "Train Epoch: 5 [832/3742 (22%)]\tLoss: 0.042974 \tOP: 0.921260\tOR: 0.823944\tOF1: 0.869888\n",
      "Train Epoch: 5 [896/3742 (24%)]\tLoss: 0.045132 \tOP: 0.943089\tOR: 0.816901\tOF1: 0.875472\n",
      "Train Epoch: 5 [960/3742 (25%)]\tLoss: 0.043158 \tOP: 0.933884\tOR: 0.849624\tOF1: 0.889764\n",
      "Train Epoch: 5 [1024/3742 (27%)]\tLoss: 0.033142 \tOP: 0.917355\tOR: 0.895161\tOF1: 0.906122\n",
      "Train Epoch: 5 [1088/3742 (29%)]\tLoss: 0.041781 \tOP: 0.926829\tOR: 0.814286\tOF1: 0.866920\n",
      "Train Epoch: 5 [1152/3742 (31%)]\tLoss: 0.038181 \tOP: 0.929134\tOR: 0.867647\tOF1: 0.897338\n",
      "Train Epoch: 5 [1216/3742 (32%)]\tLoss: 0.040180 \tOP: 0.917910\tOR: 0.848276\tOF1: 0.881720\n",
      "Train Epoch: 5 [1280/3742 (34%)]\tLoss: 0.041838 \tOP: 0.919355\tOR: 0.863636\tOF1: 0.890625\n",
      "Train Epoch: 5 [1344/3742 (36%)]\tLoss: 0.036962 \tOP: 0.912000\tOR: 0.870229\tOF1: 0.890625\n",
      "Train Epoch: 5 [1408/3742 (37%)]\tLoss: 0.040217 \tOP: 0.942623\tOR: 0.821429\tOF1: 0.877863\n",
      "Train Epoch: 5 [1472/3742 (39%)]\tLoss: 0.034882 \tOP: 0.918033\tOR: 0.842105\tOF1: 0.878431\n",
      "Train Epoch: 5 [1536/3742 (41%)]\tLoss: 0.039356 \tOP: 0.940741\tOR: 0.869863\tOF1: 0.903915\n",
      "Train Epoch: 5 [1600/3742 (42%)]\tLoss: 0.036943 \tOP: 0.940299\tOR: 0.875000\tOF1: 0.906475\n",
      "Train Epoch: 5 [1664/3742 (44%)]\tLoss: 0.046743 \tOP: 0.926230\tOR: 0.807143\tOF1: 0.862595\n",
      "Train Epoch: 5 [1728/3742 (46%)]\tLoss: 0.039415 \tOP: 0.923664\tOR: 0.889706\tOF1: 0.906367\n",
      "Train Epoch: 5 [1792/3742 (47%)]\tLoss: 0.047525 \tOP: 0.930769\tOR: 0.806667\tOF1: 0.864286\n",
      "Train Epoch: 5 [1856/3742 (49%)]\tLoss: 0.038001 \tOP: 0.927419\tOR: 0.858209\tOF1: 0.891473\n",
      "Train Epoch: 5 [1920/3742 (51%)]\tLoss: 0.039270 \tOP: 0.914062\tOR: 0.860294\tOF1: 0.886364\n",
      "Train Epoch: 5 [1984/3742 (53%)]\tLoss: 0.042598 \tOP: 0.929134\tOR: 0.802721\tOF1: 0.861314\n",
      "Train Epoch: 5 [2048/3742 (54%)]\tLoss: 0.039117 \tOP: 0.929688\tOR: 0.843972\tOF1: 0.884758\n",
      "Train Epoch: 5 [2112/3742 (56%)]\tLoss: 0.030769 \tOP: 0.915966\tOR: 0.879032\tOF1: 0.897119\n",
      "Train Epoch: 5 [2176/3742 (58%)]\tLoss: 0.033252 \tOP: 0.928571\tOR: 0.866667\tOF1: 0.896552\n",
      "Train Epoch: 5 [2240/3742 (59%)]\tLoss: 0.039531 \tOP: 0.917355\tOR: 0.822222\tOF1: 0.867188\n",
      "Train Epoch: 5 [2304/3742 (61%)]\tLoss: 0.033930 \tOP: 0.943548\tOR: 0.873134\tOF1: 0.906977\n",
      "Train Epoch: 5 [2368/3742 (63%)]\tLoss: 0.037759 \tOP: 0.937500\tOR: 0.869565\tOF1: 0.902256\n",
      "Train Epoch: 5 [2432/3742 (64%)]\tLoss: 0.047034 \tOP: 0.923077\tOR: 0.805970\tOF1: 0.860558\n",
      "Train Epoch: 5 [2496/3742 (66%)]\tLoss: 0.039489 \tOP: 0.913793\tOR: 0.815385\tOF1: 0.861789\n",
      "Train Epoch: 5 [2560/3742 (68%)]\tLoss: 0.044064 \tOP: 0.906780\tOR: 0.786765\tOF1: 0.842520\n",
      "Train Epoch: 5 [2624/3742 (69%)]\tLoss: 0.034095 \tOP: 0.929688\tOR: 0.856115\tOF1: 0.891386\n",
      "Train Epoch: 5 [2688/3742 (71%)]\tLoss: 0.044598 \tOP: 0.921875\tOR: 0.797297\tOF1: 0.855072\n",
      "Train Epoch: 5 [2752/3742 (73%)]\tLoss: 0.031873 \tOP: 0.922481\tOR: 0.894737\tOF1: 0.908397\n",
      "Train Epoch: 5 [2816/3742 (75%)]\tLoss: 0.037621 \tOP: 0.911504\tOR: 0.830645\tOF1: 0.869198\n",
      "Train Epoch: 5 [2880/3742 (76%)]\tLoss: 0.041285 \tOP: 0.925000\tOR: 0.834586\tOF1: 0.877470\n",
      "Train Epoch: 5 [2944/3742 (78%)]\tLoss: 0.035618 \tOP: 0.944000\tOR: 0.848921\tOF1: 0.893939\n",
      "Train Epoch: 5 [3008/3742 (80%)]\tLoss: 0.033923 \tOP: 0.931624\tOR: 0.865079\tOF1: 0.897119\n",
      "Train Epoch: 5 [3072/3742 (81%)]\tLoss: 0.033300 \tOP: 0.937008\tOR: 0.875000\tOF1: 0.904943\n",
      "Train Epoch: 5 [3136/3742 (83%)]\tLoss: 0.038539 \tOP: 0.925620\tOR: 0.848485\tOF1: 0.885375\n",
      "Train Epoch: 5 [3200/3742 (85%)]\tLoss: 0.051199 \tOP: 0.942149\tOR: 0.780822\tOF1: 0.853933\n",
      "Train Epoch: 5 [3264/3742 (86%)]\tLoss: 0.035326 \tOP: 0.936000\tOR: 0.847826\tOF1: 0.889734\n",
      "Train Epoch: 5 [3328/3742 (88%)]\tLoss: 0.032695 \tOP: 0.932836\tOR: 0.886525\tOF1: 0.909091\n",
      "Train Epoch: 5 [3392/3742 (90%)]\tLoss: 0.040916 \tOP: 0.924370\tOR: 0.797101\tOF1: 0.856031\n",
      "Train Epoch: 5 [3456/3742 (92%)]\tLoss: 0.038268 \tOP: 0.934959\tOR: 0.858209\tOF1: 0.894942\n",
      "Train Epoch: 5 [3520/3742 (93%)]\tLoss: 0.040355 \tOP: 0.931818\tOR: 0.842466\tOF1: 0.884892\n",
      "Train Epoch: 5 [3584/3742 (95%)]\tLoss: 0.040673 \tOP: 0.920635\tOR: 0.816901\tOF1: 0.865672\n",
      "Train Epoch: 5 [3648/3742 (97%)]\tLoss: 0.038664 \tOP: 0.926230\tOR: 0.830882\tOF1: 0.875969\n",
      "Train Epoch: 5 [1740/3742 (98%)]\tLoss: 0.033488 \tOP: 0.766667\tOR: 0.821429\tOF1: 0.793103\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "        \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for n in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for m in range(len(classes)):\n",
    "        #        if target[n][m] == 1 or preds[n][m] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[n][m] == target[n][m]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet201\n",
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "model2 = torchvision.models.densenet201(pretrained=True)\n",
    "num_ftrs = model2.classifier.in_features\n",
    "model2.classifier = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model2.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model2(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'wider')\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model2(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "        \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for n in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for m in range(len(classes)):\n",
    "        #        if target[n][m] == 1 or preds[n][m] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[n][m] == target[n][m]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'wider')\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet152 model\n",
    "from torchvision import models\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(2048, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ResNet18()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model3.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model3(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'voc07')\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model3.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model3(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "        \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for n in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for m in range(len(classes)):\n",
    "        #        if target[n][m] == 1 or preds[n][m] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[n][m] == target[n][m]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'voc07')\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG (frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_vgg11 = models.vgg11(weights=models.VGG11_Weights.DEFAULT)\n",
    "        \n",
    "#         child_counter = 0\n",
    "#         for child in model_vgg11.children():\n",
    "#             if child_counter < 1:\n",
    "#                 for param in child.parameters():\n",
    "#                     param.requires_grad = False\n",
    "#                     child_counter += 1\n",
    "#             else:\n",
    "#                 child_counter += 1\n",
    "        \n",
    "        self.model_pre = nn.Sequential(*list(model_vgg11.children())[:-1])\n",
    "        fc_feat = model_vgg11.classifier[0].in_features\n",
    "        self.model_post = nn.Sequential(\n",
    "            nn.Linear(fc_feat, 4096, bias = True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(.5, inplace = False),\n",
    "            nn.Linear(4096, 4096, bias = True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(.5, inplace = False),\n",
    "        )\n",
    "        self.fc = nn.Linear(4096, len(classes))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.model_pre(X)\n",
    "        X = torch.flatten(X,1)\n",
    "        X = self.model_post(X)\n",
    "        return self.fc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = MyCNN3()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model4.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model4.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model4(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model4.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model4(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG (Unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_vgg11 = models.vgg11(weights=models.VGG11_Weights.DEFAULT)\n",
    "        \n",
    "        child_counter = 0\n",
    "        for child in model_vgg11.children():\n",
    "            if child_counter < 1:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    child_counter += 1\n",
    "            else:\n",
    "                child_counter += 1\n",
    "        \n",
    "        self.model_pre = nn.Sequential(*list(model_vgg11.children())[:-1])\n",
    "        fc_feat = model_vgg11.classifier[0].in_features\n",
    "        self.model_post = nn.Sequential(\n",
    "            nn.Linear(fc_feat, 4096, bias = True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(.5, inplace = False),\n",
    "            nn.Linear(4096, 4096, bias = True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(.5, inplace = False),\n",
    "        )\n",
    "        self.fc = nn.Linear(4096, len(classes))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.model_pre(X)\n",
    "        X = torch.flatten(X,1)\n",
    "        X = self.model_post(X)\n",
    "        return self.fc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = MyCNN4()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model5.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model5.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model5(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model5.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model5(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
