{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "from torch.nn.modules import MSELoss, L1Loss, BCELoss\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "from numpy import array, asarray, ndarray, swapaxes\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training controls\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "training_size = 0.7\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 268, 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data holders\n",
    "x_test = []\n",
    "x_train = []\n",
    "y_test= []\n",
    "y_train= []\n",
    "tempY = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdbId', 'Imdb Link', 'Title', 'IMDB Score', 'Genre', 'Poster']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opening the dataset\n",
    "dataset = csv.reader(open(\"MovieGenre3.csv\",encoding=\"utf8\",errors='replace'), delimiter=\",\")\n",
    "\n",
    "# skipping the header line\n",
    "next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract images from zip folder\n",
    "\n",
    "import zipfile as zf\n",
    "\n",
    "files = zf.ZipFile(\"Poster.zip\", 'r')\n",
    "files.extractall()\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of image files in SampleMoviePosters folder\n",
    "flist=glob.glob('Poster/*.jpg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1354"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = []\n",
    "\n",
    "for path in flist:\n",
    "    start = path.rfind(\"/\")+1\n",
    "    end = len(path)-4\n",
    "    image_ids.append(path[start:end])\n",
    "    \n",
    "#image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160905</td>\n",
       "      <td>http://www.imdb.com/title/tt160905</td>\n",
       "      <td>Spooky House (2002)</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Comedy|Family</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427531</td>\n",
       "      <td>http://www.imdb.com/title/tt427531</td>\n",
       "      <td>Mezzo Forte</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Animation|Comedy|Crime</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4175088</td>\n",
       "      <td>http://www.imdb.com/title/tt4175088</td>\n",
       "      <td>Radical Grace (2015)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>Documentary|News</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1686053</td>\n",
       "      <td>http://www.imdb.com/title/tt1686053</td>\n",
       "      <td>Apnoia (2010)</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22921</td>\n",
       "      <td>http://www.imdb.com/title/tt22921</td>\n",
       "      <td>Broadway to Cheyenne (1932)</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>821638</td>\n",
       "      <td>http://www.imdb.com/title/tt821638</td>\n",
       "      <td>Bury My Heart at Wounded Knee (2007)</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Drama|History|Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>87835</td>\n",
       "      <td>http://www.imdb.com/title/tt87835</td>\n",
       "      <td>Oh, God! You Devil (1984)</td>\n",
       "      <td>5.3</td>\n",
       "      <td>Comedy|Fantasy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>103710</td>\n",
       "      <td>http://www.imdb.com/title/tt103710</td>\n",
       "      <td>AprÌ¬s l'amour (1992)</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>108265</td>\n",
       "      <td>http://www.imdb.com/title/tt108265</td>\n",
       "      <td>Swing Kids (1993)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Drama|Music</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>56468</td>\n",
       "      <td>http://www.imdb.com/title/tt56468</td>\n",
       "      <td>I sequestrati di Altona (1962)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Drama|History</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       imdbId                            Imdb Link  \\\n",
       "0      160905   http://www.imdb.com/title/tt160905   \n",
       "1      427531   http://www.imdb.com/title/tt427531   \n",
       "2     4175088  http://www.imdb.com/title/tt4175088   \n",
       "3     1686053  http://www.imdb.com/title/tt1686053   \n",
       "4       22921    http://www.imdb.com/title/tt22921   \n",
       "...       ...                                  ...   \n",
       "1349   821638   http://www.imdb.com/title/tt821638   \n",
       "1350    87835    http://www.imdb.com/title/tt87835   \n",
       "1351   103710   http://www.imdb.com/title/tt103710   \n",
       "1352   108265   http://www.imdb.com/title/tt108265   \n",
       "1353    56468    http://www.imdb.com/title/tt56468   \n",
       "\n",
       "                                     Title  IMDB Score  \\\n",
       "0                      Spooky House (2002)         5.4   \n",
       "1                              Mezzo Forte         6.9   \n",
       "2                     Radical Grace (2015)         8.3   \n",
       "3                            Apnoia (2010)         5.6   \n",
       "4              Broadway to Cheyenne (1932)         4.8   \n",
       "...                                    ...         ...   \n",
       "1349  Bury My Heart at Wounded Knee (2007)         7.2   \n",
       "1350             Oh, God! You Devil (1984)         5.3   \n",
       "1351                 AprÌ¬s l'amour (1992)         6.3   \n",
       "1352                     Swing Kids (1993)         6.8   \n",
       "1353        I sequestrati di Altona (1962)         7.0   \n",
       "\n",
       "                       Genre  \\\n",
       "0              Comedy|Family   \n",
       "1     Animation|Comedy|Crime   \n",
       "2           Documentary|News   \n",
       "3             Drama|Thriller   \n",
       "4                    Western   \n",
       "...                      ...   \n",
       "1349   Drama|History|Western   \n",
       "1350          Comedy|Fantasy   \n",
       "1351           Drama|Romance   \n",
       "1352             Drama|Music   \n",
       "1353           Drama|History   \n",
       "\n",
       "                                                 Poster  \n",
       "0     https://images-na.ssl-images-amazon.com/images...  \n",
       "1     https://images-na.ssl-images-amazon.com/images...  \n",
       "2     https://images-na.ssl-images-amazon.com/images...  \n",
       "3     https://images-na.ssl-images-amazon.com/images...  \n",
       "4     https://images-na.ssl-images-amazon.com/images...  \n",
       "...                                                 ...  \n",
       "1349  https://images-na.ssl-images-amazon.com/images...  \n",
       "1350  https://images-na.ssl-images-amazon.com/images...  \n",
       "1351  https://images-na.ssl-images-amazon.com/images...  \n",
       "1352  https://images-na.ssl-images-amazon.com/images...  \n",
       "1353  https://images-na.ssl-images-amazon.com/images...  \n",
       "\n",
       "[1354 rows x 6 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset2 = pd.read_csv(\"MovieGenre3.csv\")\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "indexlist = []\n",
    "classes = tuple()\n",
    "ids = dataset2.imdbId.values.tolist()\n",
    "for image_id in image_ids:\n",
    "    genres = tuple((dataset2[dataset2[\"imdbId\"] == int(image_id)][\"Genre\"].values[0]).split(\"|\"))\n",
    "    if int(image_id) in ids:\n",
    "        indexlist.append(image_id)\n",
    "    y.append(genres)\n",
    "    classes = classes + genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y)\n",
    "y = mlb.transform(y)\n",
    "classes = set(classes)\n",
    "classes = list(classes)\n",
    "classes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>...</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>News</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788556</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4767340</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363473</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393775</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018920</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244244</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897945</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151987</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Action  Adventure  Animation  Biography  Comedy  Crime  Documentary  \\\n",
       "87913         0          0          0          0       0      0            0   \n",
       "2788556       0          0          0          0       1      0            0   \n",
       "4767340       0          0          0          0       1      0            0   \n",
       "363473        0          0          0          1       0      0            0   \n",
       "393775        0          0          0          0       1      0            0   \n",
       "...         ...        ...        ...        ...     ...    ...          ...   \n",
       "1018920       0          0          0          0       0      0            1   \n",
       "244244        1          0          0          0       0      1            0   \n",
       "1897945       0          0          0          0       1      0            0   \n",
       "377569        0          0          0          0       0      0            0   \n",
       "151987        1          1          0          0       0      1            0   \n",
       "\n",
       "         Drama  Family  Fantasy  ...  Musical  Mystery  News  Romance  Sci-Fi  \\\n",
       "87913        1       0        0  ...        0        0     0        0       0   \n",
       "2788556      1       0        0  ...        0        0     0        1       0   \n",
       "4767340      1       0        0  ...        0        0     0        0       0   \n",
       "363473       1       0        0  ...        0        0     0        0       0   \n",
       "393775       1       0        0  ...        0        0     0        0       0   \n",
       "...        ...     ...      ...  ...      ...      ...   ...      ...     ...   \n",
       "1018920      0       0        0  ...        0        0     0        0       0   \n",
       "244244       0       0        0  ...        0        0     0        0       0   \n",
       "1897945      0       0        0  ...        0        0     0        1       0   \n",
       "377569       1       0        0  ...        0        0     0        0       0   \n",
       "151987       0       0        0  ...        0        0     0        0       0   \n",
       "\n",
       "         Short  Sport  Thriller  War  Western  \n",
       "87913        0      0         0    0        0  \n",
       "2788556      0      0         0    0        0  \n",
       "4767340      0      0         0    0        0  \n",
       "363473       0      0         0    0        0  \n",
       "393775       0      0         0    0        0  \n",
       "...        ...    ...       ...  ...      ...  \n",
       "1018920      1      0         0    0        0  \n",
       "244244       0      0         1    0        0  \n",
       "1897945      0      0         0    0        0  \n",
       "377569       0      0         0    0        0  \n",
       "151987       0      0         0    0        0  \n",
       "\n",
       "[1354 rows x 24 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame(y, columns = classes, index = indexlist)\n",
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "      <th>genrelst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160905</td>\n",
       "      <td>http://www.imdb.com/title/tt160905</td>\n",
       "      <td>Spooky House (2002)</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Comedy|Family</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427531</td>\n",
       "      <td>http://www.imdb.com/title/tt427531</td>\n",
       "      <td>Mezzo Forte</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Animation|Comedy|Crime</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4175088</td>\n",
       "      <td>http://www.imdb.com/title/tt4175088</td>\n",
       "      <td>Radical Grace (2015)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>Documentary|News</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1686053</td>\n",
       "      <td>http://www.imdb.com/title/tt1686053</td>\n",
       "      <td>Apnoia (2010)</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22921</td>\n",
       "      <td>http://www.imdb.com/title/tt22921</td>\n",
       "      <td>Broadway to Cheyenne (1932)</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>821638</td>\n",
       "      <td>http://www.imdb.com/title/tt821638</td>\n",
       "      <td>Bury My Heart at Wounded Knee (2007)</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Drama|History|Western</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>87835</td>\n",
       "      <td>http://www.imdb.com/title/tt87835</td>\n",
       "      <td>Oh, God! You Devil (1984)</td>\n",
       "      <td>5.3</td>\n",
       "      <td>Comedy|Fantasy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>103710</td>\n",
       "      <td>http://www.imdb.com/title/tt103710</td>\n",
       "      <td>AprÌ¬s l'amour (1992)</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>108265</td>\n",
       "      <td>http://www.imdb.com/title/tt108265</td>\n",
       "      <td>Swing Kids (1993)</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Drama|Music</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>56468</td>\n",
       "      <td>http://www.imdb.com/title/tt56468</td>\n",
       "      <td>I sequestrati di Altona (1962)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Drama|History</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       imdbId                            Imdb Link  \\\n",
       "0      160905   http://www.imdb.com/title/tt160905   \n",
       "1      427531   http://www.imdb.com/title/tt427531   \n",
       "2     4175088  http://www.imdb.com/title/tt4175088   \n",
       "3     1686053  http://www.imdb.com/title/tt1686053   \n",
       "4       22921    http://www.imdb.com/title/tt22921   \n",
       "...       ...                                  ...   \n",
       "1349   821638   http://www.imdb.com/title/tt821638   \n",
       "1350    87835    http://www.imdb.com/title/tt87835   \n",
       "1351   103710   http://www.imdb.com/title/tt103710   \n",
       "1352   108265   http://www.imdb.com/title/tt108265   \n",
       "1353    56468    http://www.imdb.com/title/tt56468   \n",
       "\n",
       "                                     Title  IMDB Score  \\\n",
       "0                      Spooky House (2002)         5.4   \n",
       "1                              Mezzo Forte         6.9   \n",
       "2                     Radical Grace (2015)         8.3   \n",
       "3                            Apnoia (2010)         5.6   \n",
       "4              Broadway to Cheyenne (1932)         4.8   \n",
       "...                                    ...         ...   \n",
       "1349  Bury My Heart at Wounded Knee (2007)         7.2   \n",
       "1350             Oh, God! You Devil (1984)         5.3   \n",
       "1351                 AprÌ¬s l'amour (1992)         6.3   \n",
       "1352                     Swing Kids (1993)         6.8   \n",
       "1353        I sequestrati di Altona (1962)         7.0   \n",
       "\n",
       "                       Genre  \\\n",
       "0              Comedy|Family   \n",
       "1     Animation|Comedy|Crime   \n",
       "2           Documentary|News   \n",
       "3             Drama|Thriller   \n",
       "4                    Western   \n",
       "...                      ...   \n",
       "1349   Drama|History|Western   \n",
       "1350          Comedy|Fantasy   \n",
       "1351           Drama|Romance   \n",
       "1352             Drama|Music   \n",
       "1353           Drama|History   \n",
       "\n",
       "                                                 Poster  \\\n",
       "0     https://images-na.ssl-images-amazon.com/images...   \n",
       "1     https://images-na.ssl-images-amazon.com/images...   \n",
       "2     https://images-na.ssl-images-amazon.com/images...   \n",
       "3     https://images-na.ssl-images-amazon.com/images...   \n",
       "4     https://images-na.ssl-images-amazon.com/images...   \n",
       "...                                                 ...   \n",
       "1349  https://images-na.ssl-images-amazon.com/images...   \n",
       "1350  https://images-na.ssl-images-amazon.com/images...   \n",
       "1351  https://images-na.ssl-images-amazon.com/images...   \n",
       "1352  https://images-na.ssl-images-amazon.com/images...   \n",
       "1353  https://images-na.ssl-images-amazon.com/images...   \n",
       "\n",
       "                                               genrelst  \n",
       "0     [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "1349  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1350  [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "1351  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1352  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "1353  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "\n",
       "[1354 rows x 7 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df_reset = y_df.reset_index()\n",
    "\n",
    "shape = y_df_reset.shape[1]\n",
    "\n",
    "index_value = []\n",
    "genre_lst = []\n",
    "\n",
    "for i in range(len(y_df_reset)):\n",
    "    index_value.append(int(y_df_reset.loc[i,\"index\"]))\n",
    "    temp_list = []\n",
    "    for j in y_df_reset.columns[1:]:\n",
    "        temp_list.append(y_df_reset.loc[i,j])\n",
    "    genre_lst.append(temp_list)\n",
    "\n",
    "df = pd.DataFrame(list(zip(index_value, genre_lst)),\n",
    "               columns =['imdbId', 'genrelst'])\n",
    "\n",
    "result = dataset2.merge(df, on=\"imdbId\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(result)):\n",
    "    tempY.append((int(result['imdbId'].iloc[x]),result['genrelst'].iloc[x]))\n",
    "\n",
    "#tempY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting the length of training data\n",
    "length=int(len(flist)*training_size)\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the data about the images that are available\n",
    "i=0\n",
    "for filename in flist:\n",
    "    name=int(filename.split('/')[-1][:-4])\n",
    "    for z in tempY:\n",
    "        if(z[0]==name):\n",
    "            \n",
    "            img = array(cv2.imread(filename))\n",
    "            img = swapaxes(img, 2,0)\n",
    "            img = swapaxes(img, 2,1)\n",
    "\n",
    "            if(i<length):\n",
    "                x_train.append(img)\n",
    "                y_train.append(z[1])\n",
    "                i+=1\n",
    "            else:\n",
    "                x_test.append(img)\n",
    "                y_test.append(z[1])\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data from lists to numpy arrays\n",
    "x_train=asarray(x_train,dtype=float)\n",
    "x_test=asarray(x_test,dtype=float)\n",
    "y_train=asarray(y_train,dtype=float)\n",
    "y_test=asarray(y_test,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling down the RGB data\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (947, 3, 268, 182)\n",
      "947 train samples\n",
      "407 test samples\n"
     ]
    }
   ],
   "source": [
    "#printing stats about the features\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = x_train.shape[0]\n",
    "\n",
    "x_train=torch.from_numpy(x_train)\n",
    "x_test=torch.from_numpy(x_test)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_test=torch.from_numpy(y_test)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train, y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(x_test, y_test)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric calculation\n",
    "\n",
    "def metric(scores, targets):\n",
    "    \"\"\"\n",
    "    :param scores: the output the model predict\n",
    "    :param targets: the gt label\n",
    "    :return: OP, OR, OF1, CP, CR, CF1\n",
    "    calculate the Precision of every class by: TP/TP+FP i.e. TP/total predict\n",
    "    calculate the Recall by: TP/total GT\n",
    "    \"\"\"\n",
    "    num, num_class = scores.shape\n",
    "    gt_num = np.zeros(num_class)\n",
    "    tp_num = np.zeros(num_class)\n",
    "    predict_num = np.zeros(num_class)\n",
    "\n",
    "\n",
    "    for index in range(num_class):\n",
    "        score = scores[:, index]\n",
    "        target = targets[:, index]\n",
    "\n",
    "        gt_num[index] = np.sum(target == 1)\n",
    "        predict_num[index] = np.sum(score >= 0.5)\n",
    "        tp_num[index] = np.sum(target * (score >= 0.5))\n",
    "\n",
    "    predict_num[predict_num == 0] = 1  # avoid dividing 0\n",
    "    OP = np.sum(tp_num) / np.sum(predict_num) #OP (Overall Precision) is the ratio of the number of correctly predicted positive samples to the total number of positive predictions made by the model\n",
    "    OR = np.sum(tp_num) / np.sum(gt_num) #OR (Overall Recall) is the ratio of the number of correctly predicted positive samples to the total number of positive samples in the ground truth.\n",
    "    OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n",
    "\n",
    "    return OP, OR, OF1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 model\n",
    "from torchvision import models\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(2048, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNet()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/947 (0%)]\tLoss: 0.694314 \tOP: 0.020408\tOR: 0.015385\tOF1: 0.017544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-71-447663f93c68>:28: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [32/947 (3%)]\tLoss: 0.405906 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [64/947 (7%)]\tLoss: 0.302793 \tOP: 0.041667\tOR: 0.014706\tOF1: 0.021739\n",
      "Train Epoch: 0 [96/947 (10%)]\tLoss: 0.295557 \tOP: 0.125000\tOR: 0.052632\tOF1: 0.074074\n",
      "Train Epoch: 0 [128/947 (13%)]\tLoss: 0.272327 \tOP: 0.068966\tOR: 0.029412\tOF1: 0.041237\n",
      "Train Epoch: 0 [160/947 (17%)]\tLoss: 0.276444 \tOP: 0.205882\tOR: 0.100000\tOF1: 0.134615\n",
      "Train Epoch: 0 [192/947 (20%)]\tLoss: 0.256322 \tOP: 0.272727\tOR: 0.140625\tOF1: 0.185567\n",
      "Train Epoch: 0 [224/947 (23%)]\tLoss: 0.312750 \tOP: 0.129032\tOR: 0.055556\tOF1: 0.077670\n",
      "Train Epoch: 0 [256/947 (27%)]\tLoss: 0.292756 \tOP: 0.142857\tOR: 0.070423\tOF1: 0.094340\n",
      "Train Epoch: 0 [288/947 (30%)]\tLoss: 0.237431 \tOP: 0.083333\tOR: 0.032258\tOF1: 0.046512\n",
      "Train Epoch: 0 [320/947 (33%)]\tLoss: 0.293724 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [352/947 (37%)]\tLoss: 0.264802 \tOP: 0.040000\tOR: 0.013699\tOF1: 0.020408\n",
      "Train Epoch: 0 [384/947 (40%)]\tLoss: 0.254141 \tOP: 0.041667\tOR: 0.015873\tOF1: 0.022989\n",
      "Train Epoch: 0 [416/947 (43%)]\tLoss: 0.218885 \tOP: 0.083333\tOR: 0.032258\tOF1: 0.046512\n",
      "Train Epoch: 0 [448/947 (47%)]\tLoss: 0.235443 \tOP: 0.083333\tOR: 0.031250\tOF1: 0.045455\n",
      "Train Epoch: 0 [480/947 (50%)]\tLoss: 0.250297 \tOP: 0.080000\tOR: 0.027778\tOF1: 0.041237\n",
      "Train Epoch: 0 [512/947 (53%)]\tLoss: 0.265553 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [544/947 (57%)]\tLoss: 0.261306 \tOP: 0.111111\tOR: 0.045455\tOF1: 0.064516\n",
      "Train Epoch: 0 [576/947 (60%)]\tLoss: 0.300981 \tOP: 0.115385\tOR: 0.041667\tOF1: 0.061224\n",
      "Train Epoch: 0 [608/947 (63%)]\tLoss: 0.264964 \tOP: 0.107143\tOR: 0.042857\tOF1: 0.061224\n",
      "Train Epoch: 0 [640/947 (67%)]\tLoss: 0.274744 \tOP: 0.068966\tOR: 0.026316\tOF1: 0.038095\n",
      "Train Epoch: 0 [672/947 (70%)]\tLoss: 0.280069 \tOP: 0.076923\tOR: 0.027778\tOF1: 0.040816\n",
      "Train Epoch: 0 [704/947 (73%)]\tLoss: 0.262877 \tOP: 0.133333\tOR: 0.057143\tOF1: 0.080000\n",
      "Train Epoch: 0 [736/947 (77%)]\tLoss: 0.237163 \tOP: 0.172414\tOR: 0.076923\tOF1: 0.106383\n",
      "Train Epoch: 0 [768/947 (80%)]\tLoss: 0.266278 \tOP: 0.151515\tOR: 0.076923\tOF1: 0.102041\n",
      "Train Epoch: 0 [800/947 (83%)]\tLoss: 0.262136 \tOP: 0.257143\tOR: 0.134328\tOF1: 0.176471\n",
      "Train Epoch: 0 [832/947 (87%)]\tLoss: 0.237834 \tOP: 0.156250\tOR: 0.072464\tOF1: 0.099010\n",
      "Train Epoch: 0 [864/947 (90%)]\tLoss: 0.236449 \tOP: 0.193548\tOR: 0.098361\tOF1: 0.130435\n",
      "Train Epoch: 0 [896/947 (93%)]\tLoss: 0.227100 \tOP: 0.133333\tOR: 0.062500\tOF1: 0.085106\n",
      "Train Epoch: 0 [551/947 (97%)]\tLoss: 0.258156 \tOP: 0.076923\tOR: 0.048780\tOF1: 0.059701\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-ea5e5cd9fa3b>:45: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 1.3307 \n",
      "OP: 0.194444\n",
      "OR: 0.132075\n",
      "OF1: 0.157303\n",
      "\n",
      "Train Epoch: 1 [0/947 (0%)]\tLoss: 0.245240 \tOP: 0.074074\tOR: 0.031746\tOF1: 0.044444\n",
      "Train Epoch: 1 [32/947 (3%)]\tLoss: 0.234321 \tOP: 0.161290\tOR: 0.070423\tOF1: 0.098039\n",
      "Train Epoch: 1 [64/947 (7%)]\tLoss: 0.253127 \tOP: 0.187500\tOR: 0.084507\tOF1: 0.116505\n",
      "Train Epoch: 1 [96/947 (10%)]\tLoss: 0.233698 \tOP: 0.107143\tOR: 0.044776\tOF1: 0.063158\n",
      "Train Epoch: 1 [128/947 (13%)]\tLoss: 0.247340 \tOP: 0.178571\tOR: 0.070423\tOF1: 0.101010\n",
      "Train Epoch: 1 [160/947 (17%)]\tLoss: 0.254522 \tOP: 0.111111\tOR: 0.043478\tOF1: 0.062500\n",
      "Train Epoch: 1 [192/947 (20%)]\tLoss: 0.245844 \tOP: 0.115385\tOR: 0.042857\tOF1: 0.062500\n",
      "Train Epoch: 1 [224/947 (23%)]\tLoss: 0.257143 \tOP: 0.142857\tOR: 0.051948\tOF1: 0.076190\n",
      "Train Epoch: 1 [256/947 (27%)]\tLoss: 0.242512 \tOP: 0.142857\tOR: 0.059701\tOF1: 0.084211\n",
      "Train Epoch: 1 [288/947 (30%)]\tLoss: 0.237800 \tOP: 0.161290\tOR: 0.072464\tOF1: 0.100000\n",
      "Train Epoch: 1 [320/947 (33%)]\tLoss: 0.234496 \tOP: 0.218750\tOR: 0.095890\tOF1: 0.133333\n",
      "Train Epoch: 1 [352/947 (37%)]\tLoss: 0.215346 \tOP: 0.222222\tOR: 0.135593\tOF1: 0.168421\n",
      "Train Epoch: 1 [384/947 (40%)]\tLoss: 0.266945 \tOP: 0.111111\tOR: 0.055556\tOF1: 0.074074\n",
      "Train Epoch: 1 [416/947 (43%)]\tLoss: 0.261061 \tOP: 0.181818\tOR: 0.084507\tOF1: 0.115385\n",
      "Train Epoch: 1 [448/947 (47%)]\tLoss: 0.233551 \tOP: 0.242424\tOR: 0.119403\tOF1: 0.160000\n",
      "Train Epoch: 1 [480/947 (50%)]\tLoss: 0.254993 \tOP: 0.064516\tOR: 0.029412\tOF1: 0.040404\n",
      "Train Epoch: 1 [512/947 (53%)]\tLoss: 0.209505 \tOP: 0.193548\tOR: 0.092308\tOF1: 0.125000\n",
      "Train Epoch: 1 [544/947 (57%)]\tLoss: 0.228642 \tOP: 0.156250\tOR: 0.076923\tOF1: 0.103093\n",
      "Train Epoch: 1 [576/947 (60%)]\tLoss: 0.247643 \tOP: 0.156250\tOR: 0.074627\tOF1: 0.101010\n",
      "Train Epoch: 1 [608/947 (63%)]\tLoss: 0.241754 \tOP: 0.120000\tOR: 0.045455\tOF1: 0.065934\n",
      "Train Epoch: 1 [640/947 (67%)]\tLoss: 0.235222 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [672/947 (70%)]\tLoss: 0.232826 \tOP: 0.040000\tOR: 0.015385\tOF1: 0.022222\n",
      "Train Epoch: 1 [704/947 (73%)]\tLoss: 0.230679 \tOP: 0.040000\tOR: 0.016393\tOF1: 0.023256\n",
      "Train Epoch: 1 [736/947 (77%)]\tLoss: 0.244684 \tOP: 0.040000\tOR: 0.014706\tOF1: 0.021505\n",
      "Train Epoch: 1 [768/947 (80%)]\tLoss: 0.246700 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [800/947 (83%)]\tLoss: 0.198063 \tOP: 0.041667\tOR: 0.017544\tOF1: 0.024691\n",
      "Train Epoch: 1 [832/947 (87%)]\tLoss: 0.260840 \tOP: 0.041667\tOR: 0.013699\tOF1: 0.020619\n",
      "Train Epoch: 1 [864/947 (90%)]\tLoss: 0.259162 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [896/947 (93%)]\tLoss: 0.277016 \tOP: 0.041667\tOR: 0.013333\tOF1: 0.020202\n",
      "Train Epoch: 1 [551/947 (97%)]\tLoss: 0.224728 \tOP: 0.041667\tOR: 0.024390\tOF1: 0.030769\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2736 \n",
      "OP: 0.040000\n",
      "OR: 0.018868\n",
      "OF1: 0.025641\n",
      "\n",
      "Train Epoch: 2 [0/947 (0%)]\tLoss: 0.225025 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 2 [32/947 (3%)]\tLoss: 0.231080 \tOP: 0.074074\tOR: 0.029851\tOF1: 0.042553\n",
      "Train Epoch: 2 [64/947 (7%)]\tLoss: 0.209475 \tOP: 0.241379\tOR: 0.104478\tOF1: 0.145833\n",
      "Train Epoch: 2 [96/947 (10%)]\tLoss: 0.213288 \tOP: 0.206897\tOR: 0.096774\tOF1: 0.131868\n",
      "Train Epoch: 2 [128/947 (13%)]\tLoss: 0.274861 \tOP: 0.176471\tOR: 0.081081\tOF1: 0.111111\n",
      "Train Epoch: 2 [160/947 (17%)]\tLoss: 0.231869 \tOP: 0.250000\tOR: 0.117647\tOF1: 0.160000\n",
      "Train Epoch: 2 [192/947 (20%)]\tLoss: 0.246074 \tOP: 0.205882\tOR: 0.104478\tOF1: 0.138614\n",
      "Train Epoch: 2 [224/947 (23%)]\tLoss: 0.231328 \tOP: 0.181818\tOR: 0.088235\tOF1: 0.118812\n",
      "Train Epoch: 2 [256/947 (27%)]\tLoss: 0.233714 \tOP: 0.200000\tOR: 0.083333\tOF1: 0.117647\n",
      "Train Epoch: 2 [288/947 (30%)]\tLoss: 0.235445 \tOP: 0.148148\tOR: 0.064516\tOF1: 0.089888\n",
      "Train Epoch: 2 [320/947 (33%)]\tLoss: 0.214228 \tOP: 0.115385\tOR: 0.044118\tOF1: 0.063830\n",
      "Train Epoch: 2 [352/947 (37%)]\tLoss: 0.215424 \tOP: 0.041667\tOR: 0.015625\tOF1: 0.022727\n",
      "Train Epoch: 2 [384/947 (40%)]\tLoss: 0.224069 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 2 [416/947 (43%)]\tLoss: 0.262305 \tOP: 0.148148\tOR: 0.057143\tOF1: 0.082474\n",
      "Train Epoch: 2 [448/947 (47%)]\tLoss: 0.213051 \tOP: 0.214286\tOR: 0.089552\tOF1: 0.126316\n",
      "Train Epoch: 2 [480/947 (50%)]\tLoss: 0.213208 \tOP: 0.120000\tOR: 0.045455\tOF1: 0.065934\n",
      "Train Epoch: 2 [512/947 (53%)]\tLoss: 0.220218 \tOP: 0.228571\tOR: 0.125000\tOF1: 0.161616\n",
      "Train Epoch: 2 [544/947 (57%)]\tLoss: 0.228847 \tOP: 0.218750\tOR: 0.109375\tOF1: 0.145833\n",
      "Train Epoch: 2 [576/947 (60%)]\tLoss: 0.249128 \tOP: 0.315789\tOR: 0.169014\tOF1: 0.220183\n",
      "Train Epoch: 2 [608/947 (63%)]\tLoss: 0.232496 \tOP: 0.277778\tOR: 0.138889\tOF1: 0.185185\n",
      "Train Epoch: 2 [640/947 (67%)]\tLoss: 0.239595 \tOP: 0.236842\tOR: 0.134328\tOF1: 0.171429\n",
      "Train Epoch: 2 [672/947 (70%)]\tLoss: 0.250317 \tOP: 0.205882\tOR: 0.104478\tOF1: 0.138614\n",
      "Train Epoch: 2 [704/947 (73%)]\tLoss: 0.245091 \tOP: 0.161290\tOR: 0.074627\tOF1: 0.102041\n",
      "Train Epoch: 2 [736/947 (77%)]\tLoss: 0.258856 \tOP: 0.107143\tOR: 0.040000\tOF1: 0.058252\n",
      "Train Epoch: 2 [768/947 (80%)]\tLoss: 0.259011 \tOP: 0.107143\tOR: 0.044776\tOF1: 0.063158\n",
      "Train Epoch: 2 [800/947 (83%)]\tLoss: 0.233561 \tOP: 0.178571\tOR: 0.075758\tOF1: 0.106383\n",
      "Train Epoch: 2 [832/947 (87%)]\tLoss: 0.237603 \tOP: 0.083333\tOR: 0.030769\tOF1: 0.044944\n",
      "Train Epoch: 2 [864/947 (90%)]\tLoss: 0.262148 \tOP: 0.178571\tOR: 0.067568\tOF1: 0.098039\n",
      "Train Epoch: 2 [896/947 (93%)]\tLoss: 0.262641 \tOP: 0.115385\tOR: 0.040541\tOF1: 0.060000\n",
      "Train Epoch: 2 [551/947 (97%)]\tLoss: 0.226827 \tOP: 0.040000\tOR: 0.025000\tOF1: 0.030769\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.3039 \n",
      "OP: 0.138889\n",
      "OR: 0.094340\n",
      "OF1: 0.112360\n",
      "\n",
      "Train Epoch: 3 [0/947 (0%)]\tLoss: 0.215417 \tOP: 0.241379\tOR: 0.101449\tOF1: 0.142857\n",
      "Train Epoch: 3 [32/947 (3%)]\tLoss: 0.262103 \tOP: 0.225806\tOR: 0.089744\tOF1: 0.128440\n",
      "Train Epoch: 3 [64/947 (7%)]\tLoss: 0.219914 \tOP: 0.206897\tOR: 0.085714\tOF1: 0.121212\n",
      "Train Epoch: 3 [96/947 (10%)]\tLoss: 0.222943 \tOP: 0.257143\tOR: 0.136364\tOF1: 0.178218\n",
      "Train Epoch: 3 [128/947 (13%)]\tLoss: 0.225178 \tOP: 0.352941\tOR: 0.173913\tOF1: 0.233010\n",
      "Train Epoch: 3 [160/947 (17%)]\tLoss: 0.225740 \tOP: 0.218750\tOR: 0.107692\tOF1: 0.144330\n",
      "Train Epoch: 3 [192/947 (20%)]\tLoss: 0.233033 \tOP: 0.250000\tOR: 0.132353\tOF1: 0.173077\n",
      "Train Epoch: 3 [224/947 (23%)]\tLoss: 0.203816 \tOP: 0.307692\tOR: 0.193548\tOF1: 0.237624\n",
      "Train Epoch: 3 [256/947 (27%)]\tLoss: 0.218844 \tOP: 0.375000\tOR: 0.220588\tOF1: 0.277778\n",
      "Train Epoch: 3 [288/947 (30%)]\tLoss: 0.256261 \tOP: 0.277778\tOR: 0.142857\tOF1: 0.188679\n",
      "Train Epoch: 3 [320/947 (33%)]\tLoss: 0.248577 \tOP: 0.236842\tOR: 0.136364\tOF1: 0.173077\n",
      "Train Epoch: 3 [352/947 (37%)]\tLoss: 0.256020 \tOP: 0.228571\tOR: 0.112676\tOF1: 0.150943\n",
      "Train Epoch: 3 [384/947 (40%)]\tLoss: 0.209425 \tOP: 0.242424\tOR: 0.129032\tOF1: 0.168421\n",
      "Train Epoch: 3 [416/947 (43%)]\tLoss: 0.235594 \tOP: 0.142857\tOR: 0.071429\tOF1: 0.095238\n",
      "Train Epoch: 3 [448/947 (47%)]\tLoss: 0.239984 \tOP: 0.258065\tOR: 0.108108\tOF1: 0.152381\n",
      "Train Epoch: 3 [480/947 (50%)]\tLoss: 0.219508 \tOP: 0.172414\tOR: 0.078125\tOF1: 0.107527\n",
      "Train Epoch: 3 [512/947 (53%)]\tLoss: 0.242935 \tOP: 0.200000\tOR: 0.085714\tOF1: 0.120000\n",
      "Train Epoch: 3 [544/947 (57%)]\tLoss: 0.233732 \tOP: 0.111111\tOR: 0.041667\tOF1: 0.060606\n",
      "Train Epoch: 3 [576/947 (60%)]\tLoss: 0.238967 \tOP: 0.200000\tOR: 0.082192\tOF1: 0.116505\n",
      "Train Epoch: 3 [608/947 (63%)]\tLoss: 0.216112 \tOP: 0.250000\tOR: 0.120690\tOF1: 0.162791\n",
      "Train Epoch: 3 [640/947 (67%)]\tLoss: 0.228418 \tOP: 0.193548\tOR: 0.088235\tOF1: 0.121212\n",
      "Train Epoch: 3 [672/947 (70%)]\tLoss: 0.241816 \tOP: 0.218750\tOR: 0.093333\tOF1: 0.130841\n",
      "Train Epoch: 3 [704/947 (73%)]\tLoss: 0.242282 \tOP: 0.233333\tOR: 0.107692\tOF1: 0.147368\n",
      "Train Epoch: 3 [736/947 (77%)]\tLoss: 0.234624 \tOP: 0.250000\tOR: 0.114286\tOF1: 0.156863\n",
      "Train Epoch: 3 [768/947 (80%)]\tLoss: 0.213627 \tOP: 0.218750\tOR: 0.114754\tOF1: 0.150538\n",
      "Train Epoch: 3 [800/947 (83%)]\tLoss: 0.214986 \tOP: 0.281250\tOR: 0.136364\tOF1: 0.183673\n",
      "Train Epoch: 3 [832/947 (87%)]\tLoss: 0.245641 \tOP: 0.096774\tOR: 0.046154\tOF1: 0.062500\n",
      "Train Epoch: 3 [864/947 (90%)]\tLoss: 0.228621 \tOP: 0.225806\tOR: 0.106061\tOF1: 0.144330\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-ea5e5cd9fa3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-ea5e5cd9fa3b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-9d9cde8a2a0b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "        \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for n in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for m in range(len(classes)):\n",
    "        #        if target[n][m] == 1 or preds[n][m] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[n][m] == target[n][m]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# DenseNet201\n",
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "model2 = torchvision.models.densenet201(pretrained=True)\n",
    "num_ftrs = model2.classifier.in_features\n",
    "model2.classifier = nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1083 (0%)]\tLoss: 0.732575 \tOP: 0.088608\tOR: 0.097222\tOF1: 0.092715\n",
      "Train Epoch: 0 [32/1083 (3%)]\tLoss: 0.707817 \tOP: 0.136986\tOR: 0.133333\tOF1: 0.135135\n",
      "Train Epoch: 0 [64/1083 (6%)]\tLoss: 0.680108 \tOP: 0.140000\tOR: 0.101449\tOF1: 0.117647\n",
      "Train Epoch: 0 [96/1083 (9%)]\tLoss: 0.667057 \tOP: 0.069767\tOR: 0.048387\tOF1: 0.057143\n",
      "Train Epoch: 0 [128/1083 (12%)]\tLoss: 0.645402 \tOP: 0.027778\tOR: 0.014493\tOF1: 0.019048\n",
      "Train Epoch: 0 [160/1083 (15%)]\tLoss: 0.611889 \tOP: 0.138889\tOR: 0.079365\tOF1: 0.101010\n",
      "Train Epoch: 0 [192/1083 (18%)]\tLoss: 0.600936 \tOP: 0.103448\tOR: 0.042857\tOF1: 0.060606\n",
      "Train Epoch: 0 [224/1083 (21%)]\tLoss: 0.581095 \tOP: 0.040000\tOR: 0.015385\tOF1: 0.022222\n",
      "Train Epoch: 0 [256/1083 (24%)]\tLoss: 0.567764 \tOP: 0.038462\tOR: 0.014085\tOF1: 0.020619\n",
      "Train Epoch: 0 [288/1083 (26%)]\tLoss: 0.546047 \tOP: 0.040000\tOR: 0.015152\tOF1: 0.021978\n",
      "Train Epoch: 0 [320/1083 (29%)]\tLoss: 0.524420 \tOP: 0.148148\tOR: 0.055556\tOF1: 0.080808\n",
      "Train Epoch: 0 [352/1083 (32%)]\tLoss: 0.510825 \tOP: 0.076923\tOR: 0.028169\tOF1: 0.041237\n",
      "Train Epoch: 0 [384/1083 (35%)]\tLoss: 0.498525 \tOP: 0.041667\tOR: 0.014085\tOF1: 0.021053\n",
      "Train Epoch: 0 [416/1083 (38%)]\tLoss: 0.478165 \tOP: 0.083333\tOR: 0.027778\tOF1: 0.041667\n",
      "Train Epoch: 0 [448/1083 (41%)]\tLoss: 0.455891 \tOP: 0.080000\tOR: 0.032787\tOF1: 0.046512\n",
      "Train Epoch: 0 [480/1083 (44%)]\tLoss: 0.464799 \tOP: 0.115385\tOR: 0.041096\tOF1: 0.060606\n",
      "Train Epoch: 0 [512/1083 (47%)]\tLoss: 0.446531 \tOP: 0.076923\tOR: 0.028571\tOF1: 0.041667\n",
      "Train Epoch: 0 [544/1083 (50%)]\tLoss: 0.433952 \tOP: 0.076923\tOR: 0.027027\tOF1: 0.040000\n",
      "Train Epoch: 0 [576/1083 (53%)]\tLoss: 0.417255 \tOP: 0.040000\tOR: 0.015385\tOF1: 0.022222\n",
      "Train Epoch: 0 [608/1083 (56%)]\tLoss: 0.405292 \tOP: 0.074074\tOR: 0.028986\tOF1: 0.041667\n",
      "Train Epoch: 0 [640/1083 (59%)]\tLoss: 0.392836 \tOP: 0.107143\tOR: 0.044776\tOF1: 0.063158\n",
      "Train Epoch: 0 [672/1083 (62%)]\tLoss: 0.384446 \tOP: 0.076923\tOR: 0.031746\tOF1: 0.044944\n",
      "Train Epoch: 0 [704/1083 (65%)]\tLoss: 0.368415 \tOP: 0.041667\tOR: 0.015873\tOF1: 0.022989\n",
      "Train Epoch: 0 [736/1083 (68%)]\tLoss: 0.363796 \tOP: 0.071429\tOR: 0.031250\tOF1: 0.043478\n",
      "Train Epoch: 0 [768/1083 (71%)]\tLoss: 0.363952 \tOP: 0.074074\tOR: 0.028571\tOF1: 0.041237\n",
      "Train Epoch: 0 [800/1083 (74%)]\tLoss: 0.348020 \tOP: 0.076923\tOR: 0.031746\tOF1: 0.044944\n",
      "Train Epoch: 0 [832/1083 (76%)]\tLoss: 0.348457 \tOP: 0.076923\tOR: 0.030769\tOF1: 0.043956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-bbf89c8d258d>:40: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [864/1083 (79%)]\tLoss: 0.342054 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [896/1083 (82%)]\tLoss: 0.334149 \tOP: 0.153846\tOR: 0.054795\tOF1: 0.080808\n",
      "Train Epoch: 0 [928/1083 (85%)]\tLoss: 0.329278 \tOP: 0.080000\tOR: 0.029851\tOF1: 0.043478\n",
      "Train Epoch: 0 [960/1083 (88%)]\tLoss: 0.321470 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [992/1083 (91%)]\tLoss: 0.305416 \tOP: 0.041667\tOR: 0.014706\tOF1: 0.021739\n",
      "Train Epoch: 0 [1024/1083 (94%)]\tLoss: 0.314984 \tOP: 0.080000\tOR: 0.027778\tOF1: 0.041237\n",
      "Train Epoch: 0 [891/1083 (97%)]\tLoss: 0.315591 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-78eea7326411>:45: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.3080 \n",
      "OP: 0.040000\n",
      "OR: 0.028571\n",
      "OF1: 0.033333\n",
      "\n",
      "Train Epoch: 1 [0/1083 (0%)]\tLoss: 0.273257 \tOP: 0.153846\tOR: 0.058824\tOF1: 0.085106\n",
      "Train Epoch: 1 [32/1083 (3%)]\tLoss: 0.261681 \tOP: 0.206897\tOR: 0.090909\tOF1: 0.126316\n",
      "Train Epoch: 1 [64/1083 (6%)]\tLoss: 0.244387 \tOP: 0.333333\tOR: 0.180328\tOF1: 0.234043\n",
      "Train Epoch: 1 [96/1083 (9%)]\tLoss: 0.249878 \tOP: 0.290323\tOR: 0.145161\tOF1: 0.193548\n",
      "Train Epoch: 1 [128/1083 (12%)]\tLoss: 0.238852 \tOP: 0.241379\tOR: 0.106061\tOF1: 0.147368\n",
      "Train Epoch: 1 [160/1083 (15%)]\tLoss: 0.241438 \tOP: 0.153846\tOR: 0.058824\tOF1: 0.085106\n",
      "Train Epoch: 1 [192/1083 (18%)]\tLoss: 0.250312 \tOP: 0.206897\tOR: 0.082192\tOF1: 0.117647\n",
      "Train Epoch: 1 [224/1083 (21%)]\tLoss: 0.224105 \tOP: 0.371429\tOR: 0.213115\tOF1: 0.270833\n",
      "Train Epoch: 1 [256/1083 (24%)]\tLoss: 0.219151 \tOP: 0.266667\tOR: 0.125000\tOF1: 0.170213\n",
      "Train Epoch: 1 [288/1083 (26%)]\tLoss: 0.237909 \tOP: 0.153846\tOR: 0.058824\tOF1: 0.085106\n",
      "Train Epoch: 1 [320/1083 (29%)]\tLoss: 0.234306 \tOP: 0.352941\tOR: 0.160000\tOF1: 0.220183\n",
      "Train Epoch: 1 [352/1083 (32%)]\tLoss: 0.229571 \tOP: 0.178571\tOR: 0.072464\tOF1: 0.103093\n",
      "Train Epoch: 1 [384/1083 (35%)]\tLoss: 0.236794 \tOP: 0.214286\tOR: 0.082192\tOF1: 0.118812\n",
      "Train Epoch: 1 [416/1083 (38%)]\tLoss: 0.226759 \tOP: 0.312500\tOR: 0.142857\tOF1: 0.196078\n",
      "Train Epoch: 1 [448/1083 (41%)]\tLoss: 0.216574 \tOP: 0.333333\tOR: 0.164179\tOF1: 0.220000\n",
      "Train Epoch: 1 [480/1083 (44%)]\tLoss: 0.226856 \tOP: 0.352941\tOR: 0.166667\tOF1: 0.226415\n",
      "Train Epoch: 1 [512/1083 (47%)]\tLoss: 0.212984 \tOP: 0.361111\tOR: 0.180556\tOF1: 0.240741\n",
      "Train Epoch: 1 [544/1083 (50%)]\tLoss: 0.206020 \tOP: 0.352941\tOR: 0.184615\tOF1: 0.242424\n",
      "Train Epoch: 1 [576/1083 (53%)]\tLoss: 0.221879 \tOP: 0.351351\tOR: 0.185714\tOF1: 0.242991\n",
      "Train Epoch: 1 [608/1083 (56%)]\tLoss: 0.219578 \tOP: 0.352941\tOR: 0.171429\tOF1: 0.230769\n",
      "Train Epoch: 1 [640/1083 (59%)]\tLoss: 0.218697 \tOP: 0.250000\tOR: 0.117647\tOF1: 0.160000\n",
      "Train Epoch: 1 [672/1083 (62%)]\tLoss: 0.213043 \tOP: 0.342857\tOR: 0.176471\tOF1: 0.233010\n",
      "Train Epoch: 1 [704/1083 (65%)]\tLoss: 0.216538 \tOP: 0.281250\tOR: 0.126761\tOF1: 0.174757\n",
      "Train Epoch: 1 [736/1083 (68%)]\tLoss: 0.221604 \tOP: 0.361111\tOR: 0.180556\tOF1: 0.240741\n",
      "Train Epoch: 1 [768/1083 (71%)]\tLoss: 0.217474 \tOP: 0.314286\tOR: 0.161765\tOF1: 0.213592\n",
      "Train Epoch: 1 [800/1083 (74%)]\tLoss: 0.199759 \tOP: 0.352941\tOR: 0.196721\tOF1: 0.252632\n",
      "Train Epoch: 1 [832/1083 (76%)]\tLoss: 0.179900 \tOP: 0.410256\tOR: 0.271186\tOF1: 0.326531\n",
      "Train Epoch: 1 [864/1083 (79%)]\tLoss: 0.210547 \tOP: 0.333333\tOR: 0.152778\tOF1: 0.209524\n",
      "Train Epoch: 1 [896/1083 (82%)]\tLoss: 0.195072 \tOP: 0.388889\tOR: 0.225806\tOF1: 0.285714\n",
      "Train Epoch: 1 [928/1083 (85%)]\tLoss: 0.219061 \tOP: 0.342857\tOR: 0.169014\tOF1: 0.226415\n",
      "Train Epoch: 1 [960/1083 (88%)]\tLoss: 0.210470 \tOP: 0.394737\tOR: 0.205479\tOF1: 0.270270\n",
      "Train Epoch: 1 [992/1083 (91%)]\tLoss: 0.214075 \tOP: 0.371429\tOR: 0.173333\tOF1: 0.236364\n",
      "Train Epoch: 1 [1024/1083 (94%)]\tLoss: 0.200622 \tOP: 0.378378\tOR: 0.200000\tOF1: 0.261682\n",
      "Train Epoch: 1 [891/1083 (97%)]\tLoss: 0.202544 \tOP: 0.290323\tOR: 0.155172\tOF1: 0.202247\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2486 \n",
      "OP: 0.172414\n",
      "OR: 0.142857\n",
      "OF1: 0.156250\n",
      "\n",
      "Train Epoch: 2 [0/1083 (0%)]\tLoss: 0.175682 \tOP: 0.378378\tOR: 0.225806\tOF1: 0.282828\n",
      "Train Epoch: 2 [32/1083 (3%)]\tLoss: 0.177330 \tOP: 0.405405\tOR: 0.223881\tOF1: 0.288462\n",
      "Train Epoch: 2 [64/1083 (6%)]\tLoss: 0.169294 \tOP: 0.421053\tOR: 0.228571\tOF1: 0.296296\n",
      "Train Epoch: 2 [96/1083 (9%)]\tLoss: 0.163203 \tOP: 0.371429\tOR: 0.196970\tOF1: 0.257426\n",
      "Train Epoch: 2 [128/1083 (12%)]\tLoss: 0.173892 \tOP: 0.405405\tOR: 0.194805\tOF1: 0.263158\n",
      "Train Epoch: 2 [160/1083 (15%)]\tLoss: 0.169367 \tOP: 0.400000\tOR: 0.218750\tOF1: 0.282828\n",
      "Train Epoch: 2 [192/1083 (18%)]\tLoss: 0.148308 \tOP: 0.461538\tOR: 0.285714\tOF1: 0.352941\n",
      "Train Epoch: 2 [224/1083 (21%)]\tLoss: 0.177295 \tOP: 0.421053\tOR: 0.216216\tOF1: 0.285714\n",
      "Train Epoch: 2 [256/1083 (24%)]\tLoss: 0.158767 \tOP: 0.371429\tOR: 0.183099\tOF1: 0.245283\n",
      "Train Epoch: 2 [288/1083 (26%)]\tLoss: 0.148544 \tOP: 0.432432\tOR: 0.246154\tOF1: 0.313725\n",
      "Train Epoch: 2 [320/1083 (29%)]\tLoss: 0.159693 \tOP: 0.432432\tOR: 0.246154\tOF1: 0.313725\n",
      "Train Epoch: 2 [352/1083 (32%)]\tLoss: 0.154099 \tOP: 0.435897\tOR: 0.257576\tOF1: 0.323810\n",
      "Train Epoch: 2 [384/1083 (35%)]\tLoss: 0.172197 \tOP: 0.421053\tOR: 0.219178\tOF1: 0.288288\n",
      "Train Epoch: 2 [416/1083 (38%)]\tLoss: 0.171346 \tOP: 0.405405\tOR: 0.200000\tOF1: 0.267857\n",
      "Train Epoch: 2 [448/1083 (41%)]\tLoss: 0.160490 \tOP: 0.461538\tOR: 0.257143\tOF1: 0.330275\n",
      "Train Epoch: 2 [480/1083 (44%)]\tLoss: 0.147716 \tOP: 0.421053\tOR: 0.238806\tOF1: 0.304762\n",
      "Train Epoch: 2 [512/1083 (47%)]\tLoss: 0.180109 \tOP: 0.435897\tOR: 0.215190\tOF1: 0.288136\n",
      "Train Epoch: 2 [544/1083 (50%)]\tLoss: 0.150126 \tOP: 0.500000\tOR: 0.338983\tOF1: 0.404040\n",
      "Train Epoch: 2 [576/1083 (53%)]\tLoss: 0.156296 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 2 [608/1083 (56%)]\tLoss: 0.147878 \tOP: 0.523810\tOR: 0.323529\tOF1: 0.400000\n",
      "Train Epoch: 2 [640/1083 (59%)]\tLoss: 0.170116 \tOP: 0.487805\tOR: 0.253165\tOF1: 0.333333\n",
      "Train Epoch: 2 [672/1083 (62%)]\tLoss: 0.148120 \tOP: 0.463415\tOR: 0.292308\tOF1: 0.358491\n",
      "Train Epoch: 2 [704/1083 (65%)]\tLoss: 0.171451 \tOP: 0.447368\tOR: 0.236111\tOF1: 0.309091\n",
      "Train Epoch: 2 [736/1083 (68%)]\tLoss: 0.149093 \tOP: 0.512195\tOR: 0.318182\tOF1: 0.392523\n",
      "Train Epoch: 2 [768/1083 (71%)]\tLoss: 0.165247 \tOP: 0.388889\tOR: 0.197183\tOF1: 0.261682\n",
      "Train Epoch: 2 [800/1083 (74%)]\tLoss: 0.141323 \tOP: 0.534884\tOR: 0.353846\tOF1: 0.425926\n",
      "Train Epoch: 2 [832/1083 (76%)]\tLoss: 0.156201 \tOP: 0.463415\tOR: 0.260274\tOF1: 0.333333\n",
      "Train Epoch: 2 [864/1083 (79%)]\tLoss: 0.146774 \tOP: 0.500000\tOR: 0.308824\tOF1: 0.381818\n",
      "Train Epoch: 2 [896/1083 (82%)]\tLoss: 0.136919 \tOP: 0.522727\tOR: 0.359375\tOF1: 0.425926\n",
      "Train Epoch: 2 [928/1083 (85%)]\tLoss: 0.153322 \tOP: 0.487179\tOR: 0.275362\tOF1: 0.351852\n",
      "Train Epoch: 2 [960/1083 (88%)]\tLoss: 0.164116 \tOP: 0.500000\tOR: 0.295775\tOF1: 0.371681\n",
      "Train Epoch: 2 [992/1083 (91%)]\tLoss: 0.141680 \tOP: 0.432432\tOR: 0.253968\tOF1: 0.320000\n",
      "Train Epoch: 2 [1024/1083 (94%)]\tLoss: 0.136227 \tOP: 0.523810\tOR: 0.372881\tOF1: 0.435644\n",
      "Train Epoch: 2 [891/1083 (97%)]\tLoss: 0.153592 \tOP: 0.432432\tOR: 0.290909\tOF1: 0.347826\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2355 \n",
      "OP: 0.172414\n",
      "OR: 0.142857\n",
      "OF1: 0.156250\n",
      "\n",
      "Train Epoch: 3 [0/1083 (0%)]\tLoss: 0.132695 \tOP: 0.617021\tOR: 0.408451\tOF1: 0.491525\n",
      "Train Epoch: 3 [32/1083 (3%)]\tLoss: 0.112738 \tOP: 0.583333\tOR: 0.417910\tOF1: 0.486957\n",
      "Train Epoch: 3 [64/1083 (6%)]\tLoss: 0.134005 \tOP: 0.581395\tOR: 0.352113\tOF1: 0.438596\n",
      "Train Epoch: 3 [96/1083 (9%)]\tLoss: 0.118200 \tOP: 0.571429\tOR: 0.413793\tOF1: 0.480000\n",
      "Train Epoch: 3 [128/1083 (12%)]\tLoss: 0.141290 \tOP: 0.522727\tOR: 0.306667\tOF1: 0.386555\n",
      "Train Epoch: 3 [160/1083 (15%)]\tLoss: 0.113703 \tOP: 0.617021\tOR: 0.475410\tOF1: 0.537037\n",
      "Train Epoch: 3 [192/1083 (18%)]\tLoss: 0.116651 \tOP: 0.531915\tOR: 0.384615\tOF1: 0.446429\n",
      "Train Epoch: 3 [224/1083 (21%)]\tLoss: 0.131060 \tOP: 0.565217\tOR: 0.366197\tOF1: 0.444444\n",
      "Train Epoch: 3 [256/1083 (24%)]\tLoss: 0.120832 \tOP: 0.653061\tOR: 0.444444\tOF1: 0.528926\n",
      "Train Epoch: 3 [288/1083 (26%)]\tLoss: 0.123267 \tOP: 0.617021\tOR: 0.426471\tOF1: 0.504348\n",
      "Train Epoch: 3 [320/1083 (29%)]\tLoss: 0.122748 \tOP: 0.604167\tOR: 0.460317\tOF1: 0.522523\n",
      "Train Epoch: 3 [352/1083 (32%)]\tLoss: 0.123698 \tOP: 0.638298\tOR: 0.394737\tOF1: 0.487805\n",
      "Train Epoch: 3 [384/1083 (35%)]\tLoss: 0.115486 \tOP: 0.652174\tOR: 0.434783\tOF1: 0.521739\n",
      "Train Epoch: 3 [416/1083 (38%)]\tLoss: 0.115722 \tOP: 0.630435\tOR: 0.439394\tOF1: 0.517857\n",
      "Train Epoch: 3 [448/1083 (41%)]\tLoss: 0.111630 \tOP: 0.583333\tOR: 0.482759\tOF1: 0.528302\n",
      "Train Epoch: 3 [480/1083 (44%)]\tLoss: 0.123144 \tOP: 0.586957\tOR: 0.369863\tOF1: 0.453782\n",
      "Train Epoch: 3 [512/1083 (47%)]\tLoss: 0.122780 \tOP: 0.625000\tOR: 0.434783\tOF1: 0.512821\n",
      "Train Epoch: 3 [544/1083 (50%)]\tLoss: 0.127406 \tOP: 0.595238\tOR: 0.362319\tOF1: 0.450450\n",
      "Train Epoch: 3 [576/1083 (53%)]\tLoss: 0.103628 \tOP: 0.625000\tOR: 0.468750\tOF1: 0.535714\n",
      "Train Epoch: 3 [608/1083 (56%)]\tLoss: 0.113087 \tOP: 0.645833\tOR: 0.442857\tOF1: 0.525424\n",
      "Train Epoch: 3 [640/1083 (59%)]\tLoss: 0.116857 \tOP: 0.608696\tOR: 0.405797\tOF1: 0.486957\n",
      "Train Epoch: 3 [672/1083 (62%)]\tLoss: 0.120616 \tOP: 0.604651\tOR: 0.393939\tOF1: 0.477064\n",
      "Train Epoch: 3 [704/1083 (65%)]\tLoss: 0.118161 \tOP: 0.638298\tOR: 0.428571\tOF1: 0.512821\n",
      "Train Epoch: 3 [736/1083 (68%)]\tLoss: 0.129865 \tOP: 0.666667\tOR: 0.421053\tOF1: 0.516129\n",
      "Train Epoch: 3 [768/1083 (71%)]\tLoss: 0.113921 \tOP: 0.645833\tOR: 0.413333\tOF1: 0.504065\n",
      "Train Epoch: 3 [800/1083 (74%)]\tLoss: 0.106344 \tOP: 0.680851\tOR: 0.477612\tOF1: 0.561404\n",
      "Train Epoch: 3 [832/1083 (76%)]\tLoss: 0.124288 \tOP: 0.608696\tOR: 0.400000\tOF1: 0.482759\n",
      "Train Epoch: 3 [864/1083 (79%)]\tLoss: 0.095657 \tOP: 0.638298\tOR: 0.526316\tOF1: 0.576923\n",
      "Train Epoch: 3 [896/1083 (82%)]\tLoss: 0.113677 \tOP: 0.590909\tOR: 0.382353\tOF1: 0.464286\n",
      "Train Epoch: 3 [928/1083 (85%)]\tLoss: 0.122326 \tOP: 0.608696\tOR: 0.394366\tOF1: 0.478632\n",
      "Train Epoch: 3 [960/1083 (88%)]\tLoss: 0.116187 \tOP: 0.645833\tOR: 0.430556\tOF1: 0.516667\n",
      "Train Epoch: 3 [992/1083 (91%)]\tLoss: 0.133041 \tOP: 0.536585\tOR: 0.328358\tOF1: 0.407407\n",
      "Train Epoch: 3 [1024/1083 (94%)]\tLoss: 0.107490 \tOP: 0.666667\tOR: 0.477612\tOF1: 0.556522\n",
      "Train Epoch: 3 [891/1083 (97%)]\tLoss: 0.117041 \tOP: 0.604651\tOR: 0.456140\tOF1: 0.520000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2322 \n",
      "OP: 0.206897\n",
      "OR: 0.171429\n",
      "OF1: 0.187500\n",
      "\n",
      "Train Epoch: 4 [0/1083 (0%)]\tLoss: 0.099731 \tOP: 0.705882\tOR: 0.553846\tOF1: 0.620690\n",
      "Train Epoch: 4 [32/1083 (3%)]\tLoss: 0.095653 \tOP: 0.719298\tOR: 0.611940\tOF1: 0.661290\n",
      "Train Epoch: 4 [64/1083 (6%)]\tLoss: 0.085385 \tOP: 0.722222\tOR: 0.600000\tOF1: 0.655462\n",
      "Train Epoch: 4 [96/1083 (9%)]\tLoss: 0.102542 \tOP: 0.745763\tOR: 0.602740\tOF1: 0.666667\n",
      "Train Epoch: 4 [128/1083 (12%)]\tLoss: 0.095334 \tOP: 0.750000\tOR: 0.617647\tOF1: 0.677419\n",
      "Train Epoch: 4 [160/1083 (15%)]\tLoss: 0.101554 \tOP: 0.693878\tOR: 0.492754\tOF1: 0.576271\n",
      "Train Epoch: 4 [192/1083 (18%)]\tLoss: 0.092103 \tOP: 0.711538\tOR: 0.569231\tOF1: 0.632479\n",
      "Train Epoch: 4 [224/1083 (21%)]\tLoss: 0.088279 \tOP: 0.716981\tOR: 0.603175\tOF1: 0.655172\n",
      "Train Epoch: 4 [256/1083 (24%)]\tLoss: 0.094628 \tOP: 0.680851\tOR: 0.484848\tOF1: 0.566372\n",
      "Train Epoch: 4 [288/1083 (26%)]\tLoss: 0.098007 \tOP: 0.735849\tOR: 0.513158\tOF1: 0.604651\n",
      "Train Epoch: 4 [320/1083 (29%)]\tLoss: 0.079762 \tOP: 0.719298\tOR: 0.672131\tOF1: 0.694915\n",
      "Train Epoch: 4 [352/1083 (32%)]\tLoss: 0.080211 \tOP: 0.758621\tOR: 0.733333\tOF1: 0.745763\n",
      "Train Epoch: 4 [384/1083 (35%)]\tLoss: 0.089962 \tOP: 0.711538\tOR: 0.544118\tOF1: 0.616667\n",
      "Train Epoch: 4 [416/1083 (38%)]\tLoss: 0.083580 \tOP: 0.732143\tOR: 0.594203\tOF1: 0.656000\n",
      "Train Epoch: 4 [448/1083 (41%)]\tLoss: 0.110343 \tOP: 0.722222\tOR: 0.513158\tOF1: 0.600000\n",
      "Train Epoch: 4 [480/1083 (44%)]\tLoss: 0.102794 \tOP: 0.690909\tOR: 0.506667\tOF1: 0.584615\n",
      "Train Epoch: 4 [512/1083 (47%)]\tLoss: 0.095525 \tOP: 0.727273\tOR: 0.571429\tOF1: 0.640000\n",
      "Train Epoch: 4 [544/1083 (50%)]\tLoss: 0.106231 \tOP: 0.700000\tOR: 0.479452\tOF1: 0.569106\n",
      "Train Epoch: 4 [576/1083 (53%)]\tLoss: 0.093703 \tOP: 0.740741\tOR: 0.571429\tOF1: 0.645161\n",
      "Train Epoch: 4 [608/1083 (56%)]\tLoss: 0.088532 \tOP: 0.730769\tOR: 0.575758\tOF1: 0.644068\n",
      "Train Epoch: 4 [640/1083 (59%)]\tLoss: 0.080960 \tOP: 0.763636\tOR: 0.677419\tOF1: 0.717949\n",
      "Train Epoch: 4 [672/1083 (62%)]\tLoss: 0.095190 \tOP: 0.701754\tOR: 0.588235\tOF1: 0.640000\n",
      "Train Epoch: 4 [704/1083 (65%)]\tLoss: 0.090293 \tOP: 0.750000\tOR: 0.591549\tOF1: 0.661417\n",
      "Train Epoch: 4 [736/1083 (68%)]\tLoss: 0.097539 \tOP: 0.750000\tOR: 0.520000\tOF1: 0.614173\n",
      "Train Epoch: 4 [768/1083 (71%)]\tLoss: 0.088134 \tOP: 0.720000\tOR: 0.590164\tOF1: 0.648649\n",
      "Train Epoch: 4 [800/1083 (74%)]\tLoss: 0.082338 \tOP: 0.716981\tOR: 0.593750\tOF1: 0.649573\n",
      "Train Epoch: 4 [832/1083 (76%)]\tLoss: 0.087864 \tOP: 0.719298\tOR: 0.630769\tOF1: 0.672131\n",
      "Train Epoch: 4 [864/1083 (79%)]\tLoss: 0.086519 \tOP: 0.735849\tOR: 0.590909\tOF1: 0.655462\n",
      "Train Epoch: 4 [896/1083 (82%)]\tLoss: 0.094020 \tOP: 0.700000\tOR: 0.507246\tOF1: 0.588235\n",
      "Train Epoch: 4 [928/1083 (85%)]\tLoss: 0.093642 \tOP: 0.722222\tOR: 0.549296\tOF1: 0.624000\n",
      "Train Epoch: 4 [960/1083 (88%)]\tLoss: 0.085233 \tOP: 0.736842\tOR: 0.617647\tOF1: 0.672000\n",
      "Train Epoch: 4 [992/1083 (91%)]\tLoss: 0.093759 \tOP: 0.736842\tOR: 0.552632\tOF1: 0.631579\n",
      "Train Epoch: 4 [1024/1083 (94%)]\tLoss: 0.086260 \tOP: 0.765625\tOR: 0.680556\tOF1: 0.720588\n",
      "Train Epoch: 4 [891/1083 (97%)]\tLoss: 0.085284 \tOP: 0.702128\tOR: 0.600000\tOF1: 0.647059\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2341 \n",
      "OP: 0.258065\n",
      "OR: 0.228571\n",
      "OF1: 0.242424\n",
      "\n",
      "Train Epoch: 5 [0/1083 (0%)]\tLoss: 0.082723 \tOP: 0.771930\tOR: 0.628571\tOF1: 0.692913\n",
      "Train Epoch: 5 [32/1083 (3%)]\tLoss: 0.071664 \tOP: 0.775862\tOR: 0.714286\tOF1: 0.743802\n",
      "Train Epoch: 5 [64/1083 (6%)]\tLoss: 0.082233 \tOP: 0.754098\tOR: 0.647887\tOF1: 0.696970\n",
      "Train Epoch: 5 [96/1083 (9%)]\tLoss: 0.080942 \tOP: 0.779661\tOR: 0.647887\tOF1: 0.707692\n",
      "Train Epoch: 5 [128/1083 (12%)]\tLoss: 0.073296 \tOP: 0.835616\tOR: 0.802632\tOF1: 0.818792\n",
      "Train Epoch: 5 [160/1083 (15%)]\tLoss: 0.075455 \tOP: 0.774194\tOR: 0.695652\tOF1: 0.732824\n",
      "Train Epoch: 5 [192/1083 (18%)]\tLoss: 0.067902 \tOP: 0.777778\tOR: 0.753846\tOF1: 0.765625\n",
      "Train Epoch: 5 [224/1083 (21%)]\tLoss: 0.079511 \tOP: 0.762712\tOR: 0.608108\tOF1: 0.676692\n",
      "Train Epoch: 5 [256/1083 (24%)]\tLoss: 0.067190 \tOP: 0.766667\tOR: 0.730159\tOF1: 0.747967\n",
      "Train Epoch: 5 [288/1083 (26%)]\tLoss: 0.071800 \tOP: 0.777778\tOR: 0.710145\tOF1: 0.742424\n",
      "Train Epoch: 5 [320/1083 (29%)]\tLoss: 0.066782 \tOP: 0.812500\tOR: 0.800000\tOF1: 0.806202\n",
      "Train Epoch: 5 [352/1083 (32%)]\tLoss: 0.067542 \tOP: 0.803030\tOR: 0.746479\tOF1: 0.773723\n",
      "Train Epoch: 5 [384/1083 (35%)]\tLoss: 0.072402 \tOP: 0.761905\tOR: 0.695652\tOF1: 0.727273\n",
      "Train Epoch: 5 [416/1083 (38%)]\tLoss: 0.061590 \tOP: 0.808824\tOR: 0.859375\tOF1: 0.833333\n",
      "Train Epoch: 5 [448/1083 (41%)]\tLoss: 0.073154 \tOP: 0.774194\tOR: 0.676056\tOF1: 0.721805\n",
      "Train Epoch: 5 [480/1083 (44%)]\tLoss: 0.076737 \tOP: 0.740000\tOR: 0.569231\tOF1: 0.643478\n",
      "Train Epoch: 5 [512/1083 (47%)]\tLoss: 0.078641 \tOP: 0.750000\tOR: 0.617647\tOF1: 0.677419\n",
      "Train Epoch: 5 [544/1083 (50%)]\tLoss: 0.083020 \tOP: 0.716981\tOR: 0.535211\tOF1: 0.612903\n",
      "Train Epoch: 5 [576/1083 (53%)]\tLoss: 0.071396 \tOP: 0.765625\tOR: 0.700000\tOF1: 0.731343\n",
      "Train Epoch: 5 [608/1083 (56%)]\tLoss: 0.067822 \tOP: 0.779661\tOR: 0.730159\tOF1: 0.754098\n",
      "Train Epoch: 5 [640/1083 (59%)]\tLoss: 0.074991 \tOP: 0.800000\tOR: 0.716418\tOF1: 0.755906\n",
      "Train Epoch: 5 [672/1083 (62%)]\tLoss: 0.065673 \tOP: 0.774194\tOR: 0.738462\tOF1: 0.755906\n",
      "Train Epoch: 5 [704/1083 (65%)]\tLoss: 0.069376 \tOP: 0.766667\tOR: 0.696970\tOF1: 0.730159\n",
      "Train Epoch: 5 [736/1083 (68%)]\tLoss: 0.061333 \tOP: 0.787879\tOR: 0.764706\tOF1: 0.776119\n",
      "Train Epoch: 5 [768/1083 (71%)]\tLoss: 0.067837 \tOP: 0.825397\tOR: 0.764706\tOF1: 0.793893\n",
      "Train Epoch: 5 [800/1083 (74%)]\tLoss: 0.070217 \tOP: 0.770492\tOR: 0.681159\tOF1: 0.723077\n",
      "Train Epoch: 5 [832/1083 (76%)]\tLoss: 0.068066 \tOP: 0.761905\tOR: 0.738462\tOF1: 0.750000\n",
      "Train Epoch: 5 [864/1083 (79%)]\tLoss: 0.060645 \tOP: 0.776119\tOR: 0.812500\tOF1: 0.793893\n",
      "Train Epoch: 5 [896/1083 (82%)]\tLoss: 0.080902 \tOP: 0.745098\tOR: 0.535211\tOF1: 0.622951\n",
      "Train Epoch: 5 [928/1083 (85%)]\tLoss: 0.069435 \tOP: 0.779661\tOR: 0.657143\tOF1: 0.713178\n",
      "Train Epoch: 5 [960/1083 (88%)]\tLoss: 0.075073 \tOP: 0.774194\tOR: 0.666667\tOF1: 0.716418\n",
      "Train Epoch: 5 [992/1083 (91%)]\tLoss: 0.062599 \tOP: 0.754098\tOR: 0.730159\tOF1: 0.741935\n",
      "Train Epoch: 5 [1024/1083 (94%)]\tLoss: 0.069378 \tOP: 0.741379\tOR: 0.623188\tOF1: 0.677165\n",
      "Train Epoch: 5 [891/1083 (97%)]\tLoss: 0.074620 \tOP: 0.719298\tOR: 0.650794\tOF1: 0.683333\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2365 \n",
      "OP: 0.218750\n",
      "OR: 0.200000\n",
      "OF1: 0.208955\n",
      "\n",
      "Train Epoch: 6 [0/1083 (0%)]\tLoss: 0.067962 \tOP: 0.796875\tOR: 0.680000\tOF1: 0.733813\n",
      "Train Epoch: 6 [32/1083 (3%)]\tLoss: 0.054926 \tOP: 0.845070\tOR: 0.882353\tOF1: 0.863309\n",
      "Train Epoch: 6 [64/1083 (6%)]\tLoss: 0.061945 \tOP: 0.820896\tOR: 0.743243\tOF1: 0.780142\n",
      "Train Epoch: 6 [96/1083 (9%)]\tLoss: 0.070140 \tOP: 0.825397\tOR: 0.684211\tOF1: 0.748201\n",
      "Train Epoch: 6 [128/1083 (12%)]\tLoss: 0.051726 \tOP: 0.818182\tOR: 0.843750\tOF1: 0.830769\n",
      "Train Epoch: 6 [160/1083 (15%)]\tLoss: 0.069470 \tOP: 0.770492\tOR: 0.681159\tOF1: 0.723077\n",
      "Train Epoch: 6 [192/1083 (18%)]\tLoss: 0.050488 \tOP: 0.820896\tOR: 0.808824\tOF1: 0.814815\n",
      "Train Epoch: 6 [224/1083 (21%)]\tLoss: 0.053827 \tOP: 0.835616\tOR: 0.859155\tOF1: 0.847222\n",
      "Train Epoch: 6 [256/1083 (24%)]\tLoss: 0.068859 \tOP: 0.822581\tOR: 0.708333\tOF1: 0.761194\n",
      "Train Epoch: 6 [288/1083 (26%)]\tLoss: 0.060152 \tOP: 0.784615\tOR: 0.750000\tOF1: 0.766917\n",
      "Train Epoch: 6 [320/1083 (29%)]\tLoss: 0.065166 \tOP: 0.787879\tOR: 0.722222\tOF1: 0.753623\n",
      "Train Epoch: 6 [352/1083 (32%)]\tLoss: 0.057527 \tOP: 0.805970\tOR: 0.750000\tOF1: 0.776978\n",
      "Train Epoch: 6 [384/1083 (35%)]\tLoss: 0.060369 \tOP: 0.774194\tOR: 0.695652\tOF1: 0.732824\n",
      "Train Epoch: 6 [416/1083 (38%)]\tLoss: 0.053007 \tOP: 0.800000\tOR: 0.786885\tOF1: 0.793388\n",
      "Train Epoch: 6 [448/1083 (41%)]\tLoss: 0.065387 \tOP: 0.823529\tOR: 0.756757\tOF1: 0.788732\n",
      "Train Epoch: 6 [480/1083 (44%)]\tLoss: 0.050448 \tOP: 0.800000\tOR: 0.825397\tOF1: 0.812500\n",
      "Train Epoch: 6 [512/1083 (47%)]\tLoss: 0.056344 \tOP: 0.814286\tOR: 0.802817\tOF1: 0.808511\n",
      "Train Epoch: 6 [544/1083 (50%)]\tLoss: 0.053250 \tOP: 0.775862\tOR: 0.737705\tOF1: 0.756303\n",
      "Train Epoch: 6 [576/1083 (53%)]\tLoss: 0.050840 \tOP: 0.825397\tOR: 0.838710\tOF1: 0.832000\n",
      "Train Epoch: 6 [608/1083 (56%)]\tLoss: 0.057378 \tOP: 0.783333\tOR: 0.734375\tOF1: 0.758065\n",
      "Train Epoch: 6 [640/1083 (59%)]\tLoss: 0.055045 \tOP: 0.809524\tOR: 0.809524\tOF1: 0.809524\n",
      "Train Epoch: 6 [672/1083 (62%)]\tLoss: 0.051842 \tOP: 0.826667\tOR: 0.873239\tOF1: 0.849315\n",
      "Train Epoch: 6 [704/1083 (65%)]\tLoss: 0.050247 \tOP: 0.822581\tOR: 0.822581\tOF1: 0.822581\n",
      "Train Epoch: 6 [736/1083 (68%)]\tLoss: 0.057467 \tOP: 0.833333\tOR: 0.800000\tOF1: 0.816327\n",
      "Train Epoch: 6 [768/1083 (71%)]\tLoss: 0.059183 \tOP: 0.800000\tOR: 0.753623\tOF1: 0.776119\n",
      "Train Epoch: 6 [800/1083 (74%)]\tLoss: 0.044475 \tOP: 0.808824\tOR: 0.859375\tOF1: 0.833333\n",
      "Train Epoch: 6 [832/1083 (76%)]\tLoss: 0.052815 \tOP: 0.842857\tOR: 0.830986\tOF1: 0.836879\n",
      "Train Epoch: 6 [864/1083 (79%)]\tLoss: 0.056342 \tOP: 0.787879\tOR: 0.742857\tOF1: 0.764706\n",
      "Train Epoch: 6 [896/1083 (82%)]\tLoss: 0.053518 \tOP: 0.830986\tOR: 0.830986\tOF1: 0.830986\n",
      "Train Epoch: 6 [928/1083 (85%)]\tLoss: 0.051184 \tOP: 0.818182\tOR: 0.794118\tOF1: 0.805970\n",
      "Train Epoch: 6 [960/1083 (88%)]\tLoss: 0.058055 \tOP: 0.791045\tOR: 0.736111\tOF1: 0.762590\n",
      "Train Epoch: 6 [992/1083 (91%)]\tLoss: 0.044126 \tOP: 0.793651\tOR: 0.862069\tOF1: 0.826446\n",
      "Train Epoch: 6 [1024/1083 (94%)]\tLoss: 0.052173 \tOP: 0.796875\tOR: 0.772727\tOF1: 0.784615\n",
      "Train Epoch: 6 [891/1083 (97%)]\tLoss: 0.062056 \tOP: 0.744681\tOR: 0.648148\tOF1: 0.693069\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2391 \n",
      "OP: 0.250000\n",
      "OR: 0.228571\n",
      "OF1: 0.238806\n",
      "\n",
      "Train Epoch: 7 [0/1083 (0%)]\tLoss: 0.038584 \tOP: 0.814286\tOR: 0.934426\tOF1: 0.870229\n",
      "Train Epoch: 7 [32/1083 (3%)]\tLoss: 0.044713 \tOP: 0.840000\tOR: 0.887324\tOF1: 0.863014\n",
      "Train Epoch: 7 [64/1083 (6%)]\tLoss: 0.049316 \tOP: 0.819672\tOR: 0.769231\tOF1: 0.793651\n",
      "Train Epoch: 7 [96/1083 (9%)]\tLoss: 0.065410 \tOP: 0.833333\tOR: 0.671642\tOF1: 0.743802\n",
      "Train Epoch: 7 [128/1083 (12%)]\tLoss: 0.047781 \tOP: 0.840000\tOR: 0.840000\tOF1: 0.840000\n",
      "Train Epoch: 7 [160/1083 (15%)]\tLoss: 0.043038 \tOP: 0.855072\tOR: 0.867647\tOF1: 0.861314\n",
      "Train Epoch: 7 [192/1083 (18%)]\tLoss: 0.046925 \tOP: 0.838235\tOR: 0.850746\tOF1: 0.844444\n",
      "Train Epoch: 7 [224/1083 (21%)]\tLoss: 0.052192 \tOP: 0.838235\tOR: 0.780822\tOF1: 0.808511\n",
      "Train Epoch: 7 [256/1083 (24%)]\tLoss: 0.052769 \tOP: 0.844828\tOR: 0.790323\tOF1: 0.816667\n",
      "Train Epoch: 7 [288/1083 (26%)]\tLoss: 0.048123 \tOP: 0.838710\tOR: 0.800000\tOF1: 0.818898\n",
      "Train Epoch: 7 [320/1083 (29%)]\tLoss: 0.044782 \tOP: 0.826087\tOR: 0.826087\tOF1: 0.826087\n",
      "Train Epoch: 7 [352/1083 (32%)]\tLoss: 0.046685 \tOP: 0.838710\tOR: 0.812500\tOF1: 0.825397\n",
      "Train Epoch: 7 [384/1083 (35%)]\tLoss: 0.038773 \tOP: 0.842857\tOR: 0.907692\tOF1: 0.874074\n",
      "Train Epoch: 7 [416/1083 (38%)]\tLoss: 0.042747 \tOP: 0.830986\tOR: 0.880597\tOF1: 0.855072\n",
      "Train Epoch: 7 [448/1083 (41%)]\tLoss: 0.044607 \tOP: 0.823529\tOR: 0.861538\tOF1: 0.842105\n",
      "Train Epoch: 7 [480/1083 (44%)]\tLoss: 0.047713 \tOP: 0.825397\tOR: 0.764706\tOF1: 0.793893\n",
      "Train Epoch: 7 [512/1083 (47%)]\tLoss: 0.049342 \tOP: 0.840580\tOR: 0.794521\tOF1: 0.816901\n",
      "Train Epoch: 7 [544/1083 (50%)]\tLoss: 0.036610 \tOP: 0.876712\tOR: 0.941176\tOF1: 0.907801\n",
      "Train Epoch: 7 [576/1083 (53%)]\tLoss: 0.044842 \tOP: 0.866667\tOR: 0.878378\tOF1: 0.872483\n",
      "Train Epoch: 7 [608/1083 (56%)]\tLoss: 0.041568 \tOP: 0.842857\tOR: 0.907692\tOF1: 0.874074\n",
      "Train Epoch: 7 [640/1083 (59%)]\tLoss: 0.044950 \tOP: 0.840580\tOR: 0.852941\tOF1: 0.846715\n",
      "Train Epoch: 7 [672/1083 (62%)]\tLoss: 0.042871 \tOP: 0.815385\tOR: 0.803030\tOF1: 0.809160\n",
      "Train Epoch: 7 [704/1083 (65%)]\tLoss: 0.048600 \tOP: 0.848485\tOR: 0.800000\tOF1: 0.823529\n",
      "Train Epoch: 7 [736/1083 (68%)]\tLoss: 0.039781 \tOP: 0.815789\tOR: 0.911765\tOF1: 0.861111\n",
      "Train Epoch: 7 [768/1083 (71%)]\tLoss: 0.046994 \tOP: 0.846154\tOR: 0.785714\tOF1: 0.814815\n",
      "Train Epoch: 7 [800/1083 (74%)]\tLoss: 0.045123 \tOP: 0.855072\tOR: 0.842857\tOF1: 0.848921\n",
      "Train Epoch: 7 [832/1083 (76%)]\tLoss: 0.043061 \tOP: 0.805556\tOR: 0.828571\tOF1: 0.816901\n",
      "Train Epoch: 7 [864/1083 (79%)]\tLoss: 0.044383 \tOP: 0.848485\tOR: 0.835821\tOF1: 0.842105\n",
      "Train Epoch: 7 [896/1083 (82%)]\tLoss: 0.041159 \tOP: 0.838235\tOR: 0.838235\tOF1: 0.838235\n",
      "Train Epoch: 7 [928/1083 (85%)]\tLoss: 0.039251 \tOP: 0.876923\tOR: 0.876923\tOF1: 0.876923\n",
      "Train Epoch: 7 [960/1083 (88%)]\tLoss: 0.040932 \tOP: 0.840580\tOR: 0.865672\tOF1: 0.852941\n",
      "Train Epoch: 7 [992/1083 (91%)]\tLoss: 0.049000 \tOP: 0.859375\tOR: 0.785714\tOF1: 0.820896\n",
      "Train Epoch: 7 [1024/1083 (94%)]\tLoss: 0.048739 \tOP: 0.830986\tOR: 0.786667\tOF1: 0.808219\n",
      "Train Epoch: 7 [891/1083 (97%)]\tLoss: 0.050526 \tOP: 0.813559\tOR: 0.774194\tOF1: 0.793388\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2411 \n",
      "OP: 0.218750\n",
      "OR: 0.200000\n",
      "OF1: 0.208955\n",
      "\n",
      "Train Epoch: 8 [0/1083 (0%)]\tLoss: 0.037616 \tOP: 0.854839\tOR: 0.828125\tOF1: 0.841270\n",
      "Train Epoch: 8 [32/1083 (3%)]\tLoss: 0.045599 \tOP: 0.852941\tOR: 0.828571\tOF1: 0.840580\n",
      "Train Epoch: 8 [64/1083 (6%)]\tLoss: 0.033285 \tOP: 0.842857\tOR: 0.907692\tOF1: 0.874074\n",
      "Train Epoch: 8 [96/1083 (9%)]\tLoss: 0.042026 \tOP: 0.880000\tOR: 0.880000\tOF1: 0.880000\n",
      "Train Epoch: 8 [128/1083 (12%)]\tLoss: 0.040318 \tOP: 0.888889\tOR: 0.914286\tOF1: 0.901408\n",
      "Train Epoch: 8 [160/1083 (15%)]\tLoss: 0.036913 \tOP: 0.871795\tOR: 0.944444\tOF1: 0.906667\n",
      "Train Epoch: 8 [192/1083 (18%)]\tLoss: 0.041085 \tOP: 0.881579\tOR: 0.893333\tOF1: 0.887417\n",
      "Train Epoch: 8 [224/1083 (21%)]\tLoss: 0.037405 \tOP: 0.850746\tOR: 0.890625\tOF1: 0.870229\n",
      "Train Epoch: 8 [256/1083 (24%)]\tLoss: 0.039671 \tOP: 0.866667\tOR: 0.825397\tOF1: 0.845528\n",
      "Train Epoch: 8 [288/1083 (26%)]\tLoss: 0.041470 \tOP: 0.850746\tOR: 0.863636\tOF1: 0.857143\n",
      "Train Epoch: 8 [320/1083 (29%)]\tLoss: 0.033033 \tOP: 0.835616\tOR: 0.924242\tOF1: 0.877698\n",
      "Train Epoch: 8 [352/1083 (32%)]\tLoss: 0.040786 \tOP: 0.833333\tOR: 0.845070\tOF1: 0.839161\n",
      "Train Epoch: 8 [384/1083 (35%)]\tLoss: 0.038116 \tOP: 0.882353\tOR: 0.895522\tOF1: 0.888889\n",
      "Train Epoch: 8 [416/1083 (38%)]\tLoss: 0.035843 \tOP: 0.915493\tOR: 0.942029\tOF1: 0.928571\n",
      "Train Epoch: 8 [448/1083 (41%)]\tLoss: 0.037990 \tOP: 0.828571\tOR: 0.906250\tOF1: 0.865672\n",
      "Train Epoch: 8 [480/1083 (44%)]\tLoss: 0.034924 \tOP: 0.876712\tOR: 0.914286\tOF1: 0.895105\n",
      "Train Epoch: 8 [512/1083 (47%)]\tLoss: 0.037117 \tOP: 0.867647\tOR: 0.893939\tOF1: 0.880597\n",
      "Train Epoch: 8 [544/1083 (50%)]\tLoss: 0.032870 \tOP: 0.837838\tOR: 0.939394\tOF1: 0.885714\n",
      "Train Epoch: 8 [576/1083 (53%)]\tLoss: 0.038776 \tOP: 0.892308\tOR: 0.878788\tOF1: 0.885496\n",
      "Train Epoch: 8 [608/1083 (56%)]\tLoss: 0.038758 \tOP: 0.871429\tOR: 0.897059\tOF1: 0.884058\n",
      "Train Epoch: 8 [640/1083 (59%)]\tLoss: 0.043790 \tOP: 0.861538\tOR: 0.811594\tOF1: 0.835821\n",
      "Train Epoch: 8 [672/1083 (62%)]\tLoss: 0.039511 \tOP: 0.855072\tOR: 0.855072\tOF1: 0.855072\n",
      "Train Epoch: 8 [704/1083 (65%)]\tLoss: 0.043749 \tOP: 0.869565\tOR: 0.845070\tOF1: 0.857143\n",
      "Train Epoch: 8 [736/1083 (68%)]\tLoss: 0.037669 \tOP: 0.871429\tOR: 0.910448\tOF1: 0.890511\n",
      "Train Epoch: 8 [768/1083 (71%)]\tLoss: 0.039136 \tOP: 0.855072\tOR: 0.880597\tOF1: 0.867647\n",
      "Train Epoch: 8 [800/1083 (74%)]\tLoss: 0.034044 \tOP: 0.891892\tOR: 0.942857\tOF1: 0.916667\n",
      "Train Epoch: 8 [832/1083 (76%)]\tLoss: 0.032757 \tOP: 0.902778\tOR: 0.942029\tOF1: 0.921986\n",
      "Train Epoch: 8 [864/1083 (79%)]\tLoss: 0.038988 \tOP: 0.835821\tOR: 0.835821\tOF1: 0.835821\n",
      "Train Epoch: 8 [896/1083 (82%)]\tLoss: 0.038855 \tOP: 0.878378\tOR: 0.878378\tOF1: 0.878378\n",
      "Train Epoch: 8 [928/1083 (85%)]\tLoss: 0.036974 \tOP: 0.863636\tOR: 0.904762\tOF1: 0.883721\n",
      "Train Epoch: 8 [960/1083 (88%)]\tLoss: 0.036606 \tOP: 0.849315\tOR: 0.873239\tOF1: 0.861111\n",
      "Train Epoch: 8 [992/1083 (91%)]\tLoss: 0.036725 \tOP: 0.869565\tOR: 0.882353\tOF1: 0.875912\n",
      "Train Epoch: 8 [1024/1083 (94%)]\tLoss: 0.035476 \tOP: 0.850746\tOR: 0.863636\tOF1: 0.857143\n",
      "Train Epoch: 8 [891/1083 (97%)]\tLoss: 0.040774 \tOP: 0.828125\tOR: 0.883333\tOF1: 0.854839\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2444 \n",
      "OP: 0.225806\n",
      "OR: 0.200000\n",
      "OF1: 0.212121\n",
      "\n",
      "Train Epoch: 9 [0/1083 (0%)]\tLoss: 0.036764 \tOP: 0.910448\tOR: 0.897059\tOF1: 0.903704\n",
      "Train Epoch: 9 [32/1083 (3%)]\tLoss: 0.034969 \tOP: 0.871429\tOR: 0.910448\tOF1: 0.890511\n",
      "Train Epoch: 9 [64/1083 (6%)]\tLoss: 0.028639 \tOP: 0.881579\tOR: 0.957143\tOF1: 0.917808\n",
      "Train Epoch: 9 [96/1083 (9%)]\tLoss: 0.031370 \tOP: 0.867647\tOR: 0.936508\tOF1: 0.900763\n",
      "Train Epoch: 9 [128/1083 (12%)]\tLoss: 0.039058 \tOP: 0.902778\tOR: 0.890411\tOF1: 0.896552\n",
      "Train Epoch: 9 [160/1083 (15%)]\tLoss: 0.034097 \tOP: 0.884058\tOR: 0.910448\tOF1: 0.897059\n",
      "Train Epoch: 9 [192/1083 (18%)]\tLoss: 0.026125 \tOP: 0.893333\tOR: 0.985294\tOF1: 0.937063\n",
      "Train Epoch: 9 [224/1083 (21%)]\tLoss: 0.029832 \tOP: 0.852941\tOR: 0.935484\tOF1: 0.892308\n",
      "Train Epoch: 9 [256/1083 (24%)]\tLoss: 0.032943 \tOP: 0.900000\tOR: 0.940299\tOF1: 0.919708\n",
      "Train Epoch: 9 [288/1083 (26%)]\tLoss: 0.027571 \tOP: 0.880000\tOR: 0.985075\tOF1: 0.929577\n",
      "Train Epoch: 9 [320/1083 (29%)]\tLoss: 0.031710 \tOP: 0.913043\tOR: 0.954545\tOF1: 0.933333\n",
      "Train Epoch: 9 [352/1083 (32%)]\tLoss: 0.037064 \tOP: 0.915493\tOR: 0.890411\tOF1: 0.902778\n",
      "Train Epoch: 9 [384/1083 (35%)]\tLoss: 0.029533 \tOP: 0.861538\tOR: 0.903226\tOF1: 0.881890\n",
      "Train Epoch: 9 [416/1083 (38%)]\tLoss: 0.028270 \tOP: 0.890411\tOR: 0.984848\tOF1: 0.935252\n",
      "Train Epoch: 9 [448/1083 (41%)]\tLoss: 0.031547 \tOP: 0.900000\tOR: 0.972973\tOF1: 0.935065\n",
      "Train Epoch: 9 [480/1083 (44%)]\tLoss: 0.029378 \tOP: 0.916667\tOR: 0.970588\tOF1: 0.942857\n",
      "Train Epoch: 9 [512/1083 (47%)]\tLoss: 0.029911 \tOP: 0.861538\tOR: 0.888889\tOF1: 0.875000\n",
      "Train Epoch: 9 [544/1083 (50%)]\tLoss: 0.026960 \tOP: 0.861111\tOR: 0.953846\tOF1: 0.905109\n",
      "Train Epoch: 9 [576/1083 (53%)]\tLoss: 0.030793 \tOP: 0.878378\tOR: 0.928571\tOF1: 0.902778\n",
      "Train Epoch: 9 [608/1083 (56%)]\tLoss: 0.032946 \tOP: 0.910256\tOR: 0.934211\tOF1: 0.922078\n",
      "Train Epoch: 9 [640/1083 (59%)]\tLoss: 0.034756 \tOP: 0.867647\tOR: 0.907692\tOF1: 0.887218\n",
      "Train Epoch: 9 [672/1083 (62%)]\tLoss: 0.030175 \tOP: 0.864865\tOR: 0.941176\tOF1: 0.901408\n",
      "Train Epoch: 9 [704/1083 (65%)]\tLoss: 0.034109 \tOP: 0.920000\tOR: 0.920000\tOF1: 0.920000\n",
      "Train Epoch: 9 [736/1083 (68%)]\tLoss: 0.035740 \tOP: 0.881579\tOR: 0.881579\tOF1: 0.881579\n",
      "Train Epoch: 9 [768/1083 (71%)]\tLoss: 0.035879 \tOP: 0.918919\tOR: 0.944444\tOF1: 0.931507\n",
      "Train Epoch: 9 [800/1083 (74%)]\tLoss: 0.037418 \tOP: 0.892308\tOR: 0.906250\tOF1: 0.899225\n",
      "Train Epoch: 9 [832/1083 (76%)]\tLoss: 0.033294 \tOP: 0.887324\tOR: 0.887324\tOF1: 0.887324\n",
      "Train Epoch: 9 [864/1083 (79%)]\tLoss: 0.028451 \tOP: 0.871429\tOR: 0.953125\tOF1: 0.910448\n",
      "Train Epoch: 9 [896/1083 (82%)]\tLoss: 0.028338 \tOP: 0.884058\tOR: 0.910448\tOF1: 0.897059\n",
      "Train Epoch: 9 [928/1083 (85%)]\tLoss: 0.030395 \tOP: 0.927536\tOR: 0.955224\tOF1: 0.941176\n",
      "Train Epoch: 9 [960/1083 (88%)]\tLoss: 0.046722 \tOP: 0.859375\tOR: 0.723684\tOF1: 0.785714\n",
      "Train Epoch: 9 [992/1083 (91%)]\tLoss: 0.028275 \tOP: 0.885714\tOR: 0.953846\tOF1: 0.918519\n",
      "Train Epoch: 9 [1024/1083 (94%)]\tLoss: 0.030922 \tOP: 0.869565\tOR: 0.895522\tOF1: 0.882353\n",
      "Train Epoch: 9 [891/1083 (97%)]\tLoss: 0.031736 \tOP: 0.881356\tOR: 0.928571\tOF1: 0.904348\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2468 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n",
      "Train Epoch: 10 [0/1083 (0%)]\tLoss: 0.025685 \tOP: 0.905405\tOR: 0.985294\tOF1: 0.943662\n",
      "Train Epoch: 10 [32/1083 (3%)]\tLoss: 0.027197 \tOP: 0.892308\tOR: 0.935484\tOF1: 0.913386\n",
      "Train Epoch: 10 [64/1083 (6%)]\tLoss: 0.026989 \tOP: 0.853333\tOR: 0.969697\tOF1: 0.907801\n",
      "Train Epoch: 10 [96/1083 (9%)]\tLoss: 0.025924 \tOP: 0.904762\tOR: 0.934426\tOF1: 0.919355\n",
      "Train Epoch: 10 [128/1083 (12%)]\tLoss: 0.028793 \tOP: 0.905405\tOR: 0.957143\tOF1: 0.930556\n",
      "Train Epoch: 10 [160/1083 (15%)]\tLoss: 0.022884 \tOP: 0.847222\tOR: 1.000000\tOF1: 0.917293\n",
      "Train Epoch: 10 [192/1083 (18%)]\tLoss: 0.026331 \tOP: 0.896104\tOR: 0.985714\tOF1: 0.938776\n",
      "Train Epoch: 10 [224/1083 (21%)]\tLoss: 0.028240 \tOP: 0.904110\tOR: 0.956522\tOF1: 0.929577\n",
      "Train Epoch: 10 [256/1083 (24%)]\tLoss: 0.025980 \tOP: 0.894737\tOR: 0.971429\tOF1: 0.931507\n",
      "Train Epoch: 10 [288/1083 (26%)]\tLoss: 0.021137 \tOP: 0.890411\tOR: 1.000000\tOF1: 0.942029\n",
      "Train Epoch: 10 [320/1083 (29%)]\tLoss: 0.026645 \tOP: 0.944444\tOR: 0.971429\tOF1: 0.957746\n",
      "Train Epoch: 10 [352/1083 (32%)]\tLoss: 0.029909 \tOP: 0.904110\tOR: 0.916667\tOF1: 0.910345\n",
      "Train Epoch: 10 [384/1083 (35%)]\tLoss: 0.029566 \tOP: 0.884058\tOR: 0.884058\tOF1: 0.884058\n",
      "Train Epoch: 10 [416/1083 (38%)]\tLoss: 0.021700 \tOP: 0.887324\tOR: 0.969231\tOF1: 0.926471\n",
      "Train Epoch: 10 [448/1083 (41%)]\tLoss: 0.029707 \tOP: 0.859155\tOR: 0.910448\tOF1: 0.884058\n",
      "Train Epoch: 10 [480/1083 (44%)]\tLoss: 0.026892 \tOP: 0.878378\tOR: 0.970149\tOF1: 0.921986\n",
      "Train Epoch: 10 [512/1083 (47%)]\tLoss: 0.031745 \tOP: 0.937500\tOR: 0.909091\tOF1: 0.923077\n",
      "Train Epoch: 10 [544/1083 (50%)]\tLoss: 0.022810 \tOP: 0.866667\tOR: 0.984848\tOF1: 0.921986\n",
      "Train Epoch: 10 [576/1083 (53%)]\tLoss: 0.024863 \tOP: 0.880597\tOR: 0.921875\tOF1: 0.900763\n",
      "Train Epoch: 10 [608/1083 (56%)]\tLoss: 0.022330 \tOP: 0.905405\tOR: 0.985294\tOF1: 0.943662\n",
      "Train Epoch: 10 [640/1083 (59%)]\tLoss: 0.032493 \tOP: 0.890411\tOR: 0.855263\tOF1: 0.872483\n",
      "Train Epoch: 10 [672/1083 (62%)]\tLoss: 0.026985 \tOP: 0.884615\tOR: 0.945205\tOF1: 0.913907\n",
      "Train Epoch: 10 [704/1083 (65%)]\tLoss: 0.027416 \tOP: 0.894737\tOR: 0.918919\tOF1: 0.906667\n",
      "Train Epoch: 10 [736/1083 (68%)]\tLoss: 0.028954 \tOP: 0.898551\tOR: 0.939394\tOF1: 0.918519\n",
      "Train Epoch: 10 [768/1083 (71%)]\tLoss: 0.034651 \tOP: 0.896104\tOR: 0.920000\tOF1: 0.907895\n",
      "Train Epoch: 10 [800/1083 (74%)]\tLoss: 0.023629 \tOP: 0.876923\tOR: 0.934426\tOF1: 0.904762\n",
      "Train Epoch: 10 [832/1083 (76%)]\tLoss: 0.036231 \tOP: 0.915493\tOR: 0.902778\tOF1: 0.909091\n",
      "Train Epoch: 10 [864/1083 (79%)]\tLoss: 0.030374 \tOP: 0.910448\tOR: 0.884058\tOF1: 0.897059\n",
      "Train Epoch: 10 [896/1083 (82%)]\tLoss: 0.024469 \tOP: 0.928571\tOR: 0.970149\tOF1: 0.948905\n",
      "Train Epoch: 10 [928/1083 (85%)]\tLoss: 0.022873 \tOP: 0.927536\tOR: 0.984615\tOF1: 0.955224\n",
      "Train Epoch: 10 [960/1083 (88%)]\tLoss: 0.024830 \tOP: 0.900000\tOR: 0.984375\tOF1: 0.940299\n",
      "Train Epoch: 10 [992/1083 (91%)]\tLoss: 0.035749 \tOP: 0.944444\tOR: 0.871795\tOF1: 0.906667\n",
      "Train Epoch: 10 [1024/1083 (94%)]\tLoss: 0.027621 \tOP: 0.922078\tOR: 0.934211\tOF1: 0.928105\n",
      "Train Epoch: 10 [891/1083 (97%)]\tLoss: 0.026402 \tOP: 0.866667\tOR: 0.928571\tOF1: 0.896552\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2518 \n",
      "OP: 0.187500\n",
      "OR: 0.171429\n",
      "OF1: 0.179104\n",
      "\n",
      "Train Epoch: 11 [0/1083 (0%)]\tLoss: 0.022576 \tOP: 0.906667\tOR: 0.957746\tOF1: 0.931507\n",
      "Train Epoch: 11 [32/1083 (3%)]\tLoss: 0.022676 \tOP: 0.957143\tOR: 0.985294\tOF1: 0.971014\n",
      "Train Epoch: 11 [64/1083 (6%)]\tLoss: 0.025855 \tOP: 0.915493\tOR: 0.942029\tOF1: 0.928571\n",
      "Train Epoch: 11 [96/1083 (9%)]\tLoss: 0.030741 \tOP: 0.884058\tOR: 0.924242\tOF1: 0.903704\n",
      "Train Epoch: 11 [128/1083 (12%)]\tLoss: 0.022222 \tOP: 0.895522\tOR: 0.967742\tOF1: 0.930233\n",
      "Train Epoch: 11 [160/1083 (15%)]\tLoss: 0.023864 \tOP: 0.917808\tOR: 0.985294\tOF1: 0.950355\n",
      "Train Epoch: 11 [192/1083 (18%)]\tLoss: 0.021538 \tOP: 0.876712\tOR: 0.984615\tOF1: 0.927536\n",
      "Train Epoch: 11 [224/1083 (21%)]\tLoss: 0.023629 \tOP: 0.904110\tOR: 0.985075\tOF1: 0.942857\n",
      "Train Epoch: 11 [256/1083 (24%)]\tLoss: 0.023492 \tOP: 0.913043\tOR: 0.969231\tOF1: 0.940299\n",
      "Train Epoch: 11 [288/1083 (26%)]\tLoss: 0.027456 \tOP: 0.907895\tOR: 0.945205\tOF1: 0.926174\n",
      "Train Epoch: 11 [320/1083 (29%)]\tLoss: 0.024482 \tOP: 0.924051\tOR: 0.973333\tOF1: 0.948052\n",
      "Train Epoch: 11 [352/1083 (32%)]\tLoss: 0.024420 \tOP: 0.902778\tOR: 0.942029\tOF1: 0.921986\n",
      "Train Epoch: 11 [384/1083 (35%)]\tLoss: 0.024752 \tOP: 0.948718\tOR: 0.986667\tOF1: 0.967320\n",
      "Train Epoch: 11 [416/1083 (38%)]\tLoss: 0.023081 \tOP: 0.878788\tOR: 0.950820\tOF1: 0.913386\n",
      "Train Epoch: 11 [448/1083 (41%)]\tLoss: 0.025619 \tOP: 0.907895\tOR: 0.985714\tOF1: 0.945205\n",
      "Train Epoch: 11 [480/1083 (44%)]\tLoss: 0.022122 \tOP: 0.891892\tOR: 1.000000\tOF1: 0.942857\n",
      "Train Epoch: 11 [512/1083 (47%)]\tLoss: 0.028416 \tOP: 0.893939\tOR: 0.907692\tOF1: 0.900763\n",
      "Train Epoch: 11 [544/1083 (50%)]\tLoss: 0.024100 \tOP: 0.935065\tOR: 0.986301\tOF1: 0.960000\n",
      "Train Epoch: 11 [576/1083 (53%)]\tLoss: 0.023317 \tOP: 0.861538\tOR: 0.918033\tOF1: 0.888889\n",
      "Train Epoch: 11 [608/1083 (56%)]\tLoss: 0.023491 \tOP: 0.915493\tOR: 0.942029\tOF1: 0.928571\n",
      "Train Epoch: 11 [640/1083 (59%)]\tLoss: 0.021895 \tOP: 0.936709\tOR: 0.986667\tOF1: 0.961039\n",
      "Train Epoch: 11 [672/1083 (62%)]\tLoss: 0.020746 \tOP: 0.895522\tOR: 0.967742\tOF1: 0.930233\n",
      "Train Epoch: 11 [704/1083 (65%)]\tLoss: 0.025480 \tOP: 0.928571\tOR: 0.975000\tOF1: 0.951220\n",
      "Train Epoch: 11 [736/1083 (68%)]\tLoss: 0.018013 \tOP: 0.869565\tOR: 0.983607\tOF1: 0.923077\n",
      "Train Epoch: 11 [768/1083 (71%)]\tLoss: 0.022110 \tOP: 0.932432\tOR: 1.000000\tOF1: 0.965035\n",
      "Train Epoch: 11 [800/1083 (74%)]\tLoss: 0.023264 \tOP: 0.934211\tOR: 1.000000\tOF1: 0.965986\n",
      "Train Epoch: 11 [832/1083 (76%)]\tLoss: 0.019726 \tOP: 0.890411\tOR: 1.000000\tOF1: 0.942029\n",
      "Train Epoch: 11 [864/1083 (79%)]\tLoss: 0.019010 \tOP: 0.900000\tOR: 0.984375\tOF1: 0.940299\n",
      "Train Epoch: 11 [896/1083 (82%)]\tLoss: 0.023074 \tOP: 0.904110\tOR: 0.970588\tOF1: 0.936170\n",
      "Train Epoch: 11 [928/1083 (85%)]\tLoss: 0.024645 \tOP: 0.941176\tOR: 0.941176\tOF1: 0.941176\n",
      "Train Epoch: 11 [960/1083 (88%)]\tLoss: 0.023973 \tOP: 0.900000\tOR: 0.954545\tOF1: 0.926471\n",
      "Train Epoch: 11 [992/1083 (91%)]\tLoss: 0.020962 \tOP: 0.905405\tOR: 0.985294\tOF1: 0.943662\n",
      "Train Epoch: 11 [1024/1083 (94%)]\tLoss: 0.024735 \tOP: 0.933333\tOR: 0.958904\tOF1: 0.945946\n",
      "Train Epoch: 11 [891/1083 (97%)]\tLoss: 0.021627 \tOP: 0.906250\tOR: 0.966667\tOF1: 0.935484\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2534 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n",
      "Train Epoch: 12 [0/1083 (0%)]\tLoss: 0.024058 \tOP: 0.916667\tOR: 0.942857\tOF1: 0.929577\n",
      "Train Epoch: 12 [32/1083 (3%)]\tLoss: 0.019711 \tOP: 0.891892\tOR: 0.985075\tOF1: 0.936170\n",
      "Train Epoch: 12 [64/1083 (6%)]\tLoss: 0.023529 \tOP: 0.931507\tOR: 0.971429\tOF1: 0.951049\n",
      "Train Epoch: 12 [96/1083 (9%)]\tLoss: 0.018631 \tOP: 0.884058\tOR: 1.000000\tOF1: 0.938462\n",
      "Train Epoch: 12 [128/1083 (12%)]\tLoss: 0.021216 \tOP: 0.914286\tOR: 0.984615\tOF1: 0.948148\n",
      "Train Epoch: 12 [160/1083 (15%)]\tLoss: 0.020179 \tOP: 0.916667\tOR: 1.000000\tOF1: 0.956522\n",
      "Train Epoch: 12 [192/1083 (18%)]\tLoss: 0.020677 \tOP: 0.912500\tOR: 0.986486\tOF1: 0.948052\n",
      "Train Epoch: 12 [224/1083 (21%)]\tLoss: 0.017168 \tOP: 0.900000\tOR: 1.000000\tOF1: 0.947368\n",
      "Train Epoch: 12 [256/1083 (24%)]\tLoss: 0.018670 \tOP: 0.921053\tOR: 0.985915\tOF1: 0.952381\n",
      "Train Epoch: 12 [288/1083 (26%)]\tLoss: 0.021337 \tOP: 0.932432\tOR: 0.985714\tOF1: 0.958333\n",
      "Train Epoch: 12 [320/1083 (29%)]\tLoss: 0.017750 \tOP: 0.904110\tOR: 1.000000\tOF1: 0.949640\n",
      "Train Epoch: 12 [352/1083 (32%)]\tLoss: 0.023330 \tOP: 0.897436\tOR: 0.958904\tOF1: 0.927152\n",
      "Train Epoch: 12 [384/1083 (35%)]\tLoss: 0.020616 \tOP: 0.933333\tOR: 0.985915\tOF1: 0.958904\n",
      "Train Epoch: 12 [416/1083 (38%)]\tLoss: 0.018679 \tOP: 0.909091\tOR: 1.000000\tOF1: 0.952381\n",
      "Train Epoch: 12 [448/1083 (41%)]\tLoss: 0.019876 \tOP: 0.898551\tOR: 0.939394\tOF1: 0.918519\n",
      "Train Epoch: 12 [480/1083 (44%)]\tLoss: 0.020281 \tOP: 0.904110\tOR: 0.956522\tOF1: 0.929577\n",
      "Train Epoch: 12 [512/1083 (47%)]\tLoss: 0.017809 \tOP: 0.915493\tOR: 1.000000\tOF1: 0.955882\n",
      "Train Epoch: 12 [544/1083 (50%)]\tLoss: 0.017151 \tOP: 0.884615\tOR: 1.000000\tOF1: 0.938776\n",
      "Train Epoch: 12 [576/1083 (53%)]\tLoss: 0.022285 \tOP: 0.896104\tOR: 0.945205\tOF1: 0.920000\n",
      "Train Epoch: 12 [608/1083 (56%)]\tLoss: 0.021694 \tOP: 0.941176\tOR: 0.969697\tOF1: 0.955224\n",
      "Train Epoch: 12 [640/1083 (59%)]\tLoss: 0.019229 \tOP: 0.901408\tOR: 0.984615\tOF1: 0.941176\n",
      "Train Epoch: 12 [672/1083 (62%)]\tLoss: 0.021028 \tOP: 0.939024\tOR: 0.987179\tOF1: 0.962500\n",
      "Train Epoch: 12 [704/1083 (65%)]\tLoss: 0.022825 \tOP: 0.918919\tOR: 0.944444\tOF1: 0.931507\n",
      "Train Epoch: 12 [736/1083 (68%)]\tLoss: 0.028517 \tOP: 0.893939\tOR: 0.893939\tOF1: 0.893939\n",
      "Train Epoch: 12 [768/1083 (71%)]\tLoss: 0.020562 \tOP: 0.921053\tOR: 0.985915\tOF1: 0.952381\n",
      "Train Epoch: 12 [800/1083 (74%)]\tLoss: 0.020470 \tOP: 0.878378\tOR: 0.970149\tOF1: 0.921986\n",
      "Train Epoch: 12 [832/1083 (76%)]\tLoss: 0.023894 \tOP: 0.940299\tOR: 0.940299\tOF1: 0.940299\n",
      "Train Epoch: 12 [864/1083 (79%)]\tLoss: 0.019329 \tOP: 0.924242\tOR: 0.968254\tOF1: 0.945736\n",
      "Train Epoch: 12 [896/1083 (82%)]\tLoss: 0.018219 \tOP: 0.922078\tOR: 1.000000\tOF1: 0.959459\n",
      "Train Epoch: 12 [928/1083 (85%)]\tLoss: 0.020152 \tOP: 0.928571\tOR: 0.970149\tOF1: 0.948905\n",
      "Train Epoch: 12 [960/1083 (88%)]\tLoss: 0.018203 \tOP: 0.902778\tOR: 0.970149\tOF1: 0.935252\n",
      "Train Epoch: 12 [992/1083 (91%)]\tLoss: 0.018817 \tOP: 0.923077\tOR: 1.000000\tOF1: 0.960000\n",
      "Train Epoch: 12 [1024/1083 (94%)]\tLoss: 0.022129 \tOP: 0.906667\tOR: 0.971429\tOF1: 0.937931\n",
      "Train Epoch: 12 [891/1083 (97%)]\tLoss: 0.023141 \tOP: 0.881356\tOR: 0.912281\tOF1: 0.896552\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2569 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n",
      "Train Epoch: 13 [0/1083 (0%)]\tLoss: 0.016874 \tOP: 0.891892\tOR: 0.985075\tOF1: 0.936170\n",
      "Train Epoch: 13 [32/1083 (3%)]\tLoss: 0.023929 \tOP: 0.917808\tOR: 0.917808\tOF1: 0.917808\n",
      "Train Epoch: 13 [64/1083 (6%)]\tLoss: 0.019002 \tOP: 0.917808\tOR: 0.971014\tOF1: 0.943662\n",
      "Train Epoch: 13 [96/1083 (9%)]\tLoss: 0.014394 \tOP: 0.898551\tOR: 1.000000\tOF1: 0.946565\n",
      "Train Epoch: 13 [128/1083 (12%)]\tLoss: 0.018139 \tOP: 0.913043\tOR: 0.984375\tOF1: 0.947368\n",
      "Train Epoch: 13 [160/1083 (15%)]\tLoss: 0.022038 \tOP: 0.938272\tOR: 0.974359\tOF1: 0.955975\n",
      "Train Epoch: 13 [192/1083 (18%)]\tLoss: 0.015081 \tOP: 0.884058\tOR: 0.983871\tOF1: 0.931298\n",
      "Train Epoch: 13 [224/1083 (21%)]\tLoss: 0.017658 \tOP: 0.935484\tOR: 0.983051\tOF1: 0.958678\n",
      "Train Epoch: 13 [256/1083 (24%)]\tLoss: 0.016407 \tOP: 0.911765\tOR: 1.000000\tOF1: 0.953846\n",
      "Train Epoch: 13 [288/1083 (26%)]\tLoss: 0.018364 \tOP: 0.916667\tOR: 1.000000\tOF1: 0.956522\n",
      "Train Epoch: 13 [320/1083 (29%)]\tLoss: 0.025620 \tOP: 0.906667\tOR: 0.971429\tOF1: 0.937931\n",
      "Train Epoch: 13 [352/1083 (32%)]\tLoss: 0.016227 \tOP: 0.873239\tOR: 1.000000\tOF1: 0.932331\n",
      "Train Epoch: 13 [384/1083 (35%)]\tLoss: 0.018259 \tOP: 0.924242\tOR: 0.953125\tOF1: 0.938462\n",
      "Train Epoch: 13 [416/1083 (38%)]\tLoss: 0.023141 \tOP: 0.928571\tOR: 0.902778\tOF1: 0.915493\n",
      "Train Epoch: 13 [448/1083 (41%)]\tLoss: 0.016665 \tOP: 0.916667\tOR: 0.985075\tOF1: 0.949640\n",
      "Train Epoch: 13 [480/1083 (44%)]\tLoss: 0.019006 \tOP: 0.920000\tOR: 0.971831\tOF1: 0.945205\n",
      "Train Epoch: 13 [512/1083 (47%)]\tLoss: 0.018727 \tOP: 0.898551\tOR: 0.953846\tOF1: 0.925373\n",
      "Train Epoch: 13 [544/1083 (50%)]\tLoss: 0.018097 \tOP: 0.917808\tOR: 0.985294\tOF1: 0.950355\n",
      "Train Epoch: 13 [576/1083 (53%)]\tLoss: 0.016491 \tOP: 0.907895\tOR: 1.000000\tOF1: 0.951724\n",
      "Train Epoch: 13 [608/1083 (56%)]\tLoss: 0.020052 \tOP: 0.943662\tOR: 1.000000\tOF1: 0.971014\n",
      "Train Epoch: 13 [640/1083 (59%)]\tLoss: 0.015625 \tOP: 0.870130\tOR: 1.000000\tOF1: 0.930556\n",
      "Train Epoch: 13 [672/1083 (62%)]\tLoss: 0.017458 \tOP: 0.915663\tOR: 1.000000\tOF1: 0.955975\n",
      "Train Epoch: 13 [704/1083 (65%)]\tLoss: 0.023488 \tOP: 0.931507\tOR: 0.971429\tOF1: 0.951049\n",
      "Train Epoch: 13 [736/1083 (68%)]\tLoss: 0.016464 \tOP: 0.886076\tOR: 0.985915\tOF1: 0.933333\n",
      "Train Epoch: 13 [768/1083 (71%)]\tLoss: 0.019366 \tOP: 0.929577\tOR: 0.956522\tOF1: 0.942857\n",
      "Train Epoch: 13 [800/1083 (74%)]\tLoss: 0.018983 \tOP: 0.876712\tOR: 0.984615\tOF1: 0.927536\n",
      "Train Epoch: 13 [832/1083 (76%)]\tLoss: 0.016003 \tOP: 0.945946\tOR: 1.000000\tOF1: 0.972222\n",
      "Train Epoch: 13 [864/1083 (79%)]\tLoss: 0.018864 \tOP: 0.912500\tOR: 1.000000\tOF1: 0.954248\n",
      "Train Epoch: 13 [896/1083 (82%)]\tLoss: 0.017340 \tOP: 0.918919\tOR: 0.971429\tOF1: 0.944444\n",
      "Train Epoch: 13 [928/1083 (85%)]\tLoss: 0.019895 \tOP: 0.911392\tOR: 0.960000\tOF1: 0.935065\n",
      "Train Epoch: 13 [960/1083 (88%)]\tLoss: 0.016481 \tOP: 0.906667\tOR: 1.000000\tOF1: 0.951049\n",
      "Train Epoch: 13 [992/1083 (91%)]\tLoss: 0.018884 \tOP: 0.939024\tOR: 0.987179\tOF1: 0.962500\n",
      "Train Epoch: 13 [1024/1083 (94%)]\tLoss: 0.015067 \tOP: 0.888889\tOR: 0.984615\tOF1: 0.934307\n",
      "Train Epoch: 13 [891/1083 (97%)]\tLoss: 0.016416 \tOP: 0.854839\tOR: 0.981481\tOF1: 0.913793\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2632 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n",
      "Train Epoch: 14 [0/1083 (0%)]\tLoss: 0.015885 \tOP: 0.895522\tOR: 0.983607\tOF1: 0.937500\n",
      "Train Epoch: 14 [32/1083 (3%)]\tLoss: 0.018002 \tOP: 0.943662\tOR: 0.985294\tOF1: 0.964029\n",
      "Train Epoch: 14 [64/1083 (6%)]\tLoss: 0.015298 \tOP: 0.893333\tOR: 0.985294\tOF1: 0.937063\n",
      "Train Epoch: 14 [96/1083 (9%)]\tLoss: 0.015459 \tOP: 0.924051\tOR: 1.000000\tOF1: 0.960526\n",
      "Train Epoch: 14 [128/1083 (12%)]\tLoss: 0.017620 \tOP: 0.920000\tOR: 1.000000\tOF1: 0.958333\n",
      "Train Epoch: 14 [160/1083 (15%)]\tLoss: 0.017552 \tOP: 0.913580\tOR: 0.986667\tOF1: 0.948718\n",
      "Train Epoch: 14 [192/1083 (18%)]\tLoss: 0.016817 \tOP: 0.904110\tOR: 0.970588\tOF1: 0.936170\n",
      "Train Epoch: 14 [224/1083 (21%)]\tLoss: 0.017531 \tOP: 0.891892\tOR: 0.985075\tOF1: 0.936170\n",
      "Train Epoch: 14 [256/1083 (24%)]\tLoss: 0.018165 \tOP: 0.915493\tOR: 0.970149\tOF1: 0.942029\n",
      "Train Epoch: 14 [288/1083 (26%)]\tLoss: 0.018954 \tOP: 0.884058\tOR: 0.968254\tOF1: 0.924242\n",
      "Train Epoch: 14 [320/1083 (29%)]\tLoss: 0.015280 \tOP: 0.921053\tOR: 0.985915\tOF1: 0.952381\n",
      "Train Epoch: 14 [352/1083 (32%)]\tLoss: 0.017228 \tOP: 0.955882\tOR: 0.984848\tOF1: 0.970149\n",
      "Train Epoch: 14 [384/1083 (35%)]\tLoss: 0.015742 \tOP: 0.910256\tOR: 1.000000\tOF1: 0.953020\n",
      "Train Epoch: 14 [416/1083 (38%)]\tLoss: 0.017653 \tOP: 0.970149\tOR: 0.984848\tOF1: 0.977444\n",
      "Train Epoch: 14 [448/1083 (41%)]\tLoss: 0.014194 \tOP: 0.871429\tOR: 1.000000\tOF1: 0.931298\n",
      "Train Epoch: 14 [480/1083 (44%)]\tLoss: 0.019645 \tOP: 0.918919\tOR: 0.985507\tOF1: 0.951049\n",
      "Train Epoch: 14 [512/1083 (47%)]\tLoss: 0.015064 \tOP: 0.896104\tOR: 1.000000\tOF1: 0.945205\n",
      "Train Epoch: 14 [544/1083 (50%)]\tLoss: 0.021573 \tOP: 0.932432\tOR: 0.971831\tOF1: 0.951724\n",
      "Train Epoch: 14 [576/1083 (53%)]\tLoss: 0.012485 \tOP: 0.885714\tOR: 1.000000\tOF1: 0.939394\n",
      "Train Epoch: 14 [608/1083 (56%)]\tLoss: 0.013954 \tOP: 0.904110\tOR: 0.985075\tOF1: 0.942857\n",
      "Train Epoch: 14 [640/1083 (59%)]\tLoss: 0.015534 \tOP: 0.898551\tOR: 0.984127\tOF1: 0.939394\n",
      "Train Epoch: 14 [672/1083 (62%)]\tLoss: 0.020302 \tOP: 0.891892\tOR: 0.956522\tOF1: 0.923077\n",
      "Train Epoch: 14 [704/1083 (65%)]\tLoss: 0.013661 \tOP: 0.957143\tOR: 1.000000\tOF1: 0.978102\n",
      "Train Epoch: 14 [736/1083 (68%)]\tLoss: 0.014984 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 14 [768/1083 (71%)]\tLoss: 0.013763 \tOP: 0.935897\tOR: 1.000000\tOF1: 0.966887\n",
      "Train Epoch: 14 [800/1083 (74%)]\tLoss: 0.013769 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 14 [832/1083 (76%)]\tLoss: 0.013686 \tOP: 0.905405\tOR: 0.985294\tOF1: 0.943662\n",
      "Train Epoch: 14 [864/1083 (79%)]\tLoss: 0.012301 \tOP: 0.905405\tOR: 1.000000\tOF1: 0.950355\n",
      "Train Epoch: 14 [896/1083 (82%)]\tLoss: 0.016121 \tOP: 0.906667\tOR: 0.985507\tOF1: 0.944444\n",
      "Train Epoch: 14 [928/1083 (85%)]\tLoss: 0.017827 \tOP: 0.895522\tOR: 0.937500\tOF1: 0.916031\n",
      "Train Epoch: 14 [960/1083 (88%)]\tLoss: 0.017830 \tOP: 0.938272\tOR: 0.987013\tOF1: 0.962025\n",
      "Train Epoch: 14 [992/1083 (91%)]\tLoss: 0.016353 \tOP: 0.910256\tOR: 0.972603\tOF1: 0.940397\n",
      "Train Epoch: 14 [1024/1083 (94%)]\tLoss: 0.013744 \tOP: 0.904110\tOR: 0.985075\tOF1: 0.942857\n",
      "Train Epoch: 14 [891/1083 (97%)]\tLoss: 0.020590 \tOP: 0.888889\tOR: 1.000000\tOF1: 0.941176\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2629 \n",
      "OP: 0.242424\n",
      "OR: 0.228571\n",
      "OF1: 0.235294\n",
      "\n",
      "Train Epoch: 15 [0/1083 (0%)]\tLoss: 0.017842 \tOP: 0.960000\tOR: 1.000000\tOF1: 0.979592\n",
      "Train Epoch: 15 [32/1083 (3%)]\tLoss: 0.015421 \tOP: 0.895522\tOR: 0.967742\tOF1: 0.930233\n",
      "Train Epoch: 15 [64/1083 (6%)]\tLoss: 0.011276 \tOP: 0.894737\tOR: 1.000000\tOF1: 0.944444\n",
      "Train Epoch: 15 [96/1083 (9%)]\tLoss: 0.015863 \tOP: 0.913580\tOR: 1.000000\tOF1: 0.954839\n",
      "Train Epoch: 15 [128/1083 (12%)]\tLoss: 0.014710 \tOP: 0.915493\tOR: 1.000000\tOF1: 0.955882\n",
      "Train Epoch: 15 [160/1083 (15%)]\tLoss: 0.012005 \tOP: 0.901408\tOR: 1.000000\tOF1: 0.948148\n",
      "Train Epoch: 15 [192/1083 (18%)]\tLoss: 0.016669 \tOP: 0.947368\tOR: 0.960000\tOF1: 0.953642\n",
      "Train Epoch: 15 [224/1083 (21%)]\tLoss: 0.012512 \tOP: 0.865672\tOR: 1.000000\tOF1: 0.928000\n",
      "Train Epoch: 15 [256/1083 (24%)]\tLoss: 0.012699 \tOP: 0.897059\tOR: 1.000000\tOF1: 0.945736\n",
      "Train Epoch: 15 [288/1083 (26%)]\tLoss: 0.010907 \tOP: 0.864865\tOR: 0.984615\tOF1: 0.920863\n",
      "Train Epoch: 15 [320/1083 (29%)]\tLoss: 0.016968 \tOP: 0.920000\tOR: 0.971831\tOF1: 0.945205\n",
      "Train Epoch: 15 [352/1083 (32%)]\tLoss: 0.016053 \tOP: 0.932432\tOR: 0.985714\tOF1: 0.958333\n",
      "Train Epoch: 15 [384/1083 (35%)]\tLoss: 0.010877 \tOP: 0.875000\tOR: 1.000000\tOF1: 0.933333\n",
      "Train Epoch: 15 [416/1083 (38%)]\tLoss: 0.015618 \tOP: 0.902778\tOR: 0.984848\tOF1: 0.942029\n",
      "Train Epoch: 15 [448/1083 (41%)]\tLoss: 0.017316 \tOP: 0.888889\tOR: 0.955224\tOF1: 0.920863\n",
      "Train Epoch: 15 [480/1083 (44%)]\tLoss: 0.012667 \tOP: 0.940299\tOR: 0.984375\tOF1: 0.961832\n",
      "Train Epoch: 15 [512/1083 (47%)]\tLoss: 0.013125 \tOP: 0.888889\tOR: 0.984615\tOF1: 0.934307\n",
      "Train Epoch: 15 [544/1083 (50%)]\tLoss: 0.015816 \tOP: 0.923077\tOR: 0.986301\tOF1: 0.953642\n",
      "Train Epoch: 15 [576/1083 (53%)]\tLoss: 0.014737 \tOP: 0.893333\tOR: 0.985294\tOF1: 0.937063\n",
      "Train Epoch: 15 [608/1083 (56%)]\tLoss: 0.011996 \tOP: 0.890411\tOR: 1.000000\tOF1: 0.942029\n",
      "Train Epoch: 15 [640/1083 (59%)]\tLoss: 0.011470 \tOP: 0.928571\tOR: 1.000000\tOF1: 0.962963\n",
      "Train Epoch: 15 [672/1083 (62%)]\tLoss: 0.012682 \tOP: 0.900000\tOR: 1.000000\tOF1: 0.947368\n",
      "Train Epoch: 15 [704/1083 (65%)]\tLoss: 0.017008 \tOP: 0.958333\tOR: 0.985714\tOF1: 0.971831\n",
      "Train Epoch: 15 [736/1083 (68%)]\tLoss: 0.018041 \tOP: 0.939759\tOR: 1.000000\tOF1: 0.968944\n",
      "Train Epoch: 15 [768/1083 (71%)]\tLoss: 0.014649 \tOP: 0.905405\tOR: 0.971014\tOF1: 0.937063\n",
      "Train Epoch: 15 [800/1083 (74%)]\tLoss: 0.014974 \tOP: 0.923077\tOR: 0.986301\tOF1: 0.953642\n",
      "Train Epoch: 15 [832/1083 (76%)]\tLoss: 0.012246 \tOP: 0.913043\tOR: 1.000000\tOF1: 0.954545\n",
      "Train Epoch: 15 [864/1083 (79%)]\tLoss: 0.015322 \tOP: 0.935065\tOR: 1.000000\tOF1: 0.966443\n",
      "Train Epoch: 15 [896/1083 (82%)]\tLoss: 0.018763 \tOP: 0.920000\tOR: 0.971831\tOF1: 0.945205\n",
      "Train Epoch: 15 [928/1083 (85%)]\tLoss: 0.011834 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 15 [960/1083 (88%)]\tLoss: 0.017666 \tOP: 0.973684\tOR: 0.986667\tOF1: 0.980132\n",
      "Train Epoch: 15 [992/1083 (91%)]\tLoss: 0.021571 \tOP: 0.900000\tOR: 0.969231\tOF1: 0.933333\n",
      "Train Epoch: 15 [1024/1083 (94%)]\tLoss: 0.013281 \tOP: 0.909091\tOR: 1.000000\tOF1: 0.952381\n",
      "Train Epoch: 15 [891/1083 (97%)]\tLoss: 0.012891 \tOP: 0.925373\tOR: 1.000000\tOF1: 0.961240\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2714 \n",
      "OP: 0.264706\n",
      "OR: 0.257143\n",
      "OF1: 0.260870\n",
      "\n",
      "Train Epoch: 16 [0/1083 (0%)]\tLoss: 0.013753 \tOP: 0.911392\tOR: 1.000000\tOF1: 0.953642\n",
      "Train Epoch: 16 [32/1083 (3%)]\tLoss: 0.014710 \tOP: 0.923077\tOR: 1.000000\tOF1: 0.960000\n",
      "Train Epoch: 16 [64/1083 (6%)]\tLoss: 0.013159 \tOP: 0.913043\tOR: 0.969231\tOF1: 0.940299\n",
      "Train Epoch: 16 [96/1083 (9%)]\tLoss: 0.011798 \tOP: 0.907692\tOR: 1.000000\tOF1: 0.951613\n",
      "Train Epoch: 16 [128/1083 (12%)]\tLoss: 0.011092 \tOP: 0.916667\tOR: 0.985075\tOF1: 0.949640\n",
      "Train Epoch: 16 [160/1083 (15%)]\tLoss: 0.010964 \tOP: 0.902778\tOR: 1.000000\tOF1: 0.948905\n",
      "Train Epoch: 16 [192/1083 (18%)]\tLoss: 0.010662 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 16 [224/1083 (21%)]\tLoss: 0.012729 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 16 [256/1083 (24%)]\tLoss: 0.018311 \tOP: 0.906977\tOR: 0.987342\tOF1: 0.945455\n",
      "Train Epoch: 16 [288/1083 (26%)]\tLoss: 0.015308 \tOP: 0.961538\tOR: 0.974026\tOF1: 0.967742\n",
      "Train Epoch: 16 [320/1083 (29%)]\tLoss: 0.013828 \tOP: 0.933333\tOR: 1.000000\tOF1: 0.965517\n",
      "Train Epoch: 16 [352/1083 (32%)]\tLoss: 0.011717 \tOP: 0.890411\tOR: 0.984848\tOF1: 0.935252\n",
      "Train Epoch: 16 [384/1083 (35%)]\tLoss: 0.011592 \tOP: 0.940299\tOR: 1.000000\tOF1: 0.969231\n",
      "Train Epoch: 16 [416/1083 (38%)]\tLoss: 0.012567 \tOP: 0.893333\tOR: 1.000000\tOF1: 0.943662\n",
      "Train Epoch: 16 [448/1083 (41%)]\tLoss: 0.015039 \tOP: 0.944444\tOR: 0.985507\tOF1: 0.964539\n",
      "Train Epoch: 16 [480/1083 (44%)]\tLoss: 0.013771 \tOP: 0.959459\tOR: 1.000000\tOF1: 0.979310\n",
      "Train Epoch: 16 [512/1083 (47%)]\tLoss: 0.013556 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 16 [544/1083 (50%)]\tLoss: 0.012791 \tOP: 0.900000\tOR: 0.984375\tOF1: 0.940299\n",
      "Train Epoch: 16 [576/1083 (53%)]\tLoss: 0.012955 \tOP: 0.880952\tOR: 0.986667\tOF1: 0.930818\n",
      "Train Epoch: 16 [608/1083 (56%)]\tLoss: 0.012893 \tOP: 0.902778\tOR: 0.984848\tOF1: 0.942029\n",
      "Train Epoch: 16 [640/1083 (59%)]\tLoss: 0.011972 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 16 [672/1083 (62%)]\tLoss: 0.013077 \tOP: 0.958333\tOR: 1.000000\tOF1: 0.978723\n",
      "Train Epoch: 16 [704/1083 (65%)]\tLoss: 0.011665 \tOP: 0.907895\tOR: 0.985714\tOF1: 0.945205\n",
      "Train Epoch: 16 [736/1083 (68%)]\tLoss: 0.010945 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 16 [768/1083 (71%)]\tLoss: 0.012287 \tOP: 0.909091\tOR: 0.985915\tOF1: 0.945946\n",
      "Train Epoch: 16 [800/1083 (74%)]\tLoss: 0.011322 \tOP: 0.898551\tOR: 1.000000\tOF1: 0.946565\n",
      "Train Epoch: 16 [832/1083 (76%)]\tLoss: 0.012352 \tOP: 0.914286\tOR: 1.000000\tOF1: 0.955224\n",
      "Train Epoch: 16 [864/1083 (79%)]\tLoss: 0.013381 \tOP: 0.897436\tOR: 0.985915\tOF1: 0.939597\n",
      "Train Epoch: 16 [896/1083 (82%)]\tLoss: 0.012136 \tOP: 0.932432\tOR: 0.985714\tOF1: 0.958333\n",
      "Train Epoch: 16 [928/1083 (85%)]\tLoss: 0.013132 \tOP: 0.943662\tOR: 1.000000\tOF1: 0.971014\n",
      "Train Epoch: 16 [960/1083 (88%)]\tLoss: 0.015460 \tOP: 0.909091\tOR: 0.958904\tOF1: 0.933333\n",
      "Train Epoch: 16 [992/1083 (91%)]\tLoss: 0.011375 \tOP: 0.893333\tOR: 0.985294\tOF1: 0.937063\n",
      "Train Epoch: 16 [1024/1083 (94%)]\tLoss: 0.011503 \tOP: 0.876712\tOR: 1.000000\tOF1: 0.934307\n",
      "Train Epoch: 16 [891/1083 (97%)]\tLoss: 0.011724 \tOP: 0.879310\tOR: 1.000000\tOF1: 0.935780\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2666 \n",
      "OP: 0.272727\n",
      "OR: 0.257143\n",
      "OF1: 0.264706\n",
      "\n",
      "Train Epoch: 17 [0/1083 (0%)]\tLoss: 0.010451 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 17 [32/1083 (3%)]\tLoss: 0.010876 \tOP: 0.930556\tOR: 1.000000\tOF1: 0.964029\n",
      "Train Epoch: 17 [64/1083 (6%)]\tLoss: 0.012290 \tOP: 0.914286\tOR: 1.000000\tOF1: 0.955224\n",
      "Train Epoch: 17 [96/1083 (9%)]\tLoss: 0.012678 \tOP: 0.910256\tOR: 1.000000\tOF1: 0.953020\n",
      "Train Epoch: 17 [128/1083 (12%)]\tLoss: 0.011994 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 17 [160/1083 (15%)]\tLoss: 0.010217 \tOP: 0.866667\tOR: 1.000000\tOF1: 0.928571\n",
      "Train Epoch: 17 [192/1083 (18%)]\tLoss: 0.010530 \tOP: 0.925000\tOR: 1.000000\tOF1: 0.961039\n",
      "Train Epoch: 17 [224/1083 (21%)]\tLoss: 0.012164 \tOP: 0.918919\tOR: 0.985507\tOF1: 0.951049\n",
      "Train Epoch: 17 [256/1083 (24%)]\tLoss: 0.009771 \tOP: 0.905405\tOR: 1.000000\tOF1: 0.950355\n",
      "Train Epoch: 17 [288/1083 (26%)]\tLoss: 0.011809 \tOP: 0.893333\tOR: 1.000000\tOF1: 0.943662\n",
      "Train Epoch: 17 [320/1083 (29%)]\tLoss: 0.010639 \tOP: 0.906667\tOR: 1.000000\tOF1: 0.951049\n",
      "Train Epoch: 17 [352/1083 (32%)]\tLoss: 0.013910 \tOP: 0.905405\tOR: 1.000000\tOF1: 0.950355\n",
      "Train Epoch: 17 [384/1083 (35%)]\tLoss: 0.009971 \tOP: 0.913043\tOR: 1.000000\tOF1: 0.954545\n",
      "Train Epoch: 17 [416/1083 (38%)]\tLoss: 0.013450 \tOP: 0.937500\tOR: 1.000000\tOF1: 0.967742\n",
      "Train Epoch: 17 [448/1083 (41%)]\tLoss: 0.010784 \tOP: 0.929577\tOR: 1.000000\tOF1: 0.963504\n",
      "Train Epoch: 17 [480/1083 (44%)]\tLoss: 0.010984 \tOP: 0.948718\tOR: 1.000000\tOF1: 0.973684\n",
      "Train Epoch: 17 [512/1083 (47%)]\tLoss: 0.010784 \tOP: 0.918919\tOR: 0.985507\tOF1: 0.951049\n",
      "Train Epoch: 17 [544/1083 (50%)]\tLoss: 0.010640 \tOP: 0.958333\tOR: 1.000000\tOF1: 0.978723\n",
      "Train Epoch: 17 [576/1083 (53%)]\tLoss: 0.009552 \tOP: 0.893939\tOR: 1.000000\tOF1: 0.944000\n",
      "Train Epoch: 17 [608/1083 (56%)]\tLoss: 0.011079 \tOP: 0.910256\tOR: 1.000000\tOF1: 0.953020\n",
      "Train Epoch: 17 [640/1083 (59%)]\tLoss: 0.011348 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 17 [672/1083 (62%)]\tLoss: 0.008553 \tOP: 0.828125\tOR: 1.000000\tOF1: 0.905983\n",
      "Train Epoch: 17 [704/1083 (65%)]\tLoss: 0.010462 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 17 [736/1083 (68%)]\tLoss: 0.010450 \tOP: 0.880000\tOR: 0.985075\tOF1: 0.929577\n",
      "Train Epoch: 17 [768/1083 (71%)]\tLoss: 0.012598 \tOP: 0.905882\tOR: 1.000000\tOF1: 0.950617\n",
      "Train Epoch: 17 [800/1083 (74%)]\tLoss: 0.010848 \tOP: 0.935897\tOR: 1.000000\tOF1: 0.966887\n",
      "Train Epoch: 17 [832/1083 (76%)]\tLoss: 0.011365 \tOP: 0.894737\tOR: 0.985507\tOF1: 0.937931\n",
      "Train Epoch: 17 [864/1083 (79%)]\tLoss: 0.011446 \tOP: 0.925373\tOR: 0.984127\tOF1: 0.953846\n",
      "Train Epoch: 17 [896/1083 (82%)]\tLoss: 0.012768 \tOP: 0.906667\tOR: 1.000000\tOF1: 0.951049\n",
      "Train Epoch: 17 [928/1083 (85%)]\tLoss: 0.010755 \tOP: 0.945205\tOR: 1.000000\tOF1: 0.971831\n",
      "Train Epoch: 17 [960/1083 (88%)]\tLoss: 0.014948 \tOP: 0.948052\tOR: 1.000000\tOF1: 0.973333\n",
      "Train Epoch: 17 [992/1083 (91%)]\tLoss: 0.015779 \tOP: 0.948052\tOR: 0.986486\tOF1: 0.966887\n",
      "Train Epoch: 17 [1024/1083 (94%)]\tLoss: 0.011302 \tOP: 0.931507\tOR: 1.000000\tOF1: 0.964539\n",
      "Train Epoch: 17 [891/1083 (97%)]\tLoss: 0.010803 \tOP: 0.876923\tOR: 1.000000\tOF1: 0.934426\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2724 \n",
      "OP: 0.257143\n",
      "OR: 0.257143\n",
      "OF1: 0.257143\n",
      "\n",
      "Train Epoch: 18 [0/1083 (0%)]\tLoss: 0.010361 \tOP: 0.925926\tOR: 1.000000\tOF1: 0.961538\n",
      "Train Epoch: 18 [32/1083 (3%)]\tLoss: 0.008251 \tOP: 0.845070\tOR: 1.000000\tOF1: 0.916031\n",
      "Train Epoch: 18 [64/1083 (6%)]\tLoss: 0.013793 \tOP: 0.935897\tOR: 0.986486\tOF1: 0.960526\n",
      "Train Epoch: 18 [96/1083 (9%)]\tLoss: 0.011314 \tOP: 0.941176\tOR: 1.000000\tOF1: 0.969697\n",
      "Train Epoch: 18 [128/1083 (12%)]\tLoss: 0.010441 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 18 [160/1083 (15%)]\tLoss: 0.009843 \tOP: 0.944444\tOR: 1.000000\tOF1: 0.971429\n",
      "Train Epoch: 18 [192/1083 (18%)]\tLoss: 0.010345 \tOP: 0.916667\tOR: 1.000000\tOF1: 0.956522\n",
      "Train Epoch: 18 [224/1083 (21%)]\tLoss: 0.010108 \tOP: 0.931507\tOR: 1.000000\tOF1: 0.964539\n",
      "Train Epoch: 18 [256/1083 (24%)]\tLoss: 0.008432 \tOP: 0.885714\tOR: 1.000000\tOF1: 0.939394\n",
      "Train Epoch: 18 [288/1083 (26%)]\tLoss: 0.010397 \tOP: 0.929577\tOR: 1.000000\tOF1: 0.963504\n",
      "Train Epoch: 18 [320/1083 (29%)]\tLoss: 0.010540 \tOP: 0.906667\tOR: 0.985507\tOF1: 0.944444\n",
      "Train Epoch: 18 [352/1083 (32%)]\tLoss: 0.010201 \tOP: 0.935065\tOR: 1.000000\tOF1: 0.966443\n",
      "Train Epoch: 18 [384/1083 (35%)]\tLoss: 0.009669 \tOP: 0.935897\tOR: 1.000000\tOF1: 0.966887\n",
      "Train Epoch: 18 [416/1083 (38%)]\tLoss: 0.011925 \tOP: 0.947368\tOR: 1.000000\tOF1: 0.972973\n",
      "Train Epoch: 18 [448/1083 (41%)]\tLoss: 0.010825 \tOP: 0.920000\tOR: 1.000000\tOF1: 0.958333\n",
      "Train Epoch: 18 [480/1083 (44%)]\tLoss: 0.009169 \tOP: 0.914286\tOR: 1.000000\tOF1: 0.955224\n",
      "Train Epoch: 18 [512/1083 (47%)]\tLoss: 0.012997 \tOP: 0.916667\tOR: 0.970588\tOF1: 0.942857\n",
      "Train Epoch: 18 [544/1083 (50%)]\tLoss: 0.011597 \tOP: 0.917808\tOR: 0.971014\tOF1: 0.943662\n",
      "Train Epoch: 18 [576/1083 (53%)]\tLoss: 0.011373 \tOP: 0.930556\tOR: 0.985294\tOF1: 0.957143\n",
      "Train Epoch: 18 [608/1083 (56%)]\tLoss: 0.010911 \tOP: 0.905405\tOR: 1.000000\tOF1: 0.950355\n",
      "Train Epoch: 18 [640/1083 (59%)]\tLoss: 0.011322 \tOP: 0.857143\tOR: 1.000000\tOF1: 0.923077\n",
      "Train Epoch: 18 [672/1083 (62%)]\tLoss: 0.009839 \tOP: 0.925000\tOR: 1.000000\tOF1: 0.961039\n",
      "Train Epoch: 18 [704/1083 (65%)]\tLoss: 0.009073 \tOP: 0.902778\tOR: 1.000000\tOF1: 0.948905\n",
      "Train Epoch: 18 [736/1083 (68%)]\tLoss: 0.009223 \tOP: 0.931507\tOR: 1.000000\tOF1: 0.964539\n",
      "Train Epoch: 18 [768/1083 (71%)]\tLoss: 0.011881 \tOP: 0.960000\tOR: 0.986301\tOF1: 0.972973\n",
      "Train Epoch: 18 [800/1083 (74%)]\tLoss: 0.009359 \tOP: 0.845070\tOR: 1.000000\tOF1: 0.916031\n",
      "Train Epoch: 18 [832/1083 (76%)]\tLoss: 0.014386 \tOP: 0.944444\tOR: 0.957746\tOF1: 0.951049\n",
      "Train Epoch: 18 [864/1083 (79%)]\tLoss: 0.011832 \tOP: 0.924051\tOR: 1.000000\tOF1: 0.960526\n",
      "Train Epoch: 18 [896/1083 (82%)]\tLoss: 0.007071 \tOP: 0.859375\tOR: 1.000000\tOF1: 0.924370\n",
      "Train Epoch: 18 [928/1083 (85%)]\tLoss: 0.012605 \tOP: 0.944444\tOR: 1.000000\tOF1: 0.971429\n",
      "Train Epoch: 18 [960/1083 (88%)]\tLoss: 0.009403 \tOP: 0.897436\tOR: 1.000000\tOF1: 0.945946\n",
      "Train Epoch: 18 [992/1083 (91%)]\tLoss: 0.010215 \tOP: 0.903614\tOR: 1.000000\tOF1: 0.949367\n",
      "Train Epoch: 18 [1024/1083 (94%)]\tLoss: 0.010426 \tOP: 0.914286\tOR: 1.000000\tOF1: 0.955224\n",
      "Train Epoch: 18 [891/1083 (97%)]\tLoss: 0.009434 \tOP: 0.873239\tOR: 1.000000\tOF1: 0.932331\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2798 \n",
      "OP: 0.257143\n",
      "OR: 0.257143\n",
      "OF1: 0.257143\n",
      "\n",
      "Train Epoch: 19 [0/1083 (0%)]\tLoss: 0.011848 \tOP: 0.948052\tOR: 1.000000\tOF1: 0.973333\n",
      "Train Epoch: 19 [32/1083 (3%)]\tLoss: 0.007097 \tOP: 0.885714\tOR: 1.000000\tOF1: 0.939394\n",
      "Train Epoch: 19 [64/1083 (6%)]\tLoss: 0.010086 \tOP: 0.900000\tOR: 1.000000\tOF1: 0.947368\n",
      "Train Epoch: 19 [96/1083 (9%)]\tLoss: 0.009395 \tOP: 0.947368\tOR: 1.000000\tOF1: 0.972973\n",
      "Train Epoch: 19 [128/1083 (12%)]\tLoss: 0.010589 \tOP: 0.934211\tOR: 1.000000\tOF1: 0.965986\n",
      "Train Epoch: 19 [160/1083 (15%)]\tLoss: 0.009001 \tOP: 0.901408\tOR: 1.000000\tOF1: 0.948148\n",
      "Train Epoch: 19 [192/1083 (18%)]\tLoss: 0.009972 \tOP: 0.922078\tOR: 1.000000\tOF1: 0.959459\n",
      "Train Epoch: 19 [224/1083 (21%)]\tLoss: 0.011656 \tOP: 0.923077\tOR: 0.986301\tOF1: 0.953642\n",
      "Train Epoch: 19 [256/1083 (24%)]\tLoss: 0.006877 \tOP: 0.876712\tOR: 1.000000\tOF1: 0.934307\n",
      "Train Epoch: 19 [288/1083 (26%)]\tLoss: 0.010903 \tOP: 0.913043\tOR: 1.000000\tOF1: 0.954545\n",
      "Train Epoch: 19 [320/1083 (29%)]\tLoss: 0.008541 \tOP: 0.886076\tOR: 1.000000\tOF1: 0.939597\n",
      "Train Epoch: 19 [352/1083 (32%)]\tLoss: 0.009393 \tOP: 0.906667\tOR: 1.000000\tOF1: 0.951049\n",
      "Train Epoch: 19 [384/1083 (35%)]\tLoss: 0.009389 \tOP: 0.947368\tOR: 1.000000\tOF1: 0.972973\n",
      "Train Epoch: 19 [416/1083 (38%)]\tLoss: 0.020783 \tOP: 0.974684\tOR: 0.962500\tOF1: 0.968553\n",
      "Train Epoch: 19 [448/1083 (41%)]\tLoss: 0.010788 \tOP: 0.924051\tOR: 1.000000\tOF1: 0.960526\n",
      "Train Epoch: 19 [480/1083 (44%)]\tLoss: 0.011174 \tOP: 0.916667\tOR: 1.000000\tOF1: 0.956522\n",
      "Train Epoch: 19 [512/1083 (47%)]\tLoss: 0.009014 \tOP: 0.898551\tOR: 1.000000\tOF1: 0.946565\n",
      "Train Epoch: 19 [544/1083 (50%)]\tLoss: 0.009460 \tOP: 0.917808\tOR: 1.000000\tOF1: 0.957143\n",
      "Train Epoch: 19 [576/1083 (53%)]\tLoss: 0.010140 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 19 [608/1083 (56%)]\tLoss: 0.008119 \tOP: 0.867647\tOR: 1.000000\tOF1: 0.929134\n",
      "Train Epoch: 19 [640/1083 (59%)]\tLoss: 0.007930 \tOP: 0.942029\tOR: 1.000000\tOF1: 0.970149\n",
      "Train Epoch: 19 [672/1083 (62%)]\tLoss: 0.008600 \tOP: 0.894737\tOR: 1.000000\tOF1: 0.944444\n",
      "Train Epoch: 19 [704/1083 (65%)]\tLoss: 0.007538 \tOP: 0.890411\tOR: 1.000000\tOF1: 0.942029\n",
      "Train Epoch: 19 [736/1083 (68%)]\tLoss: 0.009549 \tOP: 0.921053\tOR: 1.000000\tOF1: 0.958904\n",
      "Train Epoch: 19 [768/1083 (71%)]\tLoss: 0.009539 \tOP: 0.904110\tOR: 1.000000\tOF1: 0.949640\n",
      "Train Epoch: 19 [800/1083 (74%)]\tLoss: 0.009238 \tOP: 0.957143\tOR: 1.000000\tOF1: 0.978102\n",
      "Train Epoch: 19 [832/1083 (76%)]\tLoss: 0.009505 \tOP: 0.922078\tOR: 1.000000\tOF1: 0.959459\n",
      "Train Epoch: 19 [864/1083 (79%)]\tLoss: 0.007877 \tOP: 0.892308\tOR: 1.000000\tOF1: 0.943089\n",
      "Train Epoch: 19 [896/1083 (82%)]\tLoss: 0.008917 \tOP: 0.909091\tOR: 0.985915\tOF1: 0.945946\n",
      "Train Epoch: 19 [928/1083 (85%)]\tLoss: 0.009004 \tOP: 0.894737\tOR: 0.985507\tOF1: 0.937931\n",
      "Train Epoch: 19 [960/1083 (88%)]\tLoss: 0.010999 \tOP: 0.943662\tOR: 1.000000\tOF1: 0.971014\n",
      "Train Epoch: 19 [992/1083 (91%)]\tLoss: 0.009886 \tOP: 0.900000\tOR: 1.000000\tOF1: 0.947368\n",
      "Train Epoch: 19 [1024/1083 (94%)]\tLoss: 0.012401 \tOP: 0.932432\tOR: 1.000000\tOF1: 0.965035\n",
      "Train Epoch: 19 [891/1083 (97%)]\tLoss: 0.011761 \tOP: 0.873016\tOR: 0.982143\tOF1: 0.924370\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2768 \n",
      "OP: 0.235294\n",
      "OR: 0.228571\n",
      "OF1: 0.231884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model2.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model2(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'wider')\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model2(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "        \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for n in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for m in range(len(classes)):\n",
    "        #        if target[n][m] == 1 or preds[n][m] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[n][m] == target[n][m]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'wider')\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet152 model\n",
    "from torchvision import models\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(2048, len(classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rjw8ng/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model3 = ResNet18()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1083 (0%)]\tLoss: 0.924930 \tOP: 0.744186\tOR: 0.477612\tOF1: 0.581818\n",
      "Train Epoch: 0 [32/1083 (3%)]\tLoss: 0.914003 \tOP: 0.675000\tOR: 0.421875\tOF1: 0.519231\n",
      "Train Epoch: 0 [64/1083 (6%)]\tLoss: 0.897528 \tOP: 0.560976\tOR: 0.333333\tOF1: 0.418182\n",
      "Train Epoch: 0 [96/1083 (9%)]\tLoss: 0.885361 \tOP: 0.538462\tOR: 0.318182\tOF1: 0.400000\n",
      "Train Epoch: 0 [128/1083 (12%)]\tLoss: 0.872881 \tOP: 0.333333\tOR: 0.149254\tOF1: 0.206186\n",
      "Train Epoch: 0 [160/1083 (15%)]\tLoss: 0.862220 \tOP: 0.354839\tOR: 0.171875\tOF1: 0.231579\n",
      "Train Epoch: 0 [192/1083 (18%)]\tLoss: 0.847336 \tOP: 0.178571\tOR: 0.072464\tOF1: 0.103093\n",
      "Train Epoch: 0 [224/1083 (21%)]\tLoss: 0.839043 \tOP: 0.185185\tOR: 0.078125\tOF1: 0.109890\n",
      "Train Epoch: 0 [256/1083 (24%)]\tLoss: 0.826515 \tOP: 0.120000\tOR: 0.044118\tOF1: 0.064516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-bbf89c8d258d>:40: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [288/1083 (26%)]\tLoss: 0.816072 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [320/1083 (29%)]\tLoss: 0.808090 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [352/1083 (32%)]\tLoss: 0.795985 \tOP: 0.185185\tOR: 0.074627\tOF1: 0.106383\n",
      "Train Epoch: 0 [384/1083 (35%)]\tLoss: 0.786944 \tOP: 0.153846\tOR: 0.054795\tOF1: 0.080808\n",
      "Train Epoch: 0 [416/1083 (38%)]\tLoss: 0.781203 \tOP: 0.153846\tOR: 0.060606\tOF1: 0.086957\n",
      "Train Epoch: 0 [448/1083 (41%)]\tLoss: 0.769401 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [480/1083 (44%)]\tLoss: 0.765841 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [512/1083 (47%)]\tLoss: 0.756202 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [544/1083 (50%)]\tLoss: 0.752000 \tOP: 0.041667\tOR: 0.014085\tOF1: 0.021053\n",
      "Train Epoch: 0 [576/1083 (53%)]\tLoss: 0.747602 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [608/1083 (56%)]\tLoss: 0.743030 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [640/1083 (59%)]\tLoss: 0.738278 \tOP: 0.041667\tOR: 0.015873\tOF1: 0.022989\n",
      "Train Epoch: 0 [672/1083 (62%)]\tLoss: 0.735059 \tOP: 0.083333\tOR: 0.028986\tOF1: 0.043011\n",
      "Train Epoch: 0 [704/1083 (65%)]\tLoss: 0.729598 \tOP: 0.041667\tOR: 0.013514\tOF1: 0.020408\n",
      "Train Epoch: 0 [736/1083 (68%)]\tLoss: 0.729817 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [768/1083 (71%)]\tLoss: 0.725650 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [800/1083 (74%)]\tLoss: 0.722392 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [832/1083 (76%)]\tLoss: 0.718147 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [864/1083 (79%)]\tLoss: 0.715926 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [896/1083 (82%)]\tLoss: 0.714491 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [928/1083 (85%)]\tLoss: 0.712209 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [960/1083 (88%)]\tLoss: 0.710794 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [992/1083 (91%)]\tLoss: 0.708570 \tOP: 0.041667\tOR: 0.013889\tOF1: 0.020833\n",
      "Train Epoch: 0 [1024/1083 (94%)]\tLoss: 0.708928 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [891/1083 (97%)]\tLoss: 0.704310 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-59783cd7f140>:45: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.7064 \n",
      "OP: 0.000000\n",
      "OR: 0.000000\n",
      "OF1: nan\n",
      "\n",
      "Train Epoch: 1 [0/1083 (0%)]\tLoss: 0.705251 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [32/1083 (3%)]\tLoss: 0.703731 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [64/1083 (6%)]\tLoss: 0.701759 \tOP: 0.083333\tOR: 0.029851\tOF1: 0.043956\n",
      "Train Epoch: 1 [96/1083 (9%)]\tLoss: 0.700672 \tOP: 0.041667\tOR: 0.016129\tOF1: 0.023256\n",
      "Train Epoch: 1 [128/1083 (12%)]\tLoss: 0.701930 \tOP: 0.083333\tOR: 0.026316\tOF1: 0.040000\n",
      "Train Epoch: 1 [160/1083 (15%)]\tLoss: 0.700172 \tOP: 0.041667\tOR: 0.013699\tOF1: 0.020619\n",
      "Train Epoch: 1 [192/1083 (18%)]\tLoss: 0.699418 \tOP: 0.083333\tOR: 0.027778\tOF1: 0.041667\n",
      "Train Epoch: 1 [224/1083 (21%)]\tLoss: 0.700266 \tOP: 0.185185\tOR: 0.076923\tOF1: 0.108696\n",
      "Train Epoch: 1 [256/1083 (24%)]\tLoss: 0.699635 \tOP: 0.120000\tOR: 0.046154\tOF1: 0.066667\n",
      "Train Epoch: 1 [288/1083 (26%)]\tLoss: 0.698465 \tOP: 0.185185\tOR: 0.076923\tOF1: 0.108696\n",
      "Train Epoch: 1 [320/1083 (29%)]\tLoss: 0.696940 \tOP: 0.241379\tOR: 0.100000\tOF1: 0.141414\n",
      "Train Epoch: 1 [352/1083 (32%)]\tLoss: 0.698158 \tOP: 0.214286\tOR: 0.092308\tOF1: 0.129032\n",
      "Train Epoch: 1 [384/1083 (35%)]\tLoss: 0.698070 \tOP: 0.266667\tOR: 0.117647\tOF1: 0.163265\n",
      "Train Epoch: 1 [416/1083 (38%)]\tLoss: 0.695764 \tOP: 0.290323\tOR: 0.132353\tOF1: 0.181818\n",
      "Train Epoch: 1 [448/1083 (41%)]\tLoss: 0.695140 \tOP: 0.266667\tOR: 0.117647\tOF1: 0.163265\n",
      "Train Epoch: 1 [480/1083 (44%)]\tLoss: 0.697183 \tOP: 0.290323\tOR: 0.126761\tOF1: 0.176471\n",
      "Train Epoch: 1 [512/1083 (47%)]\tLoss: 0.695044 \tOP: 0.333333\tOR: 0.154930\tOF1: 0.211538\n",
      "Train Epoch: 1 [544/1083 (50%)]\tLoss: 0.695582 \tOP: 0.312500\tOR: 0.158730\tOF1: 0.210526\n",
      "Train Epoch: 1 [576/1083 (53%)]\tLoss: 0.698472 \tOP: 0.185185\tOR: 0.074627\tOF1: 0.106383\n",
      "Train Epoch: 1 [608/1083 (56%)]\tLoss: 0.695571 \tOP: 0.312500\tOR: 0.144928\tOF1: 0.198020\n",
      "Train Epoch: 1 [640/1083 (59%)]\tLoss: 0.697217 \tOP: 0.290323\tOR: 0.132353\tOF1: 0.181818\n",
      "Train Epoch: 1 [672/1083 (62%)]\tLoss: 0.694012 \tOP: 0.333333\tOR: 0.177419\tOF1: 0.231579\n",
      "Train Epoch: 1 [704/1083 (65%)]\tLoss: 0.692899 \tOP: 0.352941\tOR: 0.164384\tOF1: 0.224299\n",
      "Train Epoch: 1 [736/1083 (68%)]\tLoss: 0.696760 \tOP: 0.241379\tOR: 0.102941\tOF1: 0.144330\n",
      "Train Epoch: 1 [768/1083 (71%)]\tLoss: 0.696129 \tOP: 0.241379\tOR: 0.097222\tOF1: 0.138614\n",
      "Train Epoch: 1 [800/1083 (74%)]\tLoss: 0.695802 \tOP: 0.312500\tOR: 0.151515\tOF1: 0.204082\n",
      "Train Epoch: 1 [832/1083 (76%)]\tLoss: 0.693889 \tOP: 0.312500\tOR: 0.151515\tOF1: 0.204082\n",
      "Train Epoch: 1 [864/1083 (79%)]\tLoss: 0.692381 \tOP: 0.371429\tOR: 0.183099\tOF1: 0.245283\n",
      "Train Epoch: 1 [896/1083 (82%)]\tLoss: 0.691758 \tOP: 0.333333\tOR: 0.152778\tOF1: 0.209524\n",
      "Train Epoch: 1 [928/1083 (85%)]\tLoss: 0.695874 \tOP: 0.214286\tOR: 0.086957\tOF1: 0.123711\n",
      "Train Epoch: 1 [960/1083 (88%)]\tLoss: 0.694102 \tOP: 0.333333\tOR: 0.159420\tOF1: 0.215686\n",
      "Train Epoch: 1 [992/1083 (91%)]\tLoss: 0.691754 \tOP: 0.352941\tOR: 0.187500\tOF1: 0.244898\n",
      "Train Epoch: 1 [1024/1083 (94%)]\tLoss: 0.690953 \tOP: 0.388889\tOR: 0.189189\tOF1: 0.254545\n",
      "Train Epoch: 1 [891/1083 (97%)]\tLoss: 0.697694 \tOP: 0.214286\tOR: 0.109091\tOF1: 0.144578\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6953 \n",
      "OP: 0.241379\n",
      "OR: 0.200000\n",
      "OF1: 0.218750\n",
      "\n",
      "Train Epoch: 2 [0/1083 (0%)]\tLoss: 0.688572 \tOP: 0.450000\tOR: 0.257143\tOF1: 0.327273\n",
      "Train Epoch: 2 [32/1083 (3%)]\tLoss: 0.690742 \tOP: 0.388889\tOR: 0.215385\tOF1: 0.277228\n",
      "Train Epoch: 2 [64/1083 (6%)]\tLoss: 0.689665 \tOP: 0.388889\tOR: 0.208955\tOF1: 0.271845\n",
      "Train Epoch: 2 [96/1083 (9%)]\tLoss: 0.689631 \tOP: 0.405405\tOR: 0.227273\tOF1: 0.291262\n",
      "Train Epoch: 2 [128/1083 (12%)]\tLoss: 0.689758 \tOP: 0.435897\tOR: 0.232877\tOF1: 0.303571\n",
      "Train Epoch: 2 [160/1083 (15%)]\tLoss: 0.690750 \tOP: 0.421053\tOR: 0.202532\tOF1: 0.273504\n",
      "Train Epoch: 2 [192/1083 (18%)]\tLoss: 0.689715 \tOP: 0.421053\tOR: 0.228571\tOF1: 0.296296\n",
      "Train Epoch: 2 [224/1083 (21%)]\tLoss: 0.689925 \tOP: 0.435897\tOR: 0.250000\tOF1: 0.317757\n",
      "Train Epoch: 2 [256/1083 (24%)]\tLoss: 0.690257 \tOP: 0.388889\tOR: 0.233333\tOF1: 0.291667\n",
      "Train Epoch: 2 [288/1083 (26%)]\tLoss: 0.688093 \tOP: 0.450000\tOR: 0.300000\tOF1: 0.360000\n",
      "Train Epoch: 2 [320/1083 (29%)]\tLoss: 0.687284 \tOP: 0.463415\tOR: 0.316667\tOF1: 0.376238\n",
      "Train Epoch: 2 [352/1083 (32%)]\tLoss: 0.687622 \tOP: 0.463415\tOR: 0.311475\tOF1: 0.372549\n",
      "Train Epoch: 2 [384/1083 (35%)]\tLoss: 0.688823 \tOP: 0.450000\tOR: 0.264706\tOF1: 0.333333\n",
      "Train Epoch: 2 [416/1083 (38%)]\tLoss: 0.687420 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 2 [448/1083 (41%)]\tLoss: 0.686247 \tOP: 0.511111\tOR: 0.323944\tOF1: 0.396552\n",
      "Train Epoch: 2 [480/1083 (44%)]\tLoss: 0.690770 \tOP: 0.421053\tOR: 0.250000\tOF1: 0.313725\n",
      "Train Epoch: 2 [512/1083 (47%)]\tLoss: 0.692407 \tOP: 0.371429\tOR: 0.203125\tOF1: 0.262626\n",
      "Train Epoch: 2 [544/1083 (50%)]\tLoss: 0.687757 \tOP: 0.450000\tOR: 0.290323\tOF1: 0.352941\n",
      "Train Epoch: 2 [576/1083 (53%)]\tLoss: 0.688980 \tOP: 0.435897\tOR: 0.250000\tOF1: 0.317757\n",
      "Train Epoch: 2 [608/1083 (56%)]\tLoss: 0.688500 \tOP: 0.463415\tOR: 0.267606\tOF1: 0.339286\n",
      "Train Epoch: 2 [640/1083 (59%)]\tLoss: 0.688030 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 2 [672/1083 (62%)]\tLoss: 0.688340 \tOP: 0.463415\tOR: 0.275362\tOF1: 0.345455\n",
      "Train Epoch: 2 [704/1083 (65%)]\tLoss: 0.691919 \tOP: 0.388889\tOR: 0.208955\tOF1: 0.271845\n",
      "Train Epoch: 2 [736/1083 (68%)]\tLoss: 0.689013 \tOP: 0.405405\tOR: 0.217391\tOF1: 0.283019\n",
      "Train Epoch: 2 [768/1083 (71%)]\tLoss: 0.689462 \tOP: 0.421053\tOR: 0.235294\tOF1: 0.301887\n",
      "Train Epoch: 2 [800/1083 (74%)]\tLoss: 0.690114 \tOP: 0.421053\tOR: 0.242424\tOF1: 0.307692\n",
      "Train Epoch: 2 [832/1083 (76%)]\tLoss: 0.685617 \tOP: 0.463415\tOR: 0.253333\tOF1: 0.327586\n",
      "Train Epoch: 2 [864/1083 (79%)]\tLoss: 0.686613 \tOP: 0.463415\tOR: 0.250000\tOF1: 0.324786\n",
      "Train Epoch: 2 [896/1083 (82%)]\tLoss: 0.689560 \tOP: 0.435897\tOR: 0.236111\tOF1: 0.306306\n",
      "Train Epoch: 2 [928/1083 (85%)]\tLoss: 0.689653 \tOP: 0.405405\tOR: 0.205479\tOF1: 0.272727\n",
      "Train Epoch: 2 [960/1083 (88%)]\tLoss: 0.688288 \tOP: 0.450000\tOR: 0.233766\tOF1: 0.307692\n",
      "Train Epoch: 2 [992/1083 (91%)]\tLoss: 0.690199 \tOP: 0.388889\tOR: 0.197183\tOF1: 0.261682\n",
      "Train Epoch: 2 [1024/1083 (94%)]\tLoss: 0.688335 \tOP: 0.435897\tOR: 0.250000\tOF1: 0.317757\n",
      "Train Epoch: 2 [891/1083 (97%)]\tLoss: 0.689787 \tOP: 0.352941\tOR: 0.214286\tOF1: 0.266667\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6937 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 3 [0/1083 (0%)]\tLoss: 0.687882 \tOP: 0.435897\tOR: 0.293103\tOF1: 0.350515\n",
      "Train Epoch: 3 [32/1083 (3%)]\tLoss: 0.686880 \tOP: 0.463415\tOR: 0.296875\tOF1: 0.361905\n",
      "Train Epoch: 3 [64/1083 (6%)]\tLoss: 0.688531 \tOP: 0.421053\tOR: 0.253968\tOF1: 0.316832\n",
      "Train Epoch: 3 [96/1083 (9%)]\tLoss: 0.684585 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 3 [128/1083 (12%)]\tLoss: 0.688476 \tOP: 0.450000\tOR: 0.305085\tOF1: 0.363636\n",
      "Train Epoch: 3 [160/1083 (15%)]\tLoss: 0.687602 \tOP: 0.463415\tOR: 0.279412\tOF1: 0.348624\n",
      "Train Epoch: 3 [192/1083 (18%)]\tLoss: 0.685652 \tOP: 0.488372\tOR: 0.280000\tOF1: 0.355932\n",
      "Train Epoch: 3 [224/1083 (21%)]\tLoss: 0.685042 \tOP: 0.500000\tOR: 0.309859\tOF1: 0.382609\n",
      "Train Epoch: 3 [256/1083 (24%)]\tLoss: 0.688014 \tOP: 0.435897\tOR: 0.274194\tOF1: 0.336634\n",
      "Train Epoch: 3 [288/1083 (26%)]\tLoss: 0.686063 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 3 [320/1083 (29%)]\tLoss: 0.683998 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 3 [352/1083 (32%)]\tLoss: 0.687503 \tOP: 0.463415\tOR: 0.306452\tOF1: 0.368932\n",
      "Train Epoch: 3 [384/1083 (35%)]\tLoss: 0.685510 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 3 [416/1083 (38%)]\tLoss: 0.688863 \tOP: 0.405405\tOR: 0.227273\tOF1: 0.291262\n",
      "Train Epoch: 3 [448/1083 (41%)]\tLoss: 0.686134 \tOP: 0.463415\tOR: 0.275362\tOF1: 0.345455\n",
      "Train Epoch: 3 [480/1083 (44%)]\tLoss: 0.686303 \tOP: 0.463415\tOR: 0.292308\tOF1: 0.358491\n",
      "Train Epoch: 3 [512/1083 (47%)]\tLoss: 0.684984 \tOP: 0.500000\tOR: 0.338462\tOF1: 0.403670\n",
      "Train Epoch: 3 [544/1083 (50%)]\tLoss: 0.685188 \tOP: 0.488372\tOR: 0.313433\tOF1: 0.381818\n",
      "Train Epoch: 3 [576/1083 (53%)]\tLoss: 0.685218 \tOP: 0.488372\tOR: 0.291667\tOF1: 0.365217\n",
      "Train Epoch: 3 [608/1083 (56%)]\tLoss: 0.684966 \tOP: 0.488372\tOR: 0.287671\tOF1: 0.362069\n",
      "Train Epoch: 3 [640/1083 (59%)]\tLoss: 0.686606 \tOP: 0.450000\tOR: 0.236842\tOF1: 0.310345\n",
      "Train Epoch: 3 [672/1083 (62%)]\tLoss: 0.686691 \tOP: 0.450000\tOR: 0.240000\tOF1: 0.313043\n",
      "Train Epoch: 3 [704/1083 (65%)]\tLoss: 0.684202 \tOP: 0.511111\tOR: 0.333333\tOF1: 0.403509\n",
      "Train Epoch: 3 [736/1083 (68%)]\tLoss: 0.686010 \tOP: 0.463415\tOR: 0.287879\tOF1: 0.355140\n",
      "Train Epoch: 3 [768/1083 (71%)]\tLoss: 0.685745 \tOP: 0.463415\tOR: 0.267606\tOF1: 0.339286\n",
      "Train Epoch: 3 [800/1083 (74%)]\tLoss: 0.687088 \tOP: 0.435897\tOR: 0.242857\tOF1: 0.311927\n",
      "Train Epoch: 3 [832/1083 (76%)]\tLoss: 0.688156 \tOP: 0.450000\tOR: 0.272727\tOF1: 0.339623\n",
      "Train Epoch: 3 [864/1083 (79%)]\tLoss: 0.688495 \tOP: 0.421053\tOR: 0.242424\tOF1: 0.307692\n",
      "Train Epoch: 3 [896/1083 (82%)]\tLoss: 0.687100 \tOP: 0.450000\tOR: 0.243243\tOF1: 0.315789\n",
      "Train Epoch: 3 [928/1083 (85%)]\tLoss: 0.686027 \tOP: 0.463415\tOR: 0.287879\tOF1: 0.355140\n",
      "Train Epoch: 3 [960/1083 (88%)]\tLoss: 0.684003 \tOP: 0.488372\tOR: 0.272727\tOF1: 0.350000\n",
      "Train Epoch: 3 [992/1083 (91%)]\tLoss: 0.688589 \tOP: 0.421053\tOR: 0.210526\tOF1: 0.280702\n",
      "Train Epoch: 3 [1024/1083 (94%)]\tLoss: 0.685744 \tOP: 0.450000\tOR: 0.276923\tOF1: 0.342857\n",
      "Train Epoch: 3 [891/1083 (97%)]\tLoss: 0.684022 \tOP: 0.476190\tOR: 0.312500\tOF1: 0.377358\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6929 \n",
      "OP: 0.241379\n",
      "OR: 0.200000\n",
      "OF1: 0.218750\n",
      "\n",
      "Train Epoch: 4 [0/1083 (0%)]\tLoss: 0.685653 \tOP: 0.450000\tOR: 0.243243\tOF1: 0.315789\n",
      "Train Epoch: 4 [32/1083 (3%)]\tLoss: 0.686586 \tOP: 0.450000\tOR: 0.295082\tOF1: 0.356436\n",
      "Train Epoch: 4 [64/1083 (6%)]\tLoss: 0.682909 \tOP: 0.521739\tOR: 0.380952\tOF1: 0.440367\n",
      "Train Epoch: 4 [96/1083 (9%)]\tLoss: 0.688762 \tOP: 0.435897\tOR: 0.265625\tOF1: 0.330097\n",
      "Train Epoch: 4 [128/1083 (12%)]\tLoss: 0.684521 \tOP: 0.521739\tOR: 0.380952\tOF1: 0.440367\n",
      "Train Epoch: 4 [160/1083 (15%)]\tLoss: 0.684686 \tOP: 0.488372\tOR: 0.318182\tOF1: 0.385321\n",
      "Train Epoch: 4 [192/1083 (18%)]\tLoss: 0.683276 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 4 [224/1083 (21%)]\tLoss: 0.686063 \tOP: 0.476190\tOR: 0.273973\tOF1: 0.347826\n",
      "Train Epoch: 4 [256/1083 (24%)]\tLoss: 0.685376 \tOP: 0.488372\tOR: 0.300000\tOF1: 0.371681\n",
      "Train Epoch: 4 [288/1083 (26%)]\tLoss: 0.684869 \tOP: 0.488372\tOR: 0.333333\tOF1: 0.396226\n",
      "Train Epoch: 4 [320/1083 (29%)]\tLoss: 0.685748 \tOP: 0.476190\tOR: 0.312500\tOF1: 0.377358\n",
      "Train Epoch: 4 [352/1083 (32%)]\tLoss: 0.686379 \tOP: 0.476190\tOR: 0.303030\tOF1: 0.370370\n",
      "Train Epoch: 4 [384/1083 (35%)]\tLoss: 0.683754 \tOP: 0.500000\tOR: 0.318841\tOF1: 0.389381\n",
      "Train Epoch: 4 [416/1083 (38%)]\tLoss: 0.686282 \tOP: 0.463415\tOR: 0.271429\tOF1: 0.342342\n",
      "Train Epoch: 4 [448/1083 (41%)]\tLoss: 0.683265 \tOP: 0.511111\tOR: 0.328571\tOF1: 0.400000\n",
      "Train Epoch: 4 [480/1083 (44%)]\tLoss: 0.683747 \tOP: 0.511111\tOR: 0.338235\tOF1: 0.407080\n",
      "Train Epoch: 4 [512/1083 (47%)]\tLoss: 0.684308 \tOP: 0.521739\tOR: 0.333333\tOF1: 0.406780\n",
      "Train Epoch: 4 [544/1083 (50%)]\tLoss: 0.684595 \tOP: 0.488372\tOR: 0.300000\tOF1: 0.371681\n",
      "Train Epoch: 4 [576/1083 (53%)]\tLoss: 0.686459 \tOP: 0.450000\tOR: 0.250000\tOF1: 0.321429\n",
      "Train Epoch: 4 [608/1083 (56%)]\tLoss: 0.684835 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 4 [640/1083 (59%)]\tLoss: 0.684099 \tOP: 0.500000\tOR: 0.293333\tOF1: 0.369748\n",
      "Train Epoch: 4 [672/1083 (62%)]\tLoss: 0.683372 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 4 [704/1083 (65%)]\tLoss: 0.685255 \tOP: 0.500000\tOR: 0.343750\tOF1: 0.407407\n",
      "Train Epoch: 4 [736/1083 (68%)]\tLoss: 0.683696 \tOP: 0.500000\tOR: 0.309859\tOF1: 0.382609\n",
      "Train Epoch: 4 [768/1083 (71%)]\tLoss: 0.686570 \tOP: 0.488372\tOR: 0.304348\tOF1: 0.375000\n",
      "Train Epoch: 4 [800/1083 (74%)]\tLoss: 0.684314 \tOP: 0.511111\tOR: 0.315068\tOF1: 0.389831\n",
      "Train Epoch: 4 [832/1083 (76%)]\tLoss: 0.683117 \tOP: 0.511111\tOR: 0.302632\tOF1: 0.380165\n",
      "Train Epoch: 4 [864/1083 (79%)]\tLoss: 0.690328 \tOP: 0.388889\tOR: 0.218750\tOF1: 0.280000\n",
      "Train Epoch: 4 [896/1083 (82%)]\tLoss: 0.684156 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 4 [928/1083 (85%)]\tLoss: 0.688196 \tOP: 0.435897\tOR: 0.250000\tOF1: 0.317757\n",
      "Train Epoch: 4 [960/1083 (88%)]\tLoss: 0.684344 \tOP: 0.500000\tOR: 0.354839\tOF1: 0.415094\n",
      "Train Epoch: 4 [992/1083 (91%)]\tLoss: 0.685142 \tOP: 0.476190\tOR: 0.281690\tOF1: 0.353982\n",
      "Train Epoch: 4 [1024/1083 (94%)]\tLoss: 0.686201 \tOP: 0.476190\tOR: 0.270270\tOF1: 0.344828\n",
      "Train Epoch: 4 [891/1083 (97%)]\tLoss: 0.683713 \tOP: 0.463415\tOR: 0.365385\tOF1: 0.408602\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6947 \n",
      "OP: 0.290323\n",
      "OR: 0.257143\n",
      "OF1: 0.272727\n",
      "\n",
      "Train Epoch: 5 [0/1083 (0%)]\tLoss: 0.684514 \tOP: 0.488372\tOR: 0.287671\tOF1: 0.362069\n",
      "Train Epoch: 5 [32/1083 (3%)]\tLoss: 0.682529 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 5 [64/1083 (6%)]\tLoss: 0.682828 \tOP: 0.521739\tOR: 0.347826\tOF1: 0.417391\n",
      "Train Epoch: 5 [96/1083 (9%)]\tLoss: 0.684889 \tOP: 0.488372\tOR: 0.350000\tOF1: 0.407767\n",
      "Train Epoch: 5 [128/1083 (12%)]\tLoss: 0.682432 \tOP: 0.521739\tOR: 0.347826\tOF1: 0.417391\n",
      "Train Epoch: 5 [160/1083 (15%)]\tLoss: 0.681508 \tOP: 0.551020\tOR: 0.364865\tOF1: 0.439024\n",
      "Train Epoch: 5 [192/1083 (18%)]\tLoss: 0.684436 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 5 [224/1083 (21%)]\tLoss: 0.687912 \tOP: 0.435897\tOR: 0.246377\tOF1: 0.314815\n",
      "Train Epoch: 5 [256/1083 (24%)]\tLoss: 0.687256 \tOP: 0.463415\tOR: 0.306452\tOF1: 0.368932\n",
      "Train Epoch: 5 [288/1083 (26%)]\tLoss: 0.685996 \tOP: 0.450000\tOR: 0.264706\tOF1: 0.333333\n",
      "Train Epoch: 5 [320/1083 (29%)]\tLoss: 0.683934 \tOP: 0.488372\tOR: 0.280000\tOF1: 0.355932\n",
      "Train Epoch: 5 [352/1083 (32%)]\tLoss: 0.687623 \tOP: 0.450000\tOR: 0.253521\tOF1: 0.324324\n",
      "Train Epoch: 5 [384/1083 (35%)]\tLoss: 0.685193 \tOP: 0.476190\tOR: 0.285714\tOF1: 0.357143\n",
      "Train Epoch: 5 [416/1083 (38%)]\tLoss: 0.681648 \tOP: 0.551020\tOR: 0.409091\tOF1: 0.469565\n",
      "Train Epoch: 5 [448/1083 (41%)]\tLoss: 0.684105 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 5 [480/1083 (44%)]\tLoss: 0.682220 \tOP: 0.531915\tOR: 0.347222\tOF1: 0.420168\n",
      "Train Epoch: 5 [512/1083 (47%)]\tLoss: 0.683948 \tOP: 0.500000\tOR: 0.314286\tOF1: 0.385965\n",
      "Train Epoch: 5 [544/1083 (50%)]\tLoss: 0.683091 \tOP: 0.511111\tOR: 0.333333\tOF1: 0.403509\n",
      "Train Epoch: 5 [576/1083 (53%)]\tLoss: 0.681418 \tOP: 0.541667\tOR: 0.388060\tOF1: 0.452174\n",
      "Train Epoch: 5 [608/1083 (56%)]\tLoss: 0.682500 \tOP: 0.521739\tOR: 0.347826\tOF1: 0.417391\n",
      "Train Epoch: 5 [640/1083 (59%)]\tLoss: 0.685708 \tOP: 0.463415\tOR: 0.287879\tOF1: 0.355140\n",
      "Train Epoch: 5 [672/1083 (62%)]\tLoss: 0.682426 \tOP: 0.521739\tOR: 0.333333\tOF1: 0.406780\n",
      "Train Epoch: 5 [704/1083 (65%)]\tLoss: 0.684136 \tOP: 0.511111\tOR: 0.310811\tOF1: 0.386555\n",
      "Train Epoch: 5 [736/1083 (68%)]\tLoss: 0.685132 \tOP: 0.476190\tOR: 0.303030\tOF1: 0.370370\n",
      "Train Epoch: 5 [768/1083 (71%)]\tLoss: 0.681967 \tOP: 0.531915\tOR: 0.403226\tOF1: 0.458716\n",
      "Train Epoch: 5 [800/1083 (74%)]\tLoss: 0.686057 \tOP: 0.476190\tOR: 0.317460\tOF1: 0.380952\n",
      "Train Epoch: 5 [832/1083 (76%)]\tLoss: 0.682754 \tOP: 0.521739\tOR: 0.352941\tOF1: 0.421053\n",
      "Train Epoch: 5 [864/1083 (79%)]\tLoss: 0.684635 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 5 [896/1083 (82%)]\tLoss: 0.686211 \tOP: 0.450000\tOR: 0.276923\tOF1: 0.342857\n",
      "Train Epoch: 5 [928/1083 (85%)]\tLoss: 0.682318 \tOP: 0.531915\tOR: 0.367647\tOF1: 0.434783\n",
      "Train Epoch: 5 [960/1083 (88%)]\tLoss: 0.685131 \tOP: 0.476190\tOR: 0.322581\tOF1: 0.384615\n",
      "Train Epoch: 5 [992/1083 (91%)]\tLoss: 0.683480 \tOP: 0.488372\tOR: 0.291667\tOF1: 0.365217\n",
      "Train Epoch: 5 [1024/1083 (94%)]\tLoss: 0.682730 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 5 [891/1083 (97%)]\tLoss: 0.682188 \tOP: 0.500000\tOR: 0.392857\tOF1: 0.440000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6929 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 6 [0/1083 (0%)]\tLoss: 0.687641 \tOP: 0.447368\tOR: 0.246377\tOF1: 0.317757\n",
      "Train Epoch: 6 [32/1083 (3%)]\tLoss: 0.684880 \tOP: 0.488372\tOR: 0.350000\tOF1: 0.407767\n",
      "Train Epoch: 6 [64/1083 (6%)]\tLoss: 0.681722 \tOP: 0.531915\tOR: 0.357143\tOF1: 0.427350\n",
      "Train Epoch: 6 [96/1083 (9%)]\tLoss: 0.682487 \tOP: 0.521739\tOR: 0.375000\tOF1: 0.436364\n",
      "Train Epoch: 6 [128/1083 (12%)]\tLoss: 0.681432 \tOP: 0.531915\tOR: 0.357143\tOF1: 0.427350\n",
      "Train Epoch: 6 [160/1083 (15%)]\tLoss: 0.685485 \tOP: 0.450000\tOR: 0.310345\tOF1: 0.367347\n",
      "Train Epoch: 6 [192/1083 (18%)]\tLoss: 0.681823 \tOP: 0.543478\tOR: 0.403226\tOF1: 0.462963\n",
      "Train Epoch: 6 [224/1083 (21%)]\tLoss: 0.682936 \tOP: 0.500000\tOR: 0.309859\tOF1: 0.382609\n",
      "Train Epoch: 6 [256/1083 (24%)]\tLoss: 0.685344 \tOP: 0.476190\tOR: 0.294118\tOF1: 0.363636\n",
      "Train Epoch: 6 [288/1083 (26%)]\tLoss: 0.682050 \tOP: 0.521739\tOR: 0.352941\tOF1: 0.421053\n",
      "Train Epoch: 6 [320/1083 (29%)]\tLoss: 0.684946 \tOP: 0.476190\tOR: 0.285714\tOF1: 0.357143\n",
      "Train Epoch: 6 [352/1083 (32%)]\tLoss: 0.682402 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 6 [384/1083 (35%)]\tLoss: 0.686100 \tOP: 0.463415\tOR: 0.243590\tOF1: 0.319328\n",
      "Train Epoch: 6 [416/1083 (38%)]\tLoss: 0.684075 \tOP: 0.522727\tOR: 0.338235\tOF1: 0.410714\n",
      "Train Epoch: 6 [448/1083 (41%)]\tLoss: 0.681409 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 6 [480/1083 (44%)]\tLoss: 0.683700 \tOP: 0.488372\tOR: 0.323077\tOF1: 0.388889\n",
      "Train Epoch: 6 [512/1083 (47%)]\tLoss: 0.685097 \tOP: 0.500000\tOR: 0.308824\tOF1: 0.381818\n",
      "Train Epoch: 6 [544/1083 (50%)]\tLoss: 0.681387 \tOP: 0.541667\tOR: 0.351351\tOF1: 0.426230\n",
      "Train Epoch: 6 [576/1083 (53%)]\tLoss: 0.682629 \tOP: 0.511111\tOR: 0.328571\tOF1: 0.400000\n",
      "Train Epoch: 6 [608/1083 (56%)]\tLoss: 0.682431 \tOP: 0.521739\tOR: 0.369231\tOF1: 0.432432\n",
      "Train Epoch: 6 [640/1083 (59%)]\tLoss: 0.685585 \tOP: 0.500000\tOR: 0.318182\tOF1: 0.388889\n",
      "Train Epoch: 6 [672/1083 (62%)]\tLoss: 0.681143 \tOP: 0.562500\tOR: 0.415385\tOF1: 0.477876\n",
      "Train Epoch: 6 [704/1083 (65%)]\tLoss: 0.682653 \tOP: 0.521739\tOR: 0.363636\tOF1: 0.428571\n",
      "Train Epoch: 6 [736/1083 (68%)]\tLoss: 0.682551 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 6 [768/1083 (71%)]\tLoss: 0.683730 \tOP: 0.511111\tOR: 0.359375\tOF1: 0.422018\n",
      "Train Epoch: 6 [800/1083 (74%)]\tLoss: 0.685452 \tOP: 0.522727\tOR: 0.338235\tOF1: 0.410714\n",
      "Train Epoch: 6 [832/1083 (76%)]\tLoss: 0.687349 \tOP: 0.487805\tOR: 0.277778\tOF1: 0.353982\n",
      "Train Epoch: 6 [864/1083 (79%)]\tLoss: 0.684768 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 6 [896/1083 (82%)]\tLoss: 0.681277 \tOP: 0.531915\tOR: 0.396825\tOF1: 0.454545\n",
      "Train Epoch: 6 [928/1083 (85%)]\tLoss: 0.685887 \tOP: 0.463415\tOR: 0.263889\tOF1: 0.336283\n",
      "Train Epoch: 6 [960/1083 (88%)]\tLoss: 0.682145 \tOP: 0.543478\tOR: 0.342466\tOF1: 0.420168\n",
      "Train Epoch: 6 [992/1083 (91%)]\tLoss: 0.684911 \tOP: 0.488372\tOR: 0.280000\tOF1: 0.355932\n",
      "Train Epoch: 6 [1024/1083 (94%)]\tLoss: 0.682701 \tOP: 0.500000\tOR: 0.309859\tOF1: 0.382609\n",
      "Train Epoch: 6 [891/1083 (97%)]\tLoss: 0.680804 \tOP: 0.500000\tOR: 0.354839\tOF1: 0.415094\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6931 \n",
      "OP: 0.185185\n",
      "OR: 0.142857\n",
      "OF1: 0.161290\n",
      "\n",
      "Train Epoch: 7 [0/1083 (0%)]\tLoss: 0.681840 \tOP: 0.543478\tOR: 0.390625\tOF1: 0.454545\n",
      "Train Epoch: 7 [32/1083 (3%)]\tLoss: 0.682080 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 7 [64/1083 (6%)]\tLoss: 0.686401 \tOP: 0.475000\tOR: 0.271429\tOF1: 0.345455\n",
      "Train Epoch: 7 [96/1083 (9%)]\tLoss: 0.681346 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 7 [128/1083 (12%)]\tLoss: 0.688280 \tOP: 0.421053\tOR: 0.246154\tOF1: 0.310680\n",
      "Train Epoch: 7 [160/1083 (15%)]\tLoss: 0.680498 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 7 [192/1083 (18%)]\tLoss: 0.685275 \tOP: 0.487805\tOR: 0.303030\tOF1: 0.373832\n",
      "Train Epoch: 7 [224/1083 (21%)]\tLoss: 0.683235 \tOP: 0.511628\tOR: 0.318841\tOF1: 0.392857\n",
      "Train Epoch: 7 [256/1083 (24%)]\tLoss: 0.681265 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 7 [288/1083 (26%)]\tLoss: 0.685324 \tOP: 0.500000\tOR: 0.344262\tOF1: 0.407767\n",
      "Train Epoch: 7 [320/1083 (29%)]\tLoss: 0.680657 \tOP: 0.562500\tOR: 0.364865\tOF1: 0.442623\n",
      "Train Epoch: 7 [352/1083 (32%)]\tLoss: 0.682941 \tOP: 0.511111\tOR: 0.338235\tOF1: 0.407080\n",
      "Train Epoch: 7 [384/1083 (35%)]\tLoss: 0.682753 \tOP: 0.521739\tOR: 0.369231\tOF1: 0.432432\n",
      "Train Epoch: 7 [416/1083 (38%)]\tLoss: 0.681182 \tOP: 0.541667\tOR: 0.346667\tOF1: 0.422764\n",
      "Train Epoch: 7 [448/1083 (41%)]\tLoss: 0.686554 \tOP: 0.435897\tOR: 0.257576\tOF1: 0.323810\n",
      "Train Epoch: 7 [480/1083 (44%)]\tLoss: 0.682981 \tOP: 0.522727\tOR: 0.302632\tOF1: 0.383333\n",
      "Train Epoch: 7 [512/1083 (47%)]\tLoss: 0.686307 \tOP: 0.450000\tOR: 0.253521\tOF1: 0.324324\n",
      "Train Epoch: 7 [544/1083 (50%)]\tLoss: 0.684583 \tOP: 0.488372\tOR: 0.328125\tOF1: 0.392523\n",
      "Train Epoch: 7 [576/1083 (53%)]\tLoss: 0.680809 \tOP: 0.551020\tOR: 0.397059\tOF1: 0.461538\n",
      "Train Epoch: 7 [608/1083 (56%)]\tLoss: 0.681967 \tOP: 0.553191\tOR: 0.412698\tOF1: 0.472727\n",
      "Train Epoch: 7 [640/1083 (59%)]\tLoss: 0.682298 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 7 [672/1083 (62%)]\tLoss: 0.682937 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 7 [704/1083 (65%)]\tLoss: 0.681704 \tOP: 0.531915\tOR: 0.367647\tOF1: 0.434783\n",
      "Train Epoch: 7 [736/1083 (68%)]\tLoss: 0.683062 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 7 [768/1083 (71%)]\tLoss: 0.683384 \tOP: 0.533333\tOR: 0.369231\tOF1: 0.436364\n",
      "Train Epoch: 7 [800/1083 (74%)]\tLoss: 0.682410 \tOP: 0.521739\tOR: 0.363636\tOF1: 0.428571\n",
      "Train Epoch: 7 [832/1083 (76%)]\tLoss: 0.682438 \tOP: 0.522727\tOR: 0.310811\tOF1: 0.389831\n",
      "Train Epoch: 7 [864/1083 (79%)]\tLoss: 0.682490 \tOP: 0.543478\tOR: 0.357143\tOF1: 0.431034\n",
      "Train Epoch: 7 [896/1083 (82%)]\tLoss: 0.680656 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 7 [928/1083 (85%)]\tLoss: 0.684032 \tOP: 0.500000\tOR: 0.304348\tOF1: 0.378378\n",
      "Train Epoch: 7 [960/1083 (88%)]\tLoss: 0.683192 \tOP: 0.511111\tOR: 0.365079\tOF1: 0.425926\n",
      "Train Epoch: 7 [992/1083 (91%)]\tLoss: 0.680396 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 7 [1024/1083 (94%)]\tLoss: 0.681321 \tOP: 0.541667\tOR: 0.376812\tOF1: 0.444444\n",
      "Train Epoch: 7 [891/1083 (97%)]\tLoss: 0.682214 \tOP: 0.487805\tOR: 0.392157\tOF1: 0.434783\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6922 \n",
      "OP: 0.266667\n",
      "OR: 0.228571\n",
      "OF1: 0.246154\n",
      "\n",
      "Train Epoch: 8 [0/1083 (0%)]\tLoss: 0.681192 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 8 [32/1083 (3%)]\tLoss: 0.680689 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 8 [64/1083 (6%)]\tLoss: 0.680383 \tOP: 0.571429\tOR: 0.417910\tOF1: 0.482759\n",
      "Train Epoch: 8 [96/1083 (9%)]\tLoss: 0.682003 \tOP: 0.543478\tOR: 0.367647\tOF1: 0.438596\n",
      "Train Epoch: 8 [128/1083 (12%)]\tLoss: 0.685589 \tOP: 0.475000\tOR: 0.283582\tOF1: 0.355140\n",
      "Train Epoch: 8 [160/1083 (15%)]\tLoss: 0.681396 \tOP: 0.543478\tOR: 0.396825\tOF1: 0.458716\n",
      "Train Epoch: 8 [192/1083 (18%)]\tLoss: 0.682806 \tOP: 0.511111\tOR: 0.343284\tOF1: 0.410714\n",
      "Train Epoch: 8 [224/1083 (21%)]\tLoss: 0.682565 \tOP: 0.533333\tOR: 0.338028\tOF1: 0.413793\n",
      "Train Epoch: 8 [256/1083 (24%)]\tLoss: 0.683282 \tOP: 0.522727\tOR: 0.377049\tOF1: 0.438095\n",
      "Train Epoch: 8 [288/1083 (26%)]\tLoss: 0.685311 \tOP: 0.476190\tOR: 0.294118\tOF1: 0.363636\n",
      "Train Epoch: 8 [320/1083 (29%)]\tLoss: 0.685539 \tOP: 0.463415\tOR: 0.311475\tOF1: 0.372549\n",
      "Train Epoch: 8 [352/1083 (32%)]\tLoss: 0.682979 \tOP: 0.511628\tOR: 0.333333\tOF1: 0.403670\n",
      "Train Epoch: 8 [384/1083 (35%)]\tLoss: 0.684235 \tOP: 0.511628\tOR: 0.314286\tOF1: 0.389381\n",
      "Train Epoch: 8 [416/1083 (38%)]\tLoss: 0.684997 \tOP: 0.488372\tOR: 0.304348\tOF1: 0.375000\n",
      "Train Epoch: 8 [448/1083 (41%)]\tLoss: 0.681350 \tOP: 0.541667\tOR: 0.412698\tOF1: 0.468468\n",
      "Train Epoch: 8 [480/1083 (44%)]\tLoss: 0.681896 \tOP: 0.531915\tOR: 0.396825\tOF1: 0.454545\n",
      "Train Epoch: 8 [512/1083 (47%)]\tLoss: 0.683758 \tOP: 0.500000\tOR: 0.300000\tOF1: 0.375000\n",
      "Train Epoch: 8 [544/1083 (50%)]\tLoss: 0.682739 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 8 [576/1083 (53%)]\tLoss: 0.682657 \tOP: 0.521739\tOR: 0.328767\tOF1: 0.403361\n",
      "Train Epoch: 8 [608/1083 (56%)]\tLoss: 0.682200 \tOP: 0.533333\tOR: 0.324324\tOF1: 0.403361\n",
      "Train Epoch: 8 [640/1083 (59%)]\tLoss: 0.682453 \tOP: 0.511111\tOR: 0.306667\tOF1: 0.383333\n",
      "Train Epoch: 8 [672/1083 (62%)]\tLoss: 0.681138 \tOP: 0.541667\tOR: 0.400000\tOF1: 0.460177\n",
      "Train Epoch: 8 [704/1083 (65%)]\tLoss: 0.682108 \tOP: 0.522727\tOR: 0.319444\tOF1: 0.396552\n",
      "Train Epoch: 8 [736/1083 (68%)]\tLoss: 0.681666 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 8 [768/1083 (71%)]\tLoss: 0.686429 \tOP: 0.461538\tOR: 0.281250\tOF1: 0.349515\n",
      "Train Epoch: 8 [800/1083 (74%)]\tLoss: 0.681869 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 8 [832/1083 (76%)]\tLoss: 0.679768 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 8 [864/1083 (79%)]\tLoss: 0.682232 \tOP: 0.521739\tOR: 0.358209\tOF1: 0.424779\n",
      "Train Epoch: 8 [896/1083 (82%)]\tLoss: 0.681416 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 8 [928/1083 (85%)]\tLoss: 0.680946 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 8 [960/1083 (88%)]\tLoss: 0.680551 \tOP: 0.562500\tOR: 0.409091\tOF1: 0.473684\n",
      "Train Epoch: 8 [992/1083 (91%)]\tLoss: 0.682664 \tOP: 0.521739\tOR: 0.315789\tOF1: 0.393443\n",
      "Train Epoch: 8 [1024/1083 (94%)]\tLoss: 0.681863 \tOP: 0.533333\tOR: 0.333333\tOF1: 0.410256\n",
      "Train Epoch: 8 [891/1083 (97%)]\tLoss: 0.681145 \tOP: 0.511628\tOR: 0.440000\tOF1: 0.473118\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6935 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 9 [0/1083 (0%)]\tLoss: 0.681169 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 9 [32/1083 (3%)]\tLoss: 0.680984 \tOP: 0.541667\tOR: 0.361111\tOF1: 0.433333\n",
      "Train Epoch: 9 [64/1083 (6%)]\tLoss: 0.680182 \tOP: 0.560000\tOR: 0.444444\tOF1: 0.495575\n",
      "Train Epoch: 9 [96/1083 (9%)]\tLoss: 0.680563 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 9 [128/1083 (12%)]\tLoss: 0.680932 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 9 [160/1083 (15%)]\tLoss: 0.682958 \tOP: 0.533333\tOR: 0.347826\tOF1: 0.421053\n",
      "Train Epoch: 9 [192/1083 (18%)]\tLoss: 0.683561 \tOP: 0.500000\tOR: 0.360656\tOF1: 0.419048\n",
      "Train Epoch: 9 [224/1083 (21%)]\tLoss: 0.681141 \tOP: 0.553191\tOR: 0.351351\tOF1: 0.429752\n",
      "Train Epoch: 9 [256/1083 (24%)]\tLoss: 0.684031 \tOP: 0.522727\tOR: 0.359375\tOF1: 0.425926\n",
      "Train Epoch: 9 [288/1083 (26%)]\tLoss: 0.683953 \tOP: 0.522727\tOR: 0.343284\tOF1: 0.414414\n",
      "Train Epoch: 9 [320/1083 (29%)]\tLoss: 0.686148 \tOP: 0.475000\tOR: 0.275362\tOF1: 0.348624\n",
      "Train Epoch: 9 [352/1083 (32%)]\tLoss: 0.681477 \tOP: 0.543478\tOR: 0.367647\tOF1: 0.438596\n",
      "Train Epoch: 9 [384/1083 (35%)]\tLoss: 0.682411 \tOP: 0.522727\tOR: 0.323944\tOF1: 0.400000\n",
      "Train Epoch: 9 [416/1083 (38%)]\tLoss: 0.681656 \tOP: 0.541667\tOR: 0.393939\tOF1: 0.456140\n",
      "Train Epoch: 9 [448/1083 (41%)]\tLoss: 0.679550 \tOP: 0.580000\tOR: 0.381579\tOF1: 0.460317\n",
      "Train Epoch: 9 [480/1083 (44%)]\tLoss: 0.681952 \tOP: 0.533333\tOR: 0.380952\tOF1: 0.444444\n",
      "Train Epoch: 9 [512/1083 (47%)]\tLoss: 0.682503 \tOP: 0.522727\tOR: 0.348485\tOF1: 0.418182\n",
      "Train Epoch: 9 [544/1083 (50%)]\tLoss: 0.679303 \tOP: 0.588235\tOR: 0.434783\tOF1: 0.500000\n",
      "Train Epoch: 9 [576/1083 (53%)]\tLoss: 0.682294 \tOP: 0.531915\tOR: 0.347222\tOF1: 0.420168\n",
      "Train Epoch: 9 [608/1083 (56%)]\tLoss: 0.680767 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 9 [640/1083 (59%)]\tLoss: 0.682041 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 9 [672/1083 (62%)]\tLoss: 0.681436 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 9 [704/1083 (65%)]\tLoss: 0.684959 \tOP: 0.476190\tOR: 0.298507\tOF1: 0.366972\n",
      "Train Epoch: 9 [736/1083 (68%)]\tLoss: 0.680933 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 9 [768/1083 (71%)]\tLoss: 0.682967 \tOP: 0.500000\tOR: 0.318841\tOF1: 0.389381\n",
      "Train Epoch: 9 [800/1083 (74%)]\tLoss: 0.680924 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 9 [832/1083 (76%)]\tLoss: 0.681841 \tOP: 0.543478\tOR: 0.352113\tOF1: 0.427350\n",
      "Train Epoch: 9 [864/1083 (79%)]\tLoss: 0.682898 \tOP: 0.500000\tOR: 0.314286\tOF1: 0.385965\n",
      "Train Epoch: 9 [896/1083 (82%)]\tLoss: 0.683030 \tOP: 0.511628\tOR: 0.343750\tOF1: 0.411215\n",
      "Train Epoch: 9 [928/1083 (85%)]\tLoss: 0.680357 \tOP: 0.551020\tOR: 0.397059\tOF1: 0.461538\n",
      "Train Epoch: 9 [960/1083 (88%)]\tLoss: 0.682800 \tOP: 0.533333\tOR: 0.363636\tOF1: 0.432432\n",
      "Train Epoch: 9 [992/1083 (91%)]\tLoss: 0.681745 \tOP: 0.533333\tOR: 0.380952\tOF1: 0.444444\n",
      "Train Epoch: 9 [1024/1083 (94%)]\tLoss: 0.682768 \tOP: 0.522727\tOR: 0.338235\tOF1: 0.410714\n",
      "Train Epoch: 9 [891/1083 (97%)]\tLoss: 0.679839 \tOP: 0.533333\tOR: 0.393443\tOF1: 0.452830\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6955 \n",
      "OP: 0.241379\n",
      "OR: 0.200000\n",
      "OF1: 0.218750\n",
      "\n",
      "Train Epoch: 10 [0/1083 (0%)]\tLoss: 0.684221 \tOP: 0.463415\tOR: 0.283582\tOF1: 0.351852\n",
      "Train Epoch: 10 [32/1083 (3%)]\tLoss: 0.682265 \tOP: 0.543478\tOR: 0.416667\tOF1: 0.471698\n",
      "Train Epoch: 10 [64/1083 (6%)]\tLoss: 0.679982 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 10 [96/1083 (9%)]\tLoss: 0.679814 \tOP: 0.571429\tOR: 0.451613\tOF1: 0.504505\n",
      "Train Epoch: 10 [128/1083 (12%)]\tLoss: 0.681022 \tOP: 0.553191\tOR: 0.490566\tOF1: 0.520000\n",
      "Train Epoch: 10 [160/1083 (15%)]\tLoss: 0.681911 \tOP: 0.533333\tOR: 0.400000\tOF1: 0.457143\n",
      "Train Epoch: 10 [192/1083 (18%)]\tLoss: 0.683546 \tOP: 0.511628\tOR: 0.354839\tOF1: 0.419048\n",
      "Train Epoch: 10 [224/1083 (21%)]\tLoss: 0.681485 \tOP: 0.531915\tOR: 0.362319\tOF1: 0.431034\n",
      "Train Epoch: 10 [256/1083 (24%)]\tLoss: 0.681400 \tOP: 0.543478\tOR: 0.367647\tOF1: 0.438596\n",
      "Train Epoch: 10 [288/1083 (26%)]\tLoss: 0.683028 \tOP: 0.500000\tOR: 0.323529\tOF1: 0.392857\n",
      "Train Epoch: 10 [320/1083 (29%)]\tLoss: 0.680860 \tOP: 0.553191\tOR: 0.366197\tOF1: 0.440678\n",
      "Train Epoch: 10 [352/1083 (32%)]\tLoss: 0.679962 \tOP: 0.560000\tOR: 0.394366\tOF1: 0.462810\n",
      "Train Epoch: 10 [384/1083 (35%)]\tLoss: 0.683799 \tOP: 0.476190\tOR: 0.307692\tOF1: 0.373832\n",
      "Train Epoch: 10 [416/1083 (38%)]\tLoss: 0.680643 \tOP: 0.560000\tOR: 0.383562\tOF1: 0.455285\n",
      "Train Epoch: 10 [448/1083 (41%)]\tLoss: 0.680676 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 10 [480/1083 (44%)]\tLoss: 0.680468 \tOP: 0.571429\tOR: 0.400000\tOF1: 0.470588\n",
      "Train Epoch: 10 [512/1083 (47%)]\tLoss: 0.679893 \tOP: 0.571429\tOR: 0.459016\tOF1: 0.509091\n",
      "Train Epoch: 10 [544/1083 (50%)]\tLoss: 0.682638 \tOP: 0.522727\tOR: 0.302632\tOF1: 0.383333\n",
      "Train Epoch: 10 [576/1083 (53%)]\tLoss: 0.680330 \tOP: 0.571429\tOR: 0.437500\tOF1: 0.495575\n",
      "Train Epoch: 10 [608/1083 (56%)]\tLoss: 0.680190 \tOP: 0.571429\tOR: 0.363636\tOF1: 0.444444\n",
      "Train Epoch: 10 [640/1083 (59%)]\tLoss: 0.682157 \tOP: 0.533333\tOR: 0.369231\tOF1: 0.436364\n",
      "Train Epoch: 10 [672/1083 (62%)]\tLoss: 0.679988 \tOP: 0.571429\tOR: 0.383562\tOF1: 0.459016\n",
      "Train Epoch: 10 [704/1083 (65%)]\tLoss: 0.683247 \tOP: 0.522727\tOR: 0.306667\tOF1: 0.386555\n",
      "Train Epoch: 10 [736/1083 (68%)]\tLoss: 0.682209 \tOP: 0.521739\tOR: 0.338028\tOF1: 0.410256\n",
      "Train Epoch: 10 [768/1083 (71%)]\tLoss: 0.682243 \tOP: 0.521739\tOR: 0.375000\tOF1: 0.436364\n",
      "Train Epoch: 10 [800/1083 (74%)]\tLoss: 0.682216 \tOP: 0.522727\tOR: 0.319444\tOF1: 0.396552\n",
      "Train Epoch: 10 [832/1083 (76%)]\tLoss: 0.680947 \tOP: 0.553191\tOR: 0.371429\tOF1: 0.444444\n",
      "Train Epoch: 10 [864/1083 (79%)]\tLoss: 0.682639 \tOP: 0.500000\tOR: 0.314286\tOF1: 0.385965\n",
      "Train Epoch: 10 [896/1083 (82%)]\tLoss: 0.678886 \tOP: 0.588235\tOR: 0.422535\tOF1: 0.491803\n",
      "Train Epoch: 10 [928/1083 (85%)]\tLoss: 0.682578 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 10 [960/1083 (88%)]\tLoss: 0.680689 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 10 [992/1083 (91%)]\tLoss: 0.682634 \tOP: 0.522727\tOR: 0.323944\tOF1: 0.400000\n",
      "Train Epoch: 10 [1024/1083 (94%)]\tLoss: 0.680722 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 10 [891/1083 (97%)]\tLoss: 0.680640 \tOP: 0.511628\tOR: 0.360656\tOF1: 0.423077\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6934 \n",
      "OP: 0.266667\n",
      "OR: 0.228571\n",
      "OF1: 0.246154\n",
      "\n",
      "Train Epoch: 11 [0/1083 (0%)]\tLoss: 0.679774 \tOP: 0.568627\tOR: 0.414286\tOF1: 0.479339\n",
      "Train Epoch: 11 [32/1083 (3%)]\tLoss: 0.684321 \tOP: 0.511628\tOR: 0.333333\tOF1: 0.403670\n",
      "Train Epoch: 11 [64/1083 (6%)]\tLoss: 0.683748 \tOP: 0.488372\tOR: 0.328125\tOF1: 0.392523\n",
      "Train Epoch: 11 [96/1083 (9%)]\tLoss: 0.682134 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 11 [128/1083 (12%)]\tLoss: 0.680476 \tOP: 0.551020\tOR: 0.421875\tOF1: 0.477876\n",
      "Train Epoch: 11 [160/1083 (15%)]\tLoss: 0.684461 \tOP: 0.476190\tOR: 0.327869\tOF1: 0.388350\n",
      "Train Epoch: 11 [192/1083 (18%)]\tLoss: 0.681466 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 11 [224/1083 (21%)]\tLoss: 0.681114 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 11 [256/1083 (24%)]\tLoss: 0.679359 \tOP: 0.580000\tOR: 0.408451\tOF1: 0.479339\n",
      "Train Epoch: 11 [288/1083 (26%)]\tLoss: 0.682769 \tOP: 0.521739\tOR: 0.393443\tOF1: 0.448598\n",
      "Train Epoch: 11 [320/1083 (29%)]\tLoss: 0.690340 \tOP: 0.416667\tOR: 0.220588\tOF1: 0.288462\n",
      "Train Epoch: 11 [352/1083 (32%)]\tLoss: 0.680017 \tOP: 0.560000\tOR: 0.405797\tOF1: 0.470588\n",
      "Train Epoch: 11 [384/1083 (35%)]\tLoss: 0.682222 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 11 [416/1083 (38%)]\tLoss: 0.681840 \tOP: 0.533333\tOR: 0.358209\tOF1: 0.428571\n",
      "Train Epoch: 11 [448/1083 (41%)]\tLoss: 0.682337 \tOP: 0.522727\tOR: 0.370968\tOF1: 0.433962\n",
      "Train Epoch: 11 [480/1083 (44%)]\tLoss: 0.679761 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 11 [512/1083 (47%)]\tLoss: 0.678619 \tOP: 0.596154\tOR: 0.397436\tOF1: 0.476923\n",
      "Train Epoch: 11 [544/1083 (50%)]\tLoss: 0.681976 \tOP: 0.533333\tOR: 0.328767\tOF1: 0.406780\n",
      "Train Epoch: 11 [576/1083 (53%)]\tLoss: 0.681091 \tOP: 0.531915\tOR: 0.378788\tOF1: 0.442478\n",
      "Train Epoch: 11 [608/1083 (56%)]\tLoss: 0.682478 \tOP: 0.522727\tOR: 0.348485\tOF1: 0.418182\n",
      "Train Epoch: 11 [640/1083 (59%)]\tLoss: 0.682660 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 11 [672/1083 (62%)]\tLoss: 0.684072 \tOP: 0.511628\tOR: 0.328358\tOF1: 0.400000\n",
      "Train Epoch: 11 [704/1083 (65%)]\tLoss: 0.680253 \tOP: 0.560000\tOR: 0.400000\tOF1: 0.466667\n",
      "Train Epoch: 11 [736/1083 (68%)]\tLoss: 0.681158 \tOP: 0.543478\tOR: 0.357143\tOF1: 0.431034\n",
      "Train Epoch: 11 [768/1083 (71%)]\tLoss: 0.681768 \tOP: 0.533333\tOR: 0.333333\tOF1: 0.410256\n",
      "Train Epoch: 11 [800/1083 (74%)]\tLoss: 0.680159 \tOP: 0.571429\tOR: 0.451613\tOF1: 0.504505\n",
      "Train Epoch: 11 [832/1083 (76%)]\tLoss: 0.678851 \tOP: 0.588235\tOR: 0.422535\tOF1: 0.491803\n",
      "Train Epoch: 11 [864/1083 (79%)]\tLoss: 0.681652 \tOP: 0.543478\tOR: 0.378788\tOF1: 0.446429\n",
      "Train Epoch: 11 [896/1083 (82%)]\tLoss: 0.682451 \tOP: 0.522727\tOR: 0.353846\tOF1: 0.422018\n",
      "Train Epoch: 11 [928/1083 (85%)]\tLoss: 0.679576 \tOP: 0.571429\tOR: 0.437500\tOF1: 0.495575\n",
      "Train Epoch: 11 [960/1083 (88%)]\tLoss: 0.679995 \tOP: 0.562500\tOR: 0.415385\tOF1: 0.477876\n",
      "Train Epoch: 11 [992/1083 (91%)]\tLoss: 0.680574 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 11 [1024/1083 (94%)]\tLoss: 0.680613 \tOP: 0.551020\tOR: 0.375000\tOF1: 0.446281\n",
      "Train Epoch: 11 [891/1083 (97%)]\tLoss: 0.681669 \tOP: 0.488372\tOR: 0.338710\tOF1: 0.400000\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6934 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 12 [0/1083 (0%)]\tLoss: 0.680322 \tOP: 0.571429\tOR: 0.474576\tOF1: 0.518519\n",
      "Train Epoch: 12 [32/1083 (3%)]\tLoss: 0.682408 \tOP: 0.533333\tOR: 0.311688\tOF1: 0.393443\n",
      "Train Epoch: 12 [64/1083 (6%)]\tLoss: 0.683311 \tOP: 0.500000\tOR: 0.304348\tOF1: 0.378378\n",
      "Train Epoch: 12 [96/1083 (9%)]\tLoss: 0.681284 \tOP: 0.543478\tOR: 0.403226\tOF1: 0.462963\n",
      "Train Epoch: 12 [128/1083 (12%)]\tLoss: 0.678994 \tOP: 0.588235\tOR: 0.428571\tOF1: 0.495868\n",
      "Train Epoch: 12 [160/1083 (15%)]\tLoss: 0.683450 \tOP: 0.511628\tOR: 0.297297\tOF1: 0.376068\n",
      "Train Epoch: 12 [192/1083 (18%)]\tLoss: 0.680161 \tOP: 0.571429\tOR: 0.411765\tOF1: 0.478632\n",
      "Train Epoch: 12 [224/1083 (21%)]\tLoss: 0.681255 \tOP: 0.541667\tOR: 0.371429\tOF1: 0.440678\n",
      "Train Epoch: 12 [256/1083 (24%)]\tLoss: 0.681884 \tOP: 0.521739\tOR: 0.333333\tOF1: 0.406780\n",
      "Train Epoch: 12 [288/1083 (26%)]\tLoss: 0.680858 \tOP: 0.541667\tOR: 0.433333\tOF1: 0.481481\n",
      "Train Epoch: 12 [320/1083 (29%)]\tLoss: 0.680136 \tOP: 0.580000\tOR: 0.439394\tOF1: 0.500000\n",
      "Train Epoch: 12 [352/1083 (32%)]\tLoss: 0.684556 \tOP: 0.488372\tOR: 0.333333\tOF1: 0.396226\n",
      "Train Epoch: 12 [384/1083 (35%)]\tLoss: 0.682920 \tOP: 0.511628\tOR: 0.338462\tOF1: 0.407407\n",
      "Train Epoch: 12 [416/1083 (38%)]\tLoss: 0.683896 \tOP: 0.500000\tOR: 0.350000\tOF1: 0.411765\n",
      "Train Epoch: 12 [448/1083 (41%)]\tLoss: 0.680236 \tOP: 0.562500\tOR: 0.402985\tOF1: 0.469565\n",
      "Train Epoch: 12 [480/1083 (44%)]\tLoss: 0.679395 \tOP: 0.568627\tOR: 0.397260\tOF1: 0.467742\n",
      "Train Epoch: 12 [512/1083 (47%)]\tLoss: 0.683464 \tOP: 0.511628\tOR: 0.328358\tOF1: 0.400000\n",
      "Train Epoch: 12 [544/1083 (50%)]\tLoss: 0.680285 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 12 [576/1083 (53%)]\tLoss: 0.680278 \tOP: 0.562500\tOR: 0.409091\tOF1: 0.473684\n",
      "Train Epoch: 12 [608/1083 (56%)]\tLoss: 0.681714 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 12 [640/1083 (59%)]\tLoss: 0.679802 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 12 [672/1083 (62%)]\tLoss: 0.683364 \tOP: 0.488372\tOR: 0.308824\tOF1: 0.378378\n",
      "Train Epoch: 12 [704/1083 (65%)]\tLoss: 0.679895 \tOP: 0.560000\tOR: 0.394366\tOF1: 0.462810\n",
      "Train Epoch: 12 [736/1083 (68%)]\tLoss: 0.680850 \tOP: 0.541667\tOR: 0.366197\tOF1: 0.436975\n",
      "Train Epoch: 12 [768/1083 (71%)]\tLoss: 0.682577 \tOP: 0.511628\tOR: 0.305556\tOF1: 0.382609\n",
      "Train Epoch: 12 [800/1083 (74%)]\tLoss: 0.682257 \tOP: 0.543478\tOR: 0.337838\tOF1: 0.416667\n",
      "Train Epoch: 12 [832/1083 (76%)]\tLoss: 0.678630 \tOP: 0.596154\tOR: 0.442857\tOF1: 0.508197\n",
      "Train Epoch: 12 [864/1083 (79%)]\tLoss: 0.680647 \tOP: 0.541667\tOR: 0.412698\tOF1: 0.468468\n",
      "Train Epoch: 12 [896/1083 (82%)]\tLoss: 0.683614 \tOP: 0.500000\tOR: 0.313433\tOF1: 0.385321\n",
      "Train Epoch: 12 [928/1083 (85%)]\tLoss: 0.682079 \tOP: 0.543478\tOR: 0.378788\tOF1: 0.446429\n",
      "Train Epoch: 12 [960/1083 (88%)]\tLoss: 0.679376 \tOP: 0.580000\tOR: 0.483333\tOF1: 0.527273\n",
      "Train Epoch: 12 [992/1083 (91%)]\tLoss: 0.679776 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 12 [1024/1083 (94%)]\tLoss: 0.679686 \tOP: 0.580000\tOR: 0.381579\tOF1: 0.460317\n",
      "Train Epoch: 12 [891/1083 (97%)]\tLoss: 0.678145 \tOP: 0.562500\tOR: 0.421875\tOF1: 0.482143\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6931 \n",
      "OP: 0.214286\n",
      "OR: 0.171429\n",
      "OF1: 0.190476\n",
      "\n",
      "Train Epoch: 13 [0/1083 (0%)]\tLoss: 0.681413 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 13 [32/1083 (3%)]\tLoss: 0.681579 \tOP: 0.533333\tOR: 0.320000\tOF1: 0.400000\n",
      "Train Epoch: 13 [64/1083 (6%)]\tLoss: 0.681181 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 13 [96/1083 (9%)]\tLoss: 0.682764 \tOP: 0.511628\tOR: 0.338462\tOF1: 0.407407\n",
      "Train Epoch: 13 [128/1083 (12%)]\tLoss: 0.681871 \tOP: 0.533333\tOR: 0.387097\tOF1: 0.448598\n",
      "Train Epoch: 13 [160/1083 (15%)]\tLoss: 0.680987 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 13 [192/1083 (18%)]\tLoss: 0.681297 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 13 [224/1083 (21%)]\tLoss: 0.679013 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 13 [256/1083 (24%)]\tLoss: 0.686631 \tOP: 0.461538\tOR: 0.285714\tOF1: 0.352941\n",
      "Train Epoch: 13 [288/1083 (26%)]\tLoss: 0.681202 \tOP: 0.543478\tOR: 0.396825\tOF1: 0.458716\n",
      "Train Epoch: 13 [320/1083 (29%)]\tLoss: 0.679857 \tOP: 0.571429\tOR: 0.430769\tOF1: 0.491228\n",
      "Train Epoch: 13 [352/1083 (32%)]\tLoss: 0.680607 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 13 [384/1083 (35%)]\tLoss: 0.681175 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 13 [416/1083 (38%)]\tLoss: 0.679769 \tOP: 0.571429\tOR: 0.368421\tOF1: 0.448000\n",
      "Train Epoch: 13 [448/1083 (41%)]\tLoss: 0.681595 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 13 [480/1083 (44%)]\tLoss: 0.680170 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 13 [512/1083 (47%)]\tLoss: 0.680396 \tOP: 0.562500\tOR: 0.450000\tOF1: 0.500000\n",
      "Train Epoch: 13 [544/1083 (50%)]\tLoss: 0.681805 \tOP: 0.533333\tOR: 0.342857\tOF1: 0.417391\n",
      "Train Epoch: 13 [576/1083 (53%)]\tLoss: 0.681017 \tOP: 0.531915\tOR: 0.324675\tOF1: 0.403226\n",
      "Train Epoch: 13 [608/1083 (56%)]\tLoss: 0.678376 \tOP: 0.596154\tOR: 0.436620\tOF1: 0.504065\n",
      "Train Epoch: 13 [640/1083 (59%)]\tLoss: 0.683138 \tOP: 0.522727\tOR: 0.353846\tOF1: 0.422018\n",
      "Train Epoch: 13 [672/1083 (62%)]\tLoss: 0.680839 \tOP: 0.553191\tOR: 0.412698\tOF1: 0.472727\n",
      "Train Epoch: 13 [704/1083 (65%)]\tLoss: 0.682154 \tOP: 0.522727\tOR: 0.359375\tOF1: 0.425926\n",
      "Train Epoch: 13 [736/1083 (68%)]\tLoss: 0.679472 \tOP: 0.580000\tOR: 0.386667\tOF1: 0.464000\n",
      "Train Epoch: 13 [768/1083 (71%)]\tLoss: 0.680683 \tOP: 0.553191\tOR: 0.356164\tOF1: 0.433333\n",
      "Train Epoch: 13 [800/1083 (74%)]\tLoss: 0.679782 \tOP: 0.560000\tOR: 0.451613\tOF1: 0.500000\n",
      "Train Epoch: 13 [832/1083 (76%)]\tLoss: 0.680642 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 13 [864/1083 (79%)]\tLoss: 0.679879 \tOP: 0.571429\tOR: 0.400000\tOF1: 0.470588\n",
      "Train Epoch: 13 [896/1083 (82%)]\tLoss: 0.684062 \tOP: 0.463415\tOR: 0.271429\tOF1: 0.342342\n",
      "Train Epoch: 13 [928/1083 (85%)]\tLoss: 0.682239 \tOP: 0.511111\tOR: 0.323944\tOF1: 0.396552\n",
      "Train Epoch: 13 [960/1083 (88%)]\tLoss: 0.679794 \tOP: 0.571429\tOR: 0.474576\tOF1: 0.518519\n",
      "Train Epoch: 13 [992/1083 (91%)]\tLoss: 0.679642 \tOP: 0.588235\tOR: 0.416667\tOF1: 0.487805\n",
      "Train Epoch: 13 [1024/1083 (94%)]\tLoss: 0.680716 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 13 [891/1083 (97%)]\tLoss: 0.679789 \tOP: 0.521739\tOR: 0.400000\tOF1: 0.452830\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6918 \n",
      "OP: 0.312500\n",
      "OR: 0.285714\n",
      "OF1: 0.298507\n",
      "\n",
      "Train Epoch: 14 [0/1083 (0%)]\tLoss: 0.681064 \tOP: 0.543478\tOR: 0.352113\tOF1: 0.427350\n",
      "Train Epoch: 14 [32/1083 (3%)]\tLoss: 0.682320 \tOP: 0.511111\tOR: 0.370968\tOF1: 0.429907\n",
      "Train Epoch: 14 [64/1083 (6%)]\tLoss: 0.684336 \tOP: 0.476190\tOR: 0.277778\tOF1: 0.350877\n",
      "Train Epoch: 14 [96/1083 (9%)]\tLoss: 0.680517 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 14 [128/1083 (12%)]\tLoss: 0.679812 \tOP: 0.571429\tOR: 0.430769\tOF1: 0.491228\n",
      "Train Epoch: 14 [160/1083 (15%)]\tLoss: 0.681234 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 14 [192/1083 (18%)]\tLoss: 0.681118 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 14 [224/1083 (21%)]\tLoss: 0.681117 \tOP: 0.543478\tOR: 0.357143\tOF1: 0.431034\n",
      "Train Epoch: 14 [256/1083 (24%)]\tLoss: 0.681755 \tOP: 0.533333\tOR: 0.342857\tOF1: 0.417391\n",
      "Train Epoch: 14 [288/1083 (26%)]\tLoss: 0.679347 \tOP: 0.580000\tOR: 0.362500\tOF1: 0.446154\n",
      "Train Epoch: 14 [320/1083 (29%)]\tLoss: 0.679511 \tOP: 0.580000\tOR: 0.420290\tOF1: 0.487395\n",
      "Train Epoch: 14 [352/1083 (32%)]\tLoss: 0.683176 \tOP: 0.500000\tOR: 0.323077\tOF1: 0.392523\n",
      "Train Epoch: 14 [384/1083 (35%)]\tLoss: 0.682214 \tOP: 0.522727\tOR: 0.348485\tOF1: 0.418182\n",
      "Train Epoch: 14 [416/1083 (38%)]\tLoss: 0.678291 \tOP: 0.596154\tOR: 0.484375\tOF1: 0.534483\n",
      "Train Epoch: 14 [448/1083 (41%)]\tLoss: 0.681126 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 14 [480/1083 (44%)]\tLoss: 0.681975 \tOP: 0.511111\tOR: 0.353846\tOF1: 0.418182\n",
      "Train Epoch: 14 [512/1083 (47%)]\tLoss: 0.685302 \tOP: 0.461538\tOR: 0.260870\tOF1: 0.333333\n",
      "Train Epoch: 14 [544/1083 (50%)]\tLoss: 0.680385 \tOP: 0.562500\tOR: 0.428571\tOF1: 0.486486\n",
      "Train Epoch: 14 [576/1083 (53%)]\tLoss: 0.683999 \tOP: 0.476190\tOR: 0.294118\tOF1: 0.363636\n",
      "Train Epoch: 14 [608/1083 (56%)]\tLoss: 0.678431 \tOP: 0.596154\tOR: 0.455882\tOF1: 0.516667\n",
      "Train Epoch: 14 [640/1083 (59%)]\tLoss: 0.678911 \tOP: 0.580000\tOR: 0.371795\tOF1: 0.453125\n",
      "Train Epoch: 14 [672/1083 (62%)]\tLoss: 0.680594 \tOP: 0.553191\tOR: 0.388060\tOF1: 0.456140\n",
      "Train Epoch: 14 [704/1083 (65%)]\tLoss: 0.679049 \tOP: 0.588235\tOR: 0.441176\tOF1: 0.504202\n",
      "Train Epoch: 14 [736/1083 (68%)]\tLoss: 0.680303 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 14 [768/1083 (71%)]\tLoss: 0.679225 \tOP: 0.580000\tOR: 0.453125\tOF1: 0.508772\n",
      "Train Epoch: 14 [800/1083 (74%)]\tLoss: 0.680899 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 14 [832/1083 (76%)]\tLoss: 0.678578 \tOP: 0.588235\tOR: 0.434783\tOF1: 0.500000\n",
      "Train Epoch: 14 [864/1083 (79%)]\tLoss: 0.681925 \tOP: 0.533333\tOR: 0.358209\tOF1: 0.428571\n",
      "Train Epoch: 14 [896/1083 (82%)]\tLoss: 0.680374 \tOP: 0.551020\tOR: 0.442623\tOF1: 0.490909\n",
      "Train Epoch: 14 [928/1083 (85%)]\tLoss: 0.679656 \tOP: 0.568627\tOR: 0.397260\tOF1: 0.467742\n",
      "Train Epoch: 14 [960/1083 (88%)]\tLoss: 0.681105 \tOP: 0.543478\tOR: 0.384615\tOF1: 0.450450\n",
      "Train Epoch: 14 [992/1083 (91%)]\tLoss: 0.682288 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 14 [1024/1083 (94%)]\tLoss: 0.678235 \tOP: 0.596154\tOR: 0.436620\tOF1: 0.504065\n",
      "Train Epoch: 14 [891/1083 (97%)]\tLoss: 0.681508 \tOP: 0.487805\tOR: 0.307692\tOF1: 0.377358\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6939 \n",
      "OP: 0.241379\n",
      "OR: 0.200000\n",
      "OF1: 0.218750\n",
      "\n",
      "Train Epoch: 15 [0/1083 (0%)]\tLoss: 0.680587 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 15 [32/1083 (3%)]\tLoss: 0.678432 \tOP: 0.596154\tOR: 0.455882\tOF1: 0.516667\n",
      "Train Epoch: 15 [64/1083 (6%)]\tLoss: 0.680596 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 15 [96/1083 (9%)]\tLoss: 0.679476 \tOP: 0.580000\tOR: 0.432836\tOF1: 0.495726\n",
      "Train Epoch: 15 [128/1083 (12%)]\tLoss: 0.687027 \tOP: 0.435897\tOR: 0.288136\tOF1: 0.346939\n",
      "Train Epoch: 15 [160/1083 (15%)]\tLoss: 0.681202 \tOP: 0.553191\tOR: 0.406250\tOF1: 0.468468\n",
      "Train Epoch: 15 [192/1083 (18%)]\tLoss: 0.681765 \tOP: 0.533333\tOR: 0.393443\tOF1: 0.452830\n",
      "Train Epoch: 15 [224/1083 (21%)]\tLoss: 0.681299 \tOP: 0.553191\tOR: 0.412698\tOF1: 0.472727\n",
      "Train Epoch: 15 [256/1083 (24%)]\tLoss: 0.678575 \tOP: 0.588235\tOR: 0.447761\tOF1: 0.508475\n",
      "Train Epoch: 15 [288/1083 (26%)]\tLoss: 0.680390 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 15 [320/1083 (29%)]\tLoss: 0.679124 \tOP: 0.580000\tOR: 0.386667\tOF1: 0.464000\n",
      "Train Epoch: 15 [352/1083 (32%)]\tLoss: 0.683151 \tOP: 0.500000\tOR: 0.313433\tOF1: 0.385321\n",
      "Train Epoch: 15 [384/1083 (35%)]\tLoss: 0.681225 \tOP: 0.543478\tOR: 0.373134\tOF1: 0.442478\n",
      "Train Epoch: 15 [416/1083 (38%)]\tLoss: 0.682257 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 15 [448/1083 (41%)]\tLoss: 0.680080 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 15 [480/1083 (44%)]\tLoss: 0.680086 \tOP: 0.562500\tOR: 0.380282\tOF1: 0.453782\n",
      "Train Epoch: 15 [512/1083 (47%)]\tLoss: 0.678068 \tOP: 0.596154\tOR: 0.455882\tOF1: 0.516667\n",
      "Train Epoch: 15 [544/1083 (50%)]\tLoss: 0.678671 \tOP: 0.588235\tOR: 0.434783\tOF1: 0.500000\n",
      "Train Epoch: 15 [576/1083 (53%)]\tLoss: 0.681594 \tOP: 0.521739\tOR: 0.338028\tOF1: 0.410256\n",
      "Train Epoch: 15 [608/1083 (56%)]\tLoss: 0.681754 \tOP: 0.533333\tOR: 0.347826\tOF1: 0.421053\n",
      "Train Epoch: 15 [640/1083 (59%)]\tLoss: 0.680838 \tOP: 0.541667\tOR: 0.366197\tOF1: 0.436975\n",
      "Train Epoch: 15 [672/1083 (62%)]\tLoss: 0.680327 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 15 [704/1083 (65%)]\tLoss: 0.684475 \tOP: 0.500000\tOR: 0.328358\tOF1: 0.396396\n",
      "Train Epoch: 15 [736/1083 (68%)]\tLoss: 0.679259 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 15 [768/1083 (71%)]\tLoss: 0.682567 \tOP: 0.522727\tOR: 0.315068\tOF1: 0.393162\n",
      "Train Epoch: 15 [800/1083 (74%)]\tLoss: 0.682131 \tOP: 0.511111\tOR: 0.348485\tOF1: 0.414414\n",
      "Train Epoch: 15 [832/1083 (76%)]\tLoss: 0.680199 \tOP: 0.562500\tOR: 0.428571\tOF1: 0.486486\n",
      "Train Epoch: 15 [864/1083 (79%)]\tLoss: 0.680598 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 15 [896/1083 (82%)]\tLoss: 0.680752 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 15 [928/1083 (85%)]\tLoss: 0.680743 \tOP: 0.541667\tOR: 0.361111\tOF1: 0.433333\n",
      "Train Epoch: 15 [960/1083 (88%)]\tLoss: 0.681863 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 15 [992/1083 (91%)]\tLoss: 0.680673 \tOP: 0.553191\tOR: 0.406250\tOF1: 0.468468\n",
      "Train Epoch: 15 [1024/1083 (94%)]\tLoss: 0.679771 \tOP: 0.571429\tOR: 0.405797\tOF1: 0.474576\n",
      "Train Epoch: 15 [891/1083 (97%)]\tLoss: 0.678361 \tOP: 0.553191\tOR: 0.464286\tOF1: 0.504854\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6925 \n",
      "OP: 0.312500\n",
      "OR: 0.285714\n",
      "OF1: 0.298507\n",
      "\n",
      "Train Epoch: 16 [0/1083 (0%)]\tLoss: 0.682872 \tOP: 0.511628\tOR: 0.338462\tOF1: 0.407407\n",
      "Train Epoch: 16 [32/1083 (3%)]\tLoss: 0.679516 \tOP: 0.571429\tOR: 0.405797\tOF1: 0.474576\n",
      "Train Epoch: 16 [64/1083 (6%)]\tLoss: 0.682939 \tOP: 0.511628\tOR: 0.309859\tOF1: 0.385965\n",
      "Train Epoch: 16 [96/1083 (9%)]\tLoss: 0.678819 \tOP: 0.588235\tOR: 0.441176\tOF1: 0.504202\n",
      "Train Epoch: 16 [128/1083 (12%)]\tLoss: 0.681053 \tOP: 0.553191\tOR: 0.371429\tOF1: 0.444444\n",
      "Train Epoch: 16 [160/1083 (15%)]\tLoss: 0.680725 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 16 [192/1083 (18%)]\tLoss: 0.679006 \tOP: 0.588235\tOR: 0.405405\tOF1: 0.480000\n",
      "Train Epoch: 16 [224/1083 (21%)]\tLoss: 0.681108 \tOP: 0.531915\tOR: 0.352113\tOF1: 0.423729\n",
      "Train Epoch: 16 [256/1083 (24%)]\tLoss: 0.679698 \tOP: 0.571429\tOR: 0.394366\tOF1: 0.466667\n",
      "Train Epoch: 16 [288/1083 (26%)]\tLoss: 0.682308 \tOP: 0.562500\tOR: 0.375000\tOF1: 0.450000\n",
      "Train Epoch: 16 [320/1083 (29%)]\tLoss: 0.682046 \tOP: 0.522727\tOR: 0.323944\tOF1: 0.400000\n",
      "Train Epoch: 16 [352/1083 (32%)]\tLoss: 0.678857 \tOP: 0.588235\tOR: 0.434783\tOF1: 0.500000\n",
      "Train Epoch: 16 [384/1083 (35%)]\tLoss: 0.680831 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 16 [416/1083 (38%)]\tLoss: 0.682605 \tOP: 0.500000\tOR: 0.366667\tOF1: 0.423077\n",
      "Train Epoch: 16 [448/1083 (41%)]\tLoss: 0.678802 \tOP: 0.588235\tOR: 0.416667\tOF1: 0.487805\n",
      "Train Epoch: 16 [480/1083 (44%)]\tLoss: 0.682043 \tOP: 0.511111\tOR: 0.319444\tOF1: 0.393162\n",
      "Train Epoch: 16 [512/1083 (47%)]\tLoss: 0.680054 \tOP: 0.562500\tOR: 0.355263\tOF1: 0.435484\n",
      "Train Epoch: 16 [544/1083 (50%)]\tLoss: 0.680681 \tOP: 0.543478\tOR: 0.367647\tOF1: 0.438596\n",
      "Train Epoch: 16 [576/1083 (53%)]\tLoss: 0.684667 \tOP: 0.475000\tOR: 0.322034\tOF1: 0.383838\n",
      "Train Epoch: 16 [608/1083 (56%)]\tLoss: 0.678659 \tOP: 0.588235\tOR: 0.441176\tOF1: 0.504202\n",
      "Train Epoch: 16 [640/1083 (59%)]\tLoss: 0.682972 \tOP: 0.511628\tOR: 0.328358\tOF1: 0.400000\n",
      "Train Epoch: 16 [672/1083 (62%)]\tLoss: 0.682453 \tOP: 0.500000\tOR: 0.305556\tOF1: 0.379310\n",
      "Train Epoch: 16 [704/1083 (65%)]\tLoss: 0.680630 \tOP: 0.541667\tOR: 0.388060\tOF1: 0.452174\n",
      "Train Epoch: 16 [736/1083 (68%)]\tLoss: 0.683989 \tOP: 0.543478\tOR: 0.390625\tOF1: 0.454545\n",
      "Train Epoch: 16 [768/1083 (71%)]\tLoss: 0.677302 \tOP: 0.611111\tOR: 0.578947\tOF1: 0.594595\n",
      "Train Epoch: 16 [800/1083 (74%)]\tLoss: 0.679641 \tOP: 0.571429\tOR: 0.411765\tOF1: 0.478632\n",
      "Train Epoch: 16 [832/1083 (76%)]\tLoss: 0.678441 \tOP: 0.588235\tOR: 0.428571\tOF1: 0.495868\n",
      "Train Epoch: 16 [864/1083 (79%)]\tLoss: 0.682025 \tOP: 0.511111\tOR: 0.370968\tOF1: 0.429907\n",
      "Train Epoch: 16 [896/1083 (82%)]\tLoss: 0.680738 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 16 [928/1083 (85%)]\tLoss: 0.681152 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 16 [960/1083 (88%)]\tLoss: 0.677629 \tOP: 0.603774\tOR: 0.470588\tOF1: 0.528926\n",
      "Train Epoch: 16 [992/1083 (91%)]\tLoss: 0.681230 \tOP: 0.531915\tOR: 0.357143\tOF1: 0.427350\n",
      "Train Epoch: 16 [1024/1083 (94%)]\tLoss: 0.682407 \tOP: 0.521739\tOR: 0.352941\tOF1: 0.421053\n",
      "Train Epoch: 16 [891/1083 (97%)]\tLoss: 0.682534 \tOP: 0.475000\tOR: 0.322034\tOF1: 0.383838\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6936 \n",
      "OP: 0.312500\n",
      "OR: 0.285714\n",
      "OF1: 0.298507\n",
      "\n",
      "Train Epoch: 17 [0/1083 (0%)]\tLoss: 0.682521 \tOP: 0.522727\tOR: 0.315068\tOF1: 0.393162\n",
      "Train Epoch: 17 [32/1083 (3%)]\tLoss: 0.680128 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 17 [64/1083 (6%)]\tLoss: 0.678896 \tOP: 0.580000\tOR: 0.362500\tOF1: 0.446154\n",
      "Train Epoch: 17 [96/1083 (9%)]\tLoss: 0.683583 \tOP: 0.500000\tOR: 0.318182\tOF1: 0.388889\n",
      "Train Epoch: 17 [128/1083 (12%)]\tLoss: 0.685512 \tOP: 0.475000\tOR: 0.267606\tOF1: 0.342342\n",
      "Train Epoch: 17 [160/1083 (15%)]\tLoss: 0.681137 \tOP: 0.543478\tOR: 0.347222\tOF1: 0.423729\n",
      "Train Epoch: 17 [192/1083 (18%)]\tLoss: 0.677471 \tOP: 0.611111\tOR: 0.452055\tOF1: 0.519685\n",
      "Train Epoch: 17 [224/1083 (21%)]\tLoss: 0.679698 \tOP: 0.560000\tOR: 0.424242\tOF1: 0.482759\n",
      "Train Epoch: 17 [256/1083 (24%)]\tLoss: 0.683883 \tOP: 0.511628\tOR: 0.360656\tOF1: 0.423077\n",
      "Train Epoch: 17 [288/1083 (26%)]\tLoss: 0.677191 \tOP: 0.618182\tOR: 0.453333\tOF1: 0.523077\n",
      "Train Epoch: 17 [320/1083 (29%)]\tLoss: 0.681659 \tOP: 0.521739\tOR: 0.342857\tOF1: 0.413793\n",
      "Train Epoch: 17 [352/1083 (32%)]\tLoss: 0.680421 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 17 [384/1083 (35%)]\tLoss: 0.679259 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 17 [416/1083 (38%)]\tLoss: 0.683930 \tOP: 0.511628\tOR: 0.318841\tOF1: 0.392857\n",
      "Train Epoch: 17 [448/1083 (41%)]\tLoss: 0.681687 \tOP: 0.533333\tOR: 0.393443\tOF1: 0.452830\n",
      "Train Epoch: 17 [480/1083 (44%)]\tLoss: 0.679074 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 17 [512/1083 (47%)]\tLoss: 0.682134 \tOP: 0.531915\tOR: 0.384615\tOF1: 0.446429\n",
      "Train Epoch: 17 [544/1083 (50%)]\tLoss: 0.679947 \tOP: 0.571429\tOR: 0.437500\tOF1: 0.495575\n",
      "Train Epoch: 17 [576/1083 (53%)]\tLoss: 0.680969 \tOP: 0.553191\tOR: 0.400000\tOF1: 0.464286\n",
      "Train Epoch: 17 [608/1083 (56%)]\tLoss: 0.683436 \tOP: 0.487805\tOR: 0.333333\tOF1: 0.396040\n",
      "Train Epoch: 17 [640/1083 (59%)]\tLoss: 0.682705 \tOP: 0.500000\tOR: 0.349206\tOF1: 0.411215\n",
      "Train Epoch: 17 [672/1083 (62%)]\tLoss: 0.680649 \tOP: 0.553191\tOR: 0.376812\tOF1: 0.448276\n",
      "Train Epoch: 17 [704/1083 (65%)]\tLoss: 0.680292 \tOP: 0.551020\tOR: 0.421875\tOF1: 0.477876\n",
      "Train Epoch: 17 [736/1083 (68%)]\tLoss: 0.681226 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 17 [768/1083 (71%)]\tLoss: 0.683691 \tOP: 0.533333\tOR: 0.352941\tOF1: 0.424779\n",
      "Train Epoch: 17 [800/1083 (74%)]\tLoss: 0.680217 \tOP: 0.562500\tOR: 0.380282\tOF1: 0.453782\n",
      "Train Epoch: 17 [832/1083 (76%)]\tLoss: 0.680131 \tOP: 0.580000\tOR: 0.453125\tOF1: 0.508772\n",
      "Train Epoch: 17 [864/1083 (79%)]\tLoss: 0.678538 \tOP: 0.588235\tOR: 0.447761\tOF1: 0.508475\n",
      "Train Epoch: 17 [896/1083 (82%)]\tLoss: 0.679874 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 17 [928/1083 (85%)]\tLoss: 0.683130 \tOP: 0.500000\tOR: 0.318182\tOF1: 0.388889\n",
      "Train Epoch: 17 [960/1083 (88%)]\tLoss: 0.682391 \tOP: 0.522727\tOR: 0.359375\tOF1: 0.425926\n",
      "Train Epoch: 17 [992/1083 (91%)]\tLoss: 0.680690 \tOP: 0.571429\tOR: 0.388889\tOF1: 0.462810\n",
      "Train Epoch: 17 [1024/1083 (94%)]\tLoss: 0.679929 \tOP: 0.571429\tOR: 0.424242\tOF1: 0.486957\n",
      "Train Epoch: 17 [891/1083 (97%)]\tLoss: 0.680913 \tOP: 0.500000\tOR: 0.385965\tOF1: 0.435644\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6935 \n",
      "OP: 0.312500\n",
      "OR: 0.285714\n",
      "OF1: 0.298507\n",
      "\n",
      "Train Epoch: 18 [0/1083 (0%)]\tLoss: 0.679374 \tOP: 0.571429\tOR: 0.430769\tOF1: 0.491228\n",
      "Train Epoch: 18 [32/1083 (3%)]\tLoss: 0.681675 \tOP: 0.533333\tOR: 0.352941\tOF1: 0.424779\n",
      "Train Epoch: 18 [64/1083 (6%)]\tLoss: 0.680336 \tOP: 0.562500\tOR: 0.360000\tOF1: 0.439024\n",
      "Train Epoch: 18 [96/1083 (9%)]\tLoss: 0.680053 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 18 [128/1083 (12%)]\tLoss: 0.682036 \tOP: 0.522727\tOR: 0.333333\tOF1: 0.407080\n",
      "Train Epoch: 18 [160/1083 (15%)]\tLoss: 0.683741 \tOP: 0.488372\tOR: 0.350000\tOF1: 0.407767\n",
      "Train Epoch: 18 [192/1083 (18%)]\tLoss: 0.680561 \tOP: 0.541667\tOR: 0.376812\tOF1: 0.444444\n",
      "Train Epoch: 18 [224/1083 (21%)]\tLoss: 0.678702 \tOP: 0.576923\tOR: 0.405405\tOF1: 0.476190\n",
      "Train Epoch: 18 [256/1083 (24%)]\tLoss: 0.680104 \tOP: 0.571429\tOR: 0.417910\tOF1: 0.482759\n",
      "Train Epoch: 18 [288/1083 (26%)]\tLoss: 0.680127 \tOP: 0.553191\tOR: 0.361111\tOF1: 0.436975\n",
      "Train Epoch: 18 [320/1083 (29%)]\tLoss: 0.680517 \tOP: 0.541667\tOR: 0.426230\tOF1: 0.477064\n",
      "Train Epoch: 18 [352/1083 (32%)]\tLoss: 0.680049 \tOP: 0.551020\tOR: 0.428571\tOF1: 0.482143\n",
      "Train Epoch: 18 [384/1083 (35%)]\tLoss: 0.679216 \tOP: 0.560000\tOR: 0.383562\tOF1: 0.455285\n",
      "Train Epoch: 18 [416/1083 (38%)]\tLoss: 0.680346 \tOP: 0.562500\tOR: 0.402985\tOF1: 0.469565\n",
      "Train Epoch: 18 [448/1083 (41%)]\tLoss: 0.680698 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 18 [480/1083 (44%)]\tLoss: 0.682702 \tOP: 0.522727\tOR: 0.343284\tOF1: 0.414414\n",
      "Train Epoch: 18 [512/1083 (47%)]\tLoss: 0.682494 \tOP: 0.533333\tOR: 0.352941\tOF1: 0.424779\n",
      "Train Epoch: 18 [544/1083 (50%)]\tLoss: 0.680124 \tOP: 0.560000\tOR: 0.459016\tOF1: 0.504505\n",
      "Train Epoch: 18 [576/1083 (53%)]\tLoss: 0.681063 \tOP: 0.562500\tOR: 0.369863\tOF1: 0.446281\n",
      "Train Epoch: 18 [608/1083 (56%)]\tLoss: 0.680562 \tOP: 0.553191\tOR: 0.393939\tOF1: 0.460177\n",
      "Train Epoch: 18 [640/1083 (59%)]\tLoss: 0.678911 \tOP: 0.580000\tOR: 0.460317\tOF1: 0.513274\n",
      "Train Epoch: 18 [672/1083 (62%)]\tLoss: 0.680770 \tOP: 0.541667\tOR: 0.325000\tOF1: 0.406250\n",
      "Train Epoch: 18 [704/1083 (65%)]\tLoss: 0.681207 \tOP: 0.543478\tOR: 0.352113\tOF1: 0.427350\n",
      "Train Epoch: 18 [736/1083 (68%)]\tLoss: 0.680868 \tOP: 0.553191\tOR: 0.342105\tOF1: 0.422764\n",
      "Train Epoch: 18 [768/1083 (71%)]\tLoss: 0.677902 \tOP: 0.603774\tOR: 0.457143\tOF1: 0.520325\n",
      "Train Epoch: 18 [800/1083 (74%)]\tLoss: 0.684432 \tOP: 0.475000\tOR: 0.279412\tOF1: 0.351852\n",
      "Train Epoch: 18 [832/1083 (76%)]\tLoss: 0.682707 \tOP: 0.522727\tOR: 0.343284\tOF1: 0.414414\n",
      "Train Epoch: 18 [864/1083 (79%)]\tLoss: 0.679459 \tOP: 0.580000\tOR: 0.432836\tOF1: 0.495726\n",
      "Train Epoch: 18 [896/1083 (82%)]\tLoss: 0.680815 \tOP: 0.541667\tOR: 0.388060\tOF1: 0.452174\n",
      "Train Epoch: 18 [928/1083 (85%)]\tLoss: 0.682945 \tOP: 0.511111\tOR: 0.348485\tOF1: 0.414414\n",
      "Train Epoch: 18 [960/1083 (88%)]\tLoss: 0.683618 \tOP: 0.500000\tOR: 0.338710\tOF1: 0.403846\n",
      "Train Epoch: 18 [992/1083 (91%)]\tLoss: 0.681540 \tOP: 0.531915\tOR: 0.373134\tOF1: 0.438596\n",
      "Train Epoch: 18 [1024/1083 (94%)]\tLoss: 0.679835 \tOP: 0.568627\tOR: 0.432836\tOF1: 0.491525\n",
      "Train Epoch: 18 [891/1083 (97%)]\tLoss: 0.681425 \tOP: 0.500000\tOR: 0.344262\tOF1: 0.407767\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6930 \n",
      "OP: 0.290323\n",
      "OR: 0.257143\n",
      "OF1: 0.272727\n",
      "\n",
      "Train Epoch: 19 [0/1083 (0%)]\tLoss: 0.679281 \tOP: 0.580000\tOR: 0.446154\tOF1: 0.504348\n",
      "Train Epoch: 19 [32/1083 (3%)]\tLoss: 0.679240 \tOP: 0.580000\tOR: 0.408451\tOF1: 0.479339\n",
      "Train Epoch: 19 [64/1083 (6%)]\tLoss: 0.680537 \tOP: 0.553191\tOR: 0.382353\tOF1: 0.452174\n",
      "Train Epoch: 19 [96/1083 (9%)]\tLoss: 0.680571 \tOP: 0.553191\tOR: 0.342105\tOF1: 0.422764\n",
      "Train Epoch: 19 [128/1083 (12%)]\tLoss: 0.679777 \tOP: 0.562500\tOR: 0.391304\tOF1: 0.461538\n",
      "Train Epoch: 19 [160/1083 (15%)]\tLoss: 0.682959 \tOP: 0.522727\tOR: 0.365079\tOF1: 0.429907\n",
      "Train Epoch: 19 [192/1083 (18%)]\tLoss: 0.680164 \tOP: 0.562500\tOR: 0.385714\tOF1: 0.457627\n",
      "Train Epoch: 19 [224/1083 (21%)]\tLoss: 0.681358 \tOP: 0.543478\tOR: 0.403226\tOF1: 0.462963\n",
      "Train Epoch: 19 [256/1083 (24%)]\tLoss: 0.685343 \tOP: 0.461538\tOR: 0.240000\tOF1: 0.315789\n",
      "Train Epoch: 19 [288/1083 (26%)]\tLoss: 0.680184 \tOP: 0.560000\tOR: 0.437500\tOF1: 0.491228\n",
      "Train Epoch: 19 [320/1083 (29%)]\tLoss: 0.679542 \tOP: 0.571429\tOR: 0.378378\tOF1: 0.455285\n",
      "Train Epoch: 19 [352/1083 (32%)]\tLoss: 0.680531 \tOP: 0.553191\tOR: 0.412698\tOF1: 0.472727\n",
      "Train Epoch: 19 [384/1083 (35%)]\tLoss: 0.680732 \tOP: 0.553191\tOR: 0.356164\tOF1: 0.433333\n",
      "Train Epoch: 19 [416/1083 (38%)]\tLoss: 0.679295 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 19 [448/1083 (41%)]\tLoss: 0.679093 \tOP: 0.580000\tOR: 0.414286\tOF1: 0.483333\n",
      "Train Epoch: 19 [480/1083 (44%)]\tLoss: 0.684302 \tOP: 0.500000\tOR: 0.300000\tOF1: 0.375000\n",
      "Train Epoch: 19 [512/1083 (47%)]\tLoss: 0.681747 \tOP: 0.521739\tOR: 0.380952\tOF1: 0.440367\n",
      "Train Epoch: 19 [544/1083 (50%)]\tLoss: 0.679486 \tOP: 0.568627\tOR: 0.386667\tOF1: 0.460317\n",
      "Train Epoch: 19 [576/1083 (53%)]\tLoss: 0.681099 \tOP: 0.531915\tOR: 0.416667\tOF1: 0.467290\n",
      "Train Epoch: 19 [608/1083 (56%)]\tLoss: 0.681565 \tOP: 0.533333\tOR: 0.387097\tOF1: 0.448598\n",
      "Train Epoch: 19 [640/1083 (59%)]\tLoss: 0.679077 \tOP: 0.580000\tOR: 0.491525\tOF1: 0.532110\n",
      "Train Epoch: 19 [672/1083 (62%)]\tLoss: 0.681155 \tOP: 0.543478\tOR: 0.362319\tOF1: 0.434783\n",
      "Train Epoch: 19 [704/1083 (65%)]\tLoss: 0.679092 \tOP: 0.580000\tOR: 0.402778\tOF1: 0.475410\n",
      "Train Epoch: 19 [736/1083 (68%)]\tLoss: 0.680117 \tOP: 0.562500\tOR: 0.355263\tOF1: 0.435484\n",
      "Train Epoch: 19 [768/1083 (71%)]\tLoss: 0.682245 \tOP: 0.522727\tOR: 0.353846\tOF1: 0.422018\n",
      "Train Epoch: 19 [800/1083 (74%)]\tLoss: 0.680888 \tOP: 0.543478\tOR: 0.390625\tOF1: 0.454545\n",
      "Train Epoch: 19 [832/1083 (76%)]\tLoss: 0.677993 \tOP: 0.603774\tOR: 0.410256\tOF1: 0.488550\n",
      "Train Epoch: 19 [864/1083 (79%)]\tLoss: 0.680790 \tOP: 0.562500\tOR: 0.397059\tOF1: 0.465517\n",
      "Train Epoch: 19 [896/1083 (82%)]\tLoss: 0.679951 \tOP: 0.562500\tOR: 0.442623\tOF1: 0.495413\n",
      "Train Epoch: 19 [928/1083 (85%)]\tLoss: 0.682731 \tOP: 0.511628\tOR: 0.323529\tOF1: 0.396396\n",
      "Train Epoch: 19 [960/1083 (88%)]\tLoss: 0.679507 \tOP: 0.571429\tOR: 0.459016\tOF1: 0.509091\n",
      "Train Epoch: 19 [992/1083 (91%)]\tLoss: 0.679456 \tOP: 0.580000\tOR: 0.376623\tOF1: 0.456693\n",
      "Train Epoch: 19 [1024/1083 (94%)]\tLoss: 0.680933 \tOP: 0.553191\tOR: 0.342105\tOF1: 0.422764\n",
      "Train Epoch: 19 [891/1083 (97%)]\tLoss: 0.681256 \tOP: 0.488372\tOR: 0.428571\tOF1: 0.456522\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.6927 \n",
      "OP: 0.266667\n",
      "OR: 0.228571\n",
      "OF1: 0.246154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model3.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model3(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for i in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for j in range(len(classes)):\n",
    "        #        if target[i][j] == 1 or preds[i][j] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[i][j] == target[i][j]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'voc07')\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model3.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model3(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "        \n",
    "        #acc_list = []\n",
    "        #preds = torch.round(output)\n",
    "        #for n in range(len(preds)):\n",
    "        #    result = 0\n",
    "        #    denom = 0\n",
    "        #    for m in range(len(classes)):\n",
    "        #        if target[n][m] == 1 or preds[n][m] == 1:\n",
    "        #            denom += 1\n",
    "        #            if preds[n][m] == target[n][m]:\n",
    "        #                result+=1\n",
    "        #    acc_list.append(result/denom)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target, 'voc07')\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG (frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_vgg11 = models.vgg11(weights=models.VGG11_Weights.DEFAULT)\n",
    "        \n",
    "#         child_counter = 0\n",
    "#         for child in model_vgg11.children():\n",
    "#             if child_counter < 1:\n",
    "#                 for param in child.parameters():\n",
    "#                     param.requires_grad = False\n",
    "#                     child_counter += 1\n",
    "#             else:\n",
    "#                 child_counter += 1\n",
    "        \n",
    "        self.model_pre = nn.Sequential(*list(model_vgg11.children())[:-1])\n",
    "        fc_feat = model_vgg11.classifier[0].in_features\n",
    "        self.model_post = nn.Sequential(\n",
    "            nn.Linear(fc_feat, 4096, bias = True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(.5, inplace = False),\n",
    "            nn.Linear(4096, 4096, bias = True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(.5, inplace = False),\n",
    "        )\n",
    "        self.fc = nn.Linear(4096, len(classes))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.model_pre(X)\n",
    "        X = torch.flatten(X,1)\n",
    "        X = self.model_post(X)\n",
    "        return self.fc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = MyCNN3()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model4.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-136-447663f93c68>:28: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  OF1 = (2 * OP * OR) / (OP + OR) #OF1 (Overall F1 Score) is the harmonic mean of precision and recall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/947 (0%)]\tLoss: 0.696073 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [32/947 (3%)]\tLoss: 0.572799 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [64/947 (7%)]\tLoss: 0.310805 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [96/947 (10%)]\tLoss: 0.440529 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [128/947 (13%)]\tLoss: 0.360009 \tOP: 0.177778\tOR: 0.115942\tOF1: 0.140351\n",
      "Train Epoch: 0 [160/947 (17%)]\tLoss: 0.274446 \tOP: 0.320000\tOR: 0.231884\tOF1: 0.268908\n",
      "Train Epoch: 0 [192/947 (20%)]\tLoss: 0.291261 \tOP: 0.212766\tOR: 0.131579\tOF1: 0.162602\n",
      "Train Epoch: 0 [224/947 (23%)]\tLoss: 0.290616 \tOP: 0.184211\tOR: 0.109375\tOF1: 0.137255\n",
      "Train Epoch: 0 [256/947 (27%)]\tLoss: 0.305100 \tOP: 0.200000\tOR: 0.098592\tOF1: 0.132075\n",
      "Train Epoch: 0 [288/947 (30%)]\tLoss: 0.294279 \tOP: 0.107143\tOR: 0.042857\tOF1: 0.061224\n",
      "Train Epoch: 0 [320/947 (33%)]\tLoss: 0.284364 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [352/947 (37%)]\tLoss: 0.251522 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [384/947 (40%)]\tLoss: 0.274843 \tOP: 0.040000\tOR: 0.014493\tOF1: 0.021277\n",
      "Train Epoch: 0 [416/947 (43%)]\tLoss: 0.297612 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [448/947 (47%)]\tLoss: 0.273511 \tOP: 0.080000\tOR: 0.029851\tOF1: 0.043478\n",
      "Train Epoch: 0 [480/947 (50%)]\tLoss: 0.245963 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 0 [512/947 (53%)]\tLoss: 0.263090 \tOP: 0.041667\tOR: 0.014925\tOF1: 0.021978\n",
      "Train Epoch: 0 [544/947 (57%)]\tLoss: 0.288959 \tOP: 0.103448\tOR: 0.041667\tOF1: 0.059406\n",
      "Train Epoch: 0 [576/947 (60%)]\tLoss: 0.287102 \tOP: 0.151515\tOR: 0.068493\tOF1: 0.094340\n",
      "Train Epoch: 0 [608/947 (63%)]\tLoss: 0.281132 \tOP: 0.205882\tOR: 0.097222\tOF1: 0.132075\n",
      "Train Epoch: 0 [640/947 (67%)]\tLoss: 0.250114 \tOP: 0.074074\tOR: 0.034483\tOF1: 0.047059\n",
      "Train Epoch: 0 [672/947 (70%)]\tLoss: 0.238387 \tOP: 0.040000\tOR: 0.015385\tOF1: 0.022222\n",
      "Train Epoch: 0 [704/947 (73%)]\tLoss: 0.267251 \tOP: 0.111111\tOR: 0.043478\tOF1: 0.062500\n",
      "Train Epoch: 0 [736/947 (77%)]\tLoss: 0.276157 \tOP: 0.068966\tOR: 0.026667\tOF1: 0.038462\n",
      "Train Epoch: 0 [768/947 (80%)]\tLoss: 0.202602 \tOP: 0.133333\tOR: 0.068966\tOF1: 0.090909\n",
      "Train Epoch: 0 [800/947 (83%)]\tLoss: 0.253396 \tOP: 0.040000\tOR: 0.015873\tOF1: 0.022727\n",
      "Train Epoch: 0 [832/947 (87%)]\tLoss: 0.242258 \tOP: 0.107143\tOR: 0.045455\tOF1: 0.063830\n",
      "Train Epoch: 0 [864/947 (90%)]\tLoss: 0.244530 \tOP: 0.100000\tOR: 0.048387\tOF1: 0.065217\n",
      "Train Epoch: 0 [896/947 (93%)]\tLoss: 0.240731 \tOP: 0.115385\tOR: 0.044776\tOF1: 0.064516\n",
      "Train Epoch: 0 [551/947 (97%)]\tLoss: 0.268207 \tOP: 0.041667\tOR: 0.021277\tOF1: 0.028169\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-139-f62952028cc1>:33: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2483 \n",
      "OP: 0.000000\n",
      "OR: 0.000000\n",
      "OF1: nan\n",
      "\n",
      "Train Epoch: 1 [0/947 (0%)]\tLoss: 0.234743 \tOP: 0.041667\tOR: 0.015152\tOF1: 0.022222\n",
      "Train Epoch: 1 [32/947 (3%)]\tLoss: 0.250603 \tOP: 0.181818\tOR: 0.086957\tOF1: 0.117647\n",
      "Train Epoch: 1 [64/947 (7%)]\tLoss: 0.234620 \tOP: 0.107143\tOR: 0.044776\tOF1: 0.063158\n",
      "Train Epoch: 1 [96/947 (10%)]\tLoss: 0.248759 \tOP: 0.193548\tOR: 0.088235\tOF1: 0.121212\n",
      "Train Epoch: 1 [128/947 (13%)]\tLoss: 0.227804 \tOP: 0.233333\tOR: 0.098592\tOF1: 0.138614\n",
      "Train Epoch: 1 [160/947 (17%)]\tLoss: 0.250387 \tOP: 0.062500\tOR: 0.029412\tOF1: 0.040000\n",
      "Train Epoch: 1 [192/947 (20%)]\tLoss: 0.219398 \tOP: 0.107143\tOR: 0.048387\tOF1: 0.066667\n",
      "Train Epoch: 1 [224/947 (23%)]\tLoss: 0.226228 \tOP: 0.137931\tOR: 0.059701\tOF1: 0.083333\n",
      "Train Epoch: 1 [256/947 (27%)]\tLoss: 0.269625 \tOP: 0.156250\tOR: 0.067568\tOF1: 0.094340\n",
      "Train Epoch: 1 [288/947 (30%)]\tLoss: 0.243574 \tOP: 0.193548\tOR: 0.086957\tOF1: 0.120000\n",
      "Train Epoch: 1 [320/947 (33%)]\tLoss: 0.219621 \tOP: 0.153846\tOR: 0.063492\tOF1: 0.089888\n",
      "Train Epoch: 1 [352/947 (37%)]\tLoss: 0.218134 \tOP: 0.187500\tOR: 0.088235\tOF1: 0.120000\n",
      "Train Epoch: 1 [384/947 (40%)]\tLoss: 0.269264 \tOP: 0.040000\tOR: 0.015152\tOF1: 0.021978\n",
      "Train Epoch: 1 [416/947 (43%)]\tLoss: 0.224209 \tOP: 0.000000\tOR: 0.000000\tOF1: nan\n",
      "Train Epoch: 1 [448/947 (47%)]\tLoss: 0.223811 \tOP: 0.115385\tOR: 0.046154\tOF1: 0.065934\n",
      "Train Epoch: 1 [480/947 (50%)]\tLoss: 0.250049 \tOP: 0.111111\tOR: 0.046154\tOF1: 0.065217\n",
      "Train Epoch: 1 [512/947 (53%)]\tLoss: 0.240629 \tOP: 0.153846\tOR: 0.058824\tOF1: 0.085106\n",
      "Train Epoch: 1 [544/947 (57%)]\tLoss: 0.246982 \tOP: 0.076923\tOR: 0.029412\tOF1: 0.042553\n",
      "Train Epoch: 1 [576/947 (60%)]\tLoss: 0.257021 \tOP: 0.037037\tOR: 0.014286\tOF1: 0.020619\n",
      "Train Epoch: 1 [608/947 (63%)]\tLoss: 0.257757 \tOP: 0.161290\tOR: 0.070423\tOF1: 0.098039\n",
      "Train Epoch: 1 [640/947 (67%)]\tLoss: 0.249422 \tOP: 0.218750\tOR: 0.097222\tOF1: 0.134615\n",
      "Train Epoch: 1 [672/947 (70%)]\tLoss: 0.219977 \tOP: 0.225806\tOR: 0.107692\tOF1: 0.145833\n",
      "Train Epoch: 1 [704/947 (73%)]\tLoss: 0.246935 \tOP: 0.235294\tOR: 0.108108\tOF1: 0.148148\n",
      "Train Epoch: 1 [736/947 (77%)]\tLoss: 0.219986 \tOP: 0.257143\tOR: 0.140625\tOF1: 0.181818\n",
      "Train Epoch: 1 [768/947 (80%)]\tLoss: 0.223349 \tOP: 0.333333\tOR: 0.171429\tOF1: 0.226415\n",
      "Train Epoch: 1 [800/947 (83%)]\tLoss: 0.275719 \tOP: 0.194444\tOR: 0.100000\tOF1: 0.132075\n",
      "Train Epoch: 1 [832/947 (87%)]\tLoss: 0.224550 \tOP: 0.307692\tOR: 0.171429\tOF1: 0.220183\n",
      "Train Epoch: 1 [864/947 (90%)]\tLoss: 0.245866 \tOP: 0.210526\tOR: 0.133333\tOF1: 0.163265\n",
      "Train Epoch: 1 [896/947 (93%)]\tLoss: 0.246153 \tOP: 0.235294\tOR: 0.111111\tOF1: 0.150943\n",
      "Train Epoch: 1 [551/947 (97%)]\tLoss: 0.237799 \tOP: 0.115385\tOR: 0.068182\tOF1: 0.085714\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2434 \n",
      "OP: 0.193548\n",
      "OR: 0.113208\n",
      "OF1: 0.142857\n",
      "\n",
      "Train Epoch: 2 [0/947 (0%)]\tLoss: 0.175217 \tOP: 0.361111\tOR: 0.209677\tOF1: 0.265306\n",
      "Train Epoch: 2 [32/947 (3%)]\tLoss: 0.249214 \tOP: 0.277778\tOR: 0.135135\tOF1: 0.181818\n",
      "Train Epoch: 2 [64/947 (7%)]\tLoss: 0.208371 \tOP: 0.277778\tOR: 0.147059\tOF1: 0.192308\n",
      "Train Epoch: 2 [96/947 (10%)]\tLoss: 0.220379 \tOP: 0.222222\tOR: 0.121212\tOF1: 0.156863\n",
      "Train Epoch: 2 [128/947 (13%)]\tLoss: 0.237829 \tOP: 0.218750\tOR: 0.101449\tOF1: 0.138614\n",
      "Train Epoch: 2 [160/947 (17%)]\tLoss: 0.221330 \tOP: 0.218750\tOR: 0.106061\tOF1: 0.142857\n",
      "Train Epoch: 2 [192/947 (20%)]\tLoss: 0.243518 \tOP: 0.264706\tOR: 0.121622\tOF1: 0.166667\n",
      "Train Epoch: 2 [224/947 (23%)]\tLoss: 0.204195 \tOP: 0.314286\tOR: 0.169231\tOF1: 0.220000\n",
      "Train Epoch: 2 [256/947 (27%)]\tLoss: 0.257586 \tOP: 0.314286\tOR: 0.139241\tOF1: 0.192982\n",
      "Train Epoch: 2 [288/947 (30%)]\tLoss: 0.226401 \tOP: 0.257143\tOR: 0.132353\tOF1: 0.174757\n",
      "Train Epoch: 2 [320/947 (33%)]\tLoss: 0.248152 \tOP: 0.324324\tOR: 0.160000\tOF1: 0.214286\n",
      "Train Epoch: 2 [352/947 (37%)]\tLoss: 0.202269 \tOP: 0.342857\tOR: 0.190476\tOF1: 0.244898\n",
      "Train Epoch: 2 [384/947 (40%)]\tLoss: 0.196307 \tOP: 0.212121\tOR: 0.118644\tOF1: 0.152174\n",
      "Train Epoch: 2 [416/947 (43%)]\tLoss: 0.235959 \tOP: 0.264706\tOR: 0.128571\tOF1: 0.173077\n",
      "Train Epoch: 2 [448/947 (47%)]\tLoss: 0.217382 \tOP: 0.161290\tOR: 0.072464\tOF1: 0.100000\n",
      "Train Epoch: 2 [480/947 (50%)]\tLoss: 0.200185 \tOP: 0.233333\tOR: 0.114754\tOF1: 0.153846\n",
      "Train Epoch: 2 [512/947 (53%)]\tLoss: 0.218997 \tOP: 0.294118\tOR: 0.142857\tOF1: 0.192308\n",
      "Train Epoch: 2 [544/947 (57%)]\tLoss: 0.211516 \tOP: 0.263158\tOR: 0.142857\tOF1: 0.185185\n",
      "Train Epoch: 2 [576/947 (60%)]\tLoss: 0.211823 \tOP: 0.325000\tOR: 0.196970\tOF1: 0.245283\n",
      "Train Epoch: 2 [608/947 (63%)]\tLoss: 0.200280 \tOP: 0.317073\tOR: 0.224138\tOF1: 0.262626\n",
      "Train Epoch: 2 [640/947 (67%)]\tLoss: 0.223913 \tOP: 0.250000\tOR: 0.151515\tOF1: 0.188679\n",
      "Train Epoch: 2 [672/947 (70%)]\tLoss: 0.218254 \tOP: 0.361111\tOR: 0.185714\tOF1: 0.245283\n",
      "Train Epoch: 2 [704/947 (73%)]\tLoss: 0.213928 \tOP: 0.324324\tOR: 0.179104\tOF1: 0.230769\n",
      "Train Epoch: 2 [736/947 (77%)]\tLoss: 0.235554 \tOP: 0.263158\tOR: 0.140845\tOF1: 0.183486\n",
      "Train Epoch: 2 [768/947 (80%)]\tLoss: 0.233564 \tOP: 0.250000\tOR: 0.128571\tOF1: 0.169811\n",
      "Train Epoch: 2 [800/947 (83%)]\tLoss: 0.205346 \tOP: 0.342105\tOR: 0.200000\tOF1: 0.252427\n",
      "Train Epoch: 2 [832/947 (87%)]\tLoss: 0.206429 \tOP: 0.242424\tOR: 0.117647\tOF1: 0.158416\n",
      "Train Epoch: 2 [864/947 (90%)]\tLoss: 0.212161 \tOP: 0.218750\tOR: 0.102941\tOF1: 0.140000\n",
      "Train Epoch: 2 [896/947 (93%)]\tLoss: 0.221043 \tOP: 0.241379\tOR: 0.098592\tOF1: 0.140000\n",
      "Train Epoch: 2 [551/947 (97%)]\tLoss: 0.210457 \tOP: 0.222222\tOR: 0.146341\tOF1: 0.176471\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2409 \n",
      "OP: 0.166667\n",
      "OR: 0.094340\n",
      "OF1: 0.120482\n",
      "\n",
      "Train Epoch: 3 [0/947 (0%)]\tLoss: 0.218408 \tOP: 0.120000\tOR: 0.042254\tOF1: 0.062500\n",
      "Train Epoch: 3 [32/947 (3%)]\tLoss: 0.194328 \tOP: 0.405405\tOR: 0.214286\tOF1: 0.280374\n",
      "Train Epoch: 3 [64/947 (7%)]\tLoss: 0.170376 \tOP: 0.351351\tOR: 0.194030\tOF1: 0.250000\n",
      "Train Epoch: 3 [96/947 (10%)]\tLoss: 0.198456 \tOP: 0.414634\tOR: 0.246377\tOF1: 0.309091\n",
      "Train Epoch: 3 [128/947 (13%)]\tLoss: 0.145192 \tOP: 0.565217\tOR: 0.400000\tOF1: 0.468468\n",
      "Train Epoch: 3 [160/947 (17%)]\tLoss: 0.195204 \tOP: 0.400000\tOR: 0.213333\tOF1: 0.278261\n",
      "Train Epoch: 3 [192/947 (20%)]\tLoss: 0.178865 \tOP: 0.446809\tOR: 0.308824\tOF1: 0.365217\n",
      "Train Epoch: 3 [224/947 (23%)]\tLoss: 0.183814 \tOP: 0.428571\tOR: 0.285714\tOF1: 0.342857\n",
      "Train Epoch: 3 [256/947 (27%)]\tLoss: 0.185434 \tOP: 0.416667\tOR: 0.220588\tOF1: 0.288462\n",
      "Train Epoch: 3 [288/947 (30%)]\tLoss: 0.155881 \tOP: 0.476190\tOR: 0.303030\tOF1: 0.370370\n",
      "Train Epoch: 3 [320/947 (33%)]\tLoss: 0.199447 \tOP: 0.388889\tOR: 0.191781\tOF1: 0.256881\n",
      "Train Epoch: 3 [352/947 (37%)]\tLoss: 0.178762 \tOP: 0.410256\tOR: 0.235294\tOF1: 0.299065\n",
      "Train Epoch: 3 [384/947 (40%)]\tLoss: 0.171941 \tOP: 0.447368\tOR: 0.265625\tOF1: 0.333333\n",
      "Train Epoch: 3 [416/947 (43%)]\tLoss: 0.178506 \tOP: 0.432432\tOR: 0.228571\tOF1: 0.299065\n",
      "Train Epoch: 3 [448/947 (47%)]\tLoss: 0.174514 \tOP: 0.487805\tOR: 0.289855\tOF1: 0.363636\n",
      "Train Epoch: 3 [480/947 (50%)]\tLoss: 0.170506 \tOP: 0.447368\tOR: 0.250000\tOF1: 0.320755\n",
      "Train Epoch: 3 [512/947 (53%)]\tLoss: 0.163713 \tOP: 0.512195\tOR: 0.313433\tOF1: 0.388889\n",
      "Train Epoch: 3 [544/947 (57%)]\tLoss: 0.188721 \tOP: 0.425000\tOR: 0.250000\tOF1: 0.314815\n",
      "Train Epoch: 3 [576/947 (60%)]\tLoss: 0.179406 \tOP: 0.405405\tOR: 0.220588\tOF1: 0.285714\n",
      "Train Epoch: 3 [608/947 (63%)]\tLoss: 0.184345 \tOP: 0.421053\tOR: 0.228571\tOF1: 0.296296\n",
      "Train Epoch: 3 [640/947 (67%)]\tLoss: 0.177896 \tOP: 0.461538\tOR: 0.246575\tOF1: 0.321429\n",
      "Train Epoch: 3 [672/947 (70%)]\tLoss: 0.193527 \tOP: 0.473684\tOR: 0.246575\tOF1: 0.324324\n",
      "Train Epoch: 3 [704/947 (73%)]\tLoss: 0.175035 \tOP: 0.422222\tOR: 0.292308\tOF1: 0.345455\n",
      "Train Epoch: 3 [736/947 (77%)]\tLoss: 0.165190 \tOP: 0.420000\tOR: 0.362069\tOF1: 0.388889\n",
      "Train Epoch: 3 [768/947 (80%)]\tLoss: 0.148482 \tOP: 0.400000\tOR: 0.300000\tOF1: 0.342857\n",
      "Train Epoch: 3 [800/947 (83%)]\tLoss: 0.158685 \tOP: 0.488372\tOR: 0.313433\tOF1: 0.381818\n",
      "Train Epoch: 3 [832/947 (87%)]\tLoss: 0.134065 \tOP: 0.500000\tOR: 0.338983\tOF1: 0.404040\n",
      "Train Epoch: 3 [864/947 (90%)]\tLoss: 0.227105 \tOP: 0.447368\tOR: 0.226667\tOF1: 0.300885\n",
      "Train Epoch: 3 [896/947 (93%)]\tLoss: 0.167248 \tOP: 0.555556\tOR: 0.357143\tOF1: 0.434783\n",
      "Train Epoch: 3 [551/947 (97%)]\tLoss: 0.185576 \tOP: 0.290323\tOR: 0.214286\tOF1: 0.246575\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.2567 \n",
      "OP: 0.217391\n",
      "OR: 0.188679\n",
      "OF1: 0.202020\n",
      "\n",
      "Train Epoch: 4 [0/947 (0%)]\tLoss: 0.150989 \tOP: 0.500000\tOR: 0.378788\tOF1: 0.431034\n",
      "Train Epoch: 4 [32/947 (3%)]\tLoss: 0.133408 \tOP: 0.640000\tOR: 0.421053\tOF1: 0.507937\n",
      "Train Epoch: 4 [64/947 (7%)]\tLoss: 0.129559 \tOP: 0.500000\tOR: 0.333333\tOF1: 0.400000\n",
      "Train Epoch: 4 [96/947 (10%)]\tLoss: 0.105407 \tOP: 0.687500\tOR: 0.532258\tOF1: 0.600000\n",
      "Train Epoch: 4 [128/947 (13%)]\tLoss: 0.141859 \tOP: 0.536585\tOR: 0.301370\tOF1: 0.385965\n",
      "Train Epoch: 4 [160/947 (17%)]\tLoss: 0.106031 \tOP: 0.645833\tOR: 0.508197\tOF1: 0.568807\n",
      "Train Epoch: 4 [192/947 (20%)]\tLoss: 0.110139 \tOP: 0.630435\tOR: 0.420290\tOF1: 0.504348\n",
      "Train Epoch: 4 [224/947 (23%)]\tLoss: 0.124493 \tOP: 0.627451\tOR: 0.450704\tOF1: 0.524590\n",
      "Train Epoch: 4 [256/947 (27%)]\tLoss: 0.122289 \tOP: 0.750000\tOR: 0.592105\tOF1: 0.661765\n",
      "Train Epoch: 4 [288/947 (30%)]\tLoss: 0.149401 \tOP: 0.684211\tOR: 0.557143\tOF1: 0.614173\n",
      "Train Epoch: 4 [320/947 (33%)]\tLoss: 0.112083 \tOP: 0.714286\tOR: 0.597015\tOF1: 0.650407\n",
      "Train Epoch: 4 [352/947 (37%)]\tLoss: 0.129016 \tOP: 0.684211\tOR: 0.534247\tOF1: 0.600000\n",
      "Train Epoch: 4 [384/947 (40%)]\tLoss: 0.112393 \tOP: 0.678571\tOR: 0.542857\tOF1: 0.603175\n",
      "Train Epoch: 4 [416/947 (43%)]\tLoss: 0.139539 \tOP: 0.566038\tOR: 0.441176\tOF1: 0.495868\n",
      "Train Epoch: 4 [448/947 (47%)]\tLoss: 0.101226 \tOP: 0.618182\tOR: 0.539683\tOF1: 0.576271\n",
      "Train Epoch: 4 [480/947 (50%)]\tLoss: 0.113628 \tOP: 0.600000\tOR: 0.492537\tOF1: 0.540984\n",
      "Train Epoch: 4 [512/947 (53%)]\tLoss: 0.102132 \tOP: 0.727273\tOR: 0.571429\tOF1: 0.640000\n",
      "Train Epoch: 4 [544/947 (57%)]\tLoss: 0.108375 \tOP: 0.612245\tOR: 0.476190\tOF1: 0.535714\n",
      "Train Epoch: 4 [576/947 (60%)]\tLoss: 0.115771 \tOP: 0.600000\tOR: 0.409091\tOF1: 0.486486\n",
      "Train Epoch: 4 [608/947 (63%)]\tLoss: 0.084122 \tOP: 0.758621\tOR: 0.666667\tOF1: 0.709677\n",
      "Train Epoch: 4 [640/947 (67%)]\tLoss: 0.095894 \tOP: 0.678571\tOR: 0.603175\tOF1: 0.638655\n",
      "Train Epoch: 4 [672/947 (70%)]\tLoss: 0.105173 \tOP: 0.615385\tOR: 0.542373\tOF1: 0.576577\n",
      "Train Epoch: 4 [704/947 (73%)]\tLoss: 0.115078 \tOP: 0.604651\tOR: 0.382353\tOF1: 0.468468\n",
      "Train Epoch: 4 [736/947 (77%)]\tLoss: 0.089414 \tOP: 0.764706\tOR: 0.582090\tOF1: 0.661017\n",
      "Train Epoch: 4 [768/947 (80%)]\tLoss: 0.097410 \tOP: 0.693878\tOR: 0.523077\tOF1: 0.596491\n",
      "Train Epoch: 4 [800/947 (83%)]\tLoss: 0.084042 \tOP: 0.727273\tOR: 0.606061\tOF1: 0.661157\n",
      "Train Epoch: 4 [832/947 (87%)]\tLoss: 0.110736 \tOP: 0.637931\tOR: 0.506849\tOF1: 0.564885\n",
      "Train Epoch: 4 [864/947 (90%)]\tLoss: 0.108535 \tOP: 0.646154\tOR: 0.617647\tOF1: 0.631579\n",
      "Train Epoch: 4 [896/947 (93%)]\tLoss: 0.122131 \tOP: 0.650000\tOR: 0.557143\tOF1: 0.600000\n",
      "Train Epoch: 4 [551/947 (97%)]\tLoss: 0.095806 \tOP: 0.627907\tOR: 0.613636\tOF1: 0.620690\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.3038 \n",
      "OP: 0.250000\n",
      "OR: 0.188679\n",
      "OF1: 0.215054\n",
      "\n",
      "Train Epoch: 5 [0/947 (0%)]\tLoss: 0.058628 \tOP: 0.849315\tOR: 0.805195\tOF1: 0.826667\n",
      "Train Epoch: 5 [32/947 (3%)]\tLoss: 0.081798 \tOP: 0.737705\tOR: 0.671642\tOF1: 0.703125\n",
      "Train Epoch: 5 [64/947 (7%)]\tLoss: 0.047776 \tOP: 0.791045\tOR: 0.803030\tOF1: 0.796992\n",
      "Train Epoch: 5 [96/947 (10%)]\tLoss: 0.064989 \tOP: 0.823529\tOR: 0.746667\tOF1: 0.783217\n",
      "Train Epoch: 5 [128/947 (13%)]\tLoss: 0.058649 \tOP: 0.784615\tOR: 0.739130\tOF1: 0.761194\n",
      "Train Epoch: 5 [160/947 (17%)]\tLoss: 0.073532 \tOP: 0.806452\tOR: 0.694444\tOF1: 0.746269\n",
      "Train Epoch: 5 [192/947 (20%)]\tLoss: 0.056783 \tOP: 0.838710\tOR: 0.722222\tOF1: 0.776119\n",
      "Train Epoch: 5 [224/947 (23%)]\tLoss: 0.038563 \tOP: 0.800000\tOR: 0.838710\tOF1: 0.818898\n",
      "Train Epoch: 5 [256/947 (27%)]\tLoss: 0.062044 \tOP: 0.772727\tOR: 0.739130\tOF1: 0.755556\n",
      "Train Epoch: 5 [288/947 (30%)]\tLoss: 0.058109 \tOP: 0.772727\tOR: 0.796875\tOF1: 0.784615\n",
      "Train Epoch: 5 [320/947 (33%)]\tLoss: 0.042563 \tOP: 0.781250\tOR: 0.746269\tOF1: 0.763359\n",
      "Train Epoch: 5 [352/947 (37%)]\tLoss: 0.049066 \tOP: 0.868852\tOR: 0.746479\tOF1: 0.803030\n",
      "Train Epoch: 5 [384/947 (40%)]\tLoss: 0.051420 \tOP: 0.852459\tOR: 0.732394\tOF1: 0.787879\n",
      "Train Epoch: 5 [416/947 (43%)]\tLoss: 0.067272 \tOP: 0.746032\tOR: 0.770492\tOF1: 0.758065\n",
      "Train Epoch: 5 [448/947 (47%)]\tLoss: 0.067554 \tOP: 0.762712\tOR: 0.714286\tOF1: 0.737705\n",
      "Train Epoch: 5 [480/947 (50%)]\tLoss: 0.056637 \tOP: 0.790323\tOR: 0.777778\tOF1: 0.784000\n",
      "Train Epoch: 5 [512/947 (53%)]\tLoss: 0.068744 \tOP: 0.803030\tOR: 0.768116\tOF1: 0.785185\n",
      "Train Epoch: 5 [544/947 (57%)]\tLoss: 0.053228 \tOP: 0.796875\tOR: 0.796875\tOF1: 0.796875\n",
      "Train Epoch: 5 [576/947 (60%)]\tLoss: 0.061021 \tOP: 0.779661\tOR: 0.741935\tOF1: 0.760331\n",
      "Train Epoch: 5 [608/947 (63%)]\tLoss: 0.062886 \tOP: 0.794118\tOR: 0.771429\tOF1: 0.782609\n",
      "Train Epoch: 5 [640/947 (67%)]\tLoss: 0.051250 \tOP: 0.805970\tOR: 0.782609\tOF1: 0.794118\n",
      "Train Epoch: 5 [672/947 (70%)]\tLoss: 0.059844 \tOP: 0.812500\tOR: 0.712329\tOF1: 0.759124\n",
      "Train Epoch: 5 [704/947 (73%)]\tLoss: 0.047853 \tOP: 0.815385\tOR: 0.815385\tOF1: 0.815385\n",
      "Train Epoch: 5 [736/947 (77%)]\tLoss: 0.076397 \tOP: 0.774194\tOR: 0.676056\tOF1: 0.721805\n",
      "Train Epoch: 5 [768/947 (80%)]\tLoss: 0.059487 \tOP: 0.800000\tOR: 0.666667\tOF1: 0.727273\n",
      "Train Epoch: 5 [800/947 (83%)]\tLoss: 0.053987 \tOP: 0.805970\tOR: 0.805970\tOF1: 0.805970\n",
      "Train Epoch: 5 [832/947 (87%)]\tLoss: 0.049550 \tOP: 0.821429\tOR: 0.766667\tOF1: 0.793103\n",
      "Train Epoch: 5 [864/947 (90%)]\tLoss: 0.064293 \tOP: 0.760563\tOR: 0.729730\tOF1: 0.744828\n",
      "Train Epoch: 5 [896/947 (93%)]\tLoss: 0.102601 \tOP: 0.750000\tOR: 0.552632\tOF1: 0.636364\n",
      "Train Epoch: 5 [551/947 (97%)]\tLoss: 0.048283 \tOP: 0.619048\tOR: 0.764706\tOF1: 0.684211\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.3559 \n",
      "OP: 0.244444\n",
      "OR: 0.207547\n",
      "OF1: 0.224490\n",
      "\n",
      "Train Epoch: 6 [0/947 (0%)]\tLoss: 0.039021 \tOP: 0.885714\tOR: 0.873239\tOF1: 0.879433\n",
      "Train Epoch: 6 [32/947 (3%)]\tLoss: 0.038993 \tOP: 0.898551\tOR: 0.837838\tOF1: 0.867133\n",
      "Train Epoch: 6 [64/947 (7%)]\tLoss: 0.026005 \tOP: 0.852941\tOR: 0.865672\tOF1: 0.859259\n",
      "Train Epoch: 6 [96/947 (10%)]\tLoss: 0.028402 \tOP: 0.870968\tOR: 0.843750\tOF1: 0.857143\n",
      "Train Epoch: 6 [128/947 (13%)]\tLoss: 0.018600 \tOP: 0.811594\tOR: 0.918033\tOF1: 0.861538\n",
      "Train Epoch: 6 [160/947 (17%)]\tLoss: 0.036874 \tOP: 0.808219\tOR: 0.855072\tOF1: 0.830986\n",
      "Train Epoch: 6 [192/947 (20%)]\tLoss: 0.073805 \tOP: 0.796875\tOR: 0.728571\tOF1: 0.761194\n",
      "Train Epoch: 6 [224/947 (23%)]\tLoss: 0.037591 \tOP: 0.904762\tOR: 0.780822\tOF1: 0.838235\n",
      "Train Epoch: 6 [256/947 (27%)]\tLoss: 0.021355 \tOP: 0.852941\tOR: 0.892308\tOF1: 0.872180\n",
      "Train Epoch: 6 [288/947 (30%)]\tLoss: 0.039261 \tOP: 0.885246\tOR: 0.818182\tOF1: 0.850394\n",
      "Train Epoch: 6 [320/947 (33%)]\tLoss: 0.040427 \tOP: 0.880597\tOR: 0.855072\tOF1: 0.867647\n",
      "Train Epoch: 6 [352/947 (37%)]\tLoss: 0.054029 \tOP: 0.777778\tOR: 0.736842\tOF1: 0.756757\n",
      "Train Epoch: 6 [384/947 (40%)]\tLoss: 0.043648 \tOP: 0.882353\tOR: 0.869565\tOF1: 0.875912\n",
      "Train Epoch: 6 [416/947 (43%)]\tLoss: 0.031025 \tOP: 0.876923\tOR: 0.850746\tOF1: 0.863636\n",
      "Train Epoch: 6 [448/947 (47%)]\tLoss: 0.032926 \tOP: 0.859155\tOR: 0.910448\tOF1: 0.884058\n",
      "Train Epoch: 6 [480/947 (50%)]\tLoss: 0.025490 \tOP: 0.846154\tOR: 0.942857\tOF1: 0.891892\n",
      "Train Epoch: 6 [512/947 (53%)]\tLoss: 0.050542 \tOP: 0.848485\tOR: 0.823529\tOF1: 0.835821\n",
      "Train Epoch: 6 [544/947 (57%)]\tLoss: 0.043739 \tOP: 0.821918\tOR: 0.845070\tOF1: 0.833333\n",
      "Train Epoch: 6 [576/947 (60%)]\tLoss: 0.029383 \tOP: 0.845070\tOR: 0.895522\tOF1: 0.869565\n",
      "Train Epoch: 6 [608/947 (63%)]\tLoss: 0.042045 \tOP: 0.885714\tOR: 0.861111\tOF1: 0.873239\n",
      "Train Epoch: 6 [640/947 (67%)]\tLoss: 0.042250 \tOP: 0.876712\tOR: 0.842105\tOF1: 0.859060\n",
      "Train Epoch: 6 [672/947 (70%)]\tLoss: 0.036859 \tOP: 0.859155\tOR: 0.897059\tOF1: 0.877698\n",
      "Train Epoch: 6 [704/947 (73%)]\tLoss: 0.023259 \tOP: 0.861111\tOR: 0.925373\tOF1: 0.892086\n",
      "Train Epoch: 6 [736/947 (77%)]\tLoss: 0.053827 \tOP: 0.794118\tOR: 0.750000\tOF1: 0.771429\n",
      "Train Epoch: 6 [768/947 (80%)]\tLoss: 0.059172 \tOP: 0.833333\tOR: 0.800000\tOF1: 0.816327\n",
      "Train Epoch: 6 [800/947 (83%)]\tLoss: 0.025440 \tOP: 0.826087\tOR: 0.934426\tOF1: 0.876923\n",
      "Train Epoch: 6 [832/947 (87%)]\tLoss: 0.026714 \tOP: 0.863636\tOR: 0.904762\tOF1: 0.883721\n",
      "Train Epoch: 6 [864/947 (90%)]\tLoss: 0.039755 \tOP: 0.855072\tOR: 0.880597\tOF1: 0.867647\n",
      "Train Epoch: 6 [896/947 (93%)]\tLoss: 0.039699 \tOP: 0.797101\tOR: 0.887097\tOF1: 0.839695\n",
      "Train Epoch: 6 [551/947 (97%)]\tLoss: 0.036997 \tOP: 0.740000\tOR: 0.902439\tOF1: 0.813187\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.3785 \n",
      "OP: 0.305556\n",
      "OR: 0.207547\n",
      "OF1: 0.247191\n",
      "\n",
      "Train Epoch: 7 [0/947 (0%)]\tLoss: 0.026144 \tOP: 0.884058\tOR: 0.897059\tOF1: 0.890511\n",
      "Train Epoch: 7 [32/947 (3%)]\tLoss: 0.028337 \tOP: 0.873016\tOR: 0.887097\tOF1: 0.880000\n",
      "Train Epoch: 7 [64/947 (7%)]\tLoss: 0.033143 \tOP: 0.868421\tOR: 0.929577\tOF1: 0.897959\n",
      "Train Epoch: 7 [96/947 (10%)]\tLoss: 0.024483 \tOP: 0.866667\tOR: 0.915493\tOF1: 0.890411\n",
      "Train Epoch: 7 [128/947 (13%)]\tLoss: 0.023984 \tOP: 0.925373\tOR: 0.911765\tOF1: 0.918519\n",
      "Train Epoch: 7 [160/947 (17%)]\tLoss: 0.024189 \tOP: 0.842105\tOR: 0.927536\tOF1: 0.882759\n",
      "Train Epoch: 7 [192/947 (20%)]\tLoss: 0.013390 \tOP: 0.893939\tOR: 0.936508\tOF1: 0.914729\n",
      "Train Epoch: 7 [224/947 (23%)]\tLoss: 0.023524 \tOP: 0.869565\tOR: 0.869565\tOF1: 0.869565\n",
      "Train Epoch: 7 [256/947 (27%)]\tLoss: 0.035380 \tOP: 0.880597\tOR: 0.880597\tOF1: 0.880597\n",
      "Train Epoch: 7 [288/947 (30%)]\tLoss: 0.027603 \tOP: 0.906250\tOR: 0.878788\tOF1: 0.892308\n",
      "Train Epoch: 7 [320/947 (33%)]\tLoss: 0.031864 \tOP: 0.819672\tOR: 0.862069\tOF1: 0.840336\n",
      "Train Epoch: 7 [352/947 (37%)]\tLoss: 0.023504 \tOP: 0.941176\tOR: 0.901408\tOF1: 0.920863\n",
      "Train Epoch: 7 [384/947 (40%)]\tLoss: 0.018433 \tOP: 0.830986\tOR: 0.907692\tOF1: 0.867647\n",
      "Train Epoch: 7 [416/947 (43%)]\tLoss: 0.011969 \tOP: 0.833333\tOR: 0.967742\tOF1: 0.895522\n",
      "Train Epoch: 7 [448/947 (47%)]\tLoss: 0.017355 \tOP: 0.945205\tOR: 0.985714\tOF1: 0.965035\n",
      "Train Epoch: 7 [480/947 (50%)]\tLoss: 0.031444 \tOP: 0.850746\tOR: 0.919355\tOF1: 0.883721\n",
      "Train Epoch: 7 [512/947 (53%)]\tLoss: 0.033645 \tOP: 0.838235\tOR: 0.863636\tOF1: 0.850746\n",
      "Train Epoch: 7 [544/947 (57%)]\tLoss: 0.020688 \tOP: 0.910448\tOR: 0.910448\tOF1: 0.910448\n",
      "Train Epoch: 7 [576/947 (60%)]\tLoss: 0.028343 \tOP: 0.880000\tOR: 0.916667\tOF1: 0.897959\n",
      "Train Epoch: 7 [608/947 (63%)]\tLoss: 0.033513 \tOP: 0.864198\tOR: 0.897436\tOF1: 0.880503\n",
      "Train Epoch: 7 [640/947 (67%)]\tLoss: 0.035098 \tOP: 0.864407\tOR: 0.836066\tOF1: 0.850000\n",
      "Train Epoch: 7 [672/947 (70%)]\tLoss: 0.019834 \tOP: 0.914286\tOR: 0.914286\tOF1: 0.914286\n",
      "Train Epoch: 7 [704/947 (73%)]\tLoss: 0.027867 \tOP: 0.878378\tOR: 0.902778\tOF1: 0.890411\n",
      "Train Epoch: 7 [736/947 (77%)]\tLoss: 0.049300 \tOP: 0.805970\tOR: 0.857143\tOF1: 0.830769\n",
      "Train Epoch: 7 [768/947 (80%)]\tLoss: 0.033473 \tOP: 0.865672\tOR: 0.906250\tOF1: 0.885496\n",
      "Train Epoch: 7 [800/947 (83%)]\tLoss: 0.024752 \tOP: 0.892857\tOR: 0.925926\tOF1: 0.909091\n",
      "Train Epoch: 7 [832/947 (87%)]\tLoss: 0.035103 \tOP: 0.897059\tOR: 0.910448\tOF1: 0.903704\n",
      "Train Epoch: 7 [864/947 (90%)]\tLoss: 0.031436 \tOP: 0.873239\tOR: 0.911765\tOF1: 0.892086\n",
      "Train Epoch: 7 [896/947 (93%)]\tLoss: 0.028057 \tOP: 0.883117\tOR: 0.894737\tOF1: 0.888889\n",
      "Train Epoch: 7 [551/947 (97%)]\tLoss: 0.018089 \tOP: 0.833333\tOR: 0.952381\tOF1: 0.888889\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.3931 \n",
      "OP: 0.250000\n",
      "OR: 0.169811\n",
      "OF1: 0.202247\n",
      "\n",
      "Train Epoch: 8 [0/947 (0%)]\tLoss: 0.027233 \tOP: 0.890411\tOR: 0.928571\tOF1: 0.909091\n",
      "Train Epoch: 8 [32/947 (3%)]\tLoss: 0.012544 \tOP: 0.916667\tOR: 0.970588\tOF1: 0.942857\n",
      "Train Epoch: 8 [64/947 (7%)]\tLoss: 0.015035 \tOP: 0.891892\tOR: 0.970588\tOF1: 0.929577\n",
      "Train Epoch: 8 [96/947 (10%)]\tLoss: 0.022651 \tOP: 0.833333\tOR: 0.942029\tOF1: 0.884354\n",
      "Train Epoch: 8 [128/947 (13%)]\tLoss: 0.014734 \tOP: 0.866667\tOR: 0.970149\tOF1: 0.915493\n",
      "Train Epoch: 8 [160/947 (17%)]\tLoss: 0.020816 \tOP: 0.880597\tOR: 0.880597\tOF1: 0.880597\n",
      "Train Epoch: 8 [192/947 (20%)]\tLoss: 0.032538 \tOP: 0.885714\tOR: 0.861111\tOF1: 0.873239\n",
      "Train Epoch: 8 [224/947 (23%)]\tLoss: 0.016197 \tOP: 0.882353\tOR: 0.895522\tOF1: 0.888889\n",
      "Train Epoch: 8 [256/947 (27%)]\tLoss: 0.029893 \tOP: 0.911765\tOR: 0.898551\tOF1: 0.905109\n",
      "Train Epoch: 8 [288/947 (30%)]\tLoss: 0.011254 \tOP: 0.898734\tOR: 0.972603\tOF1: 0.934211\n",
      "Train Epoch: 8 [320/947 (33%)]\tLoss: 0.017998 \tOP: 0.835616\tOR: 0.924242\tOF1: 0.877698\n",
      "Train Epoch: 8 [352/947 (37%)]\tLoss: 0.013330 \tOP: 0.819444\tOR: 0.936508\tOF1: 0.874074\n",
      "Train Epoch: 8 [384/947 (40%)]\tLoss: 0.016295 \tOP: 0.861111\tOR: 0.984127\tOF1: 0.918519\n",
      "Train Epoch: 8 [416/947 (43%)]\tLoss: 0.018016 \tOP: 0.890411\tOR: 0.942029\tOF1: 0.915493\n",
      "Train Epoch: 8 [448/947 (47%)]\tLoss: 0.013558 \tOP: 0.900000\tOR: 0.940299\tOF1: 0.919708\n",
      "Train Epoch: 8 [480/947 (50%)]\tLoss: 0.017911 \tOP: 0.857143\tOR: 0.909091\tOF1: 0.882353\n",
      "Train Epoch: 8 [512/947 (53%)]\tLoss: 0.010752 \tOP: 0.914286\tOR: 0.955224\tOF1: 0.934307\n",
      "Train Epoch: 8 [544/947 (57%)]\tLoss: 0.014127 \tOP: 0.897059\tOR: 0.938462\tOF1: 0.917293\n",
      "Train Epoch: 8 [576/947 (60%)]\tLoss: 0.014077 \tOP: 0.900000\tOR: 0.940299\tOF1: 0.919708\n",
      "Train Epoch: 8 [608/947 (63%)]\tLoss: 0.021602 \tOP: 0.896104\tOR: 0.932432\tOF1: 0.913907\n",
      "Train Epoch: 8 [640/947 (67%)]\tLoss: 0.012596 \tOP: 0.918919\tOR: 0.944444\tOF1: 0.931507\n",
      "Train Epoch: 8 [672/947 (70%)]\tLoss: 0.014428 \tOP: 0.902778\tOR: 0.984848\tOF1: 0.942029\n",
      "Train Epoch: 8 [704/947 (73%)]\tLoss: 0.011140 \tOP: 0.846154\tOR: 0.948276\tOF1: 0.894309\n",
      "Train Epoch: 8 [736/947 (77%)]\tLoss: 0.007534 \tOP: 0.931507\tOR: 0.985507\tOF1: 0.957746\n",
      "Train Epoch: 8 [768/947 (80%)]\tLoss: 0.017507 \tOP: 0.882353\tOR: 0.937500\tOF1: 0.909091\n",
      "Train Epoch: 8 [800/947 (83%)]\tLoss: 0.015455 \tOP: 0.900000\tOR: 0.986301\tOF1: 0.941176\n",
      "Train Epoch: 8 [832/947 (87%)]\tLoss: 0.008423 \tOP: 0.933333\tOR: 0.972222\tOF1: 0.952381\n",
      "Train Epoch: 8 [864/947 (90%)]\tLoss: 0.013246 \tOP: 0.944444\tOR: 0.957746\tOF1: 0.951049\n",
      "Train Epoch: 8 [896/947 (93%)]\tLoss: 0.015315 \tOP: 0.901408\tOR: 0.941176\tOF1: 0.920863\n",
      "Train Epoch: 8 [551/947 (97%)]\tLoss: 0.015547 \tOP: 0.800000\tOR: 0.923077\tOF1: 0.857143\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5445 \n",
      "OP: 0.230769\n",
      "OR: 0.169811\n",
      "OF1: 0.195652\n",
      "\n",
      "Train Epoch: 9 [0/947 (0%)]\tLoss: 0.011928 \tOP: 0.898734\tOR: 0.959459\tOF1: 0.928105\n",
      "Train Epoch: 9 [32/947 (3%)]\tLoss: 0.031972 \tOP: 0.821918\tOR: 1.000000\tOF1: 0.902256\n",
      "Train Epoch: 9 [64/947 (7%)]\tLoss: 0.015844 \tOP: 0.896104\tOR: 0.945205\tOF1: 0.920000\n",
      "Train Epoch: 9 [96/947 (10%)]\tLoss: 0.008147 \tOP: 0.914286\tOR: 0.984615\tOF1: 0.948148\n",
      "Train Epoch: 9 [128/947 (13%)]\tLoss: 0.011774 \tOP: 0.917808\tOR: 0.971014\tOF1: 0.943662\n",
      "Train Epoch: 9 [160/947 (17%)]\tLoss: 0.009329 \tOP: 0.927536\tOR: 0.984615\tOF1: 0.955224\n",
      "Train Epoch: 9 [192/947 (20%)]\tLoss: 0.014139 \tOP: 0.917808\tOR: 0.881579\tOF1: 0.899329\n",
      "Train Epoch: 9 [224/947 (23%)]\tLoss: 0.017176 \tOP: 0.865672\tOR: 0.950820\tOF1: 0.906250\n",
      "Train Epoch: 9 [256/947 (27%)]\tLoss: 0.005716 \tOP: 0.929577\tOR: 0.985075\tOF1: 0.956522\n",
      "Train Epoch: 9 [288/947 (30%)]\tLoss: 0.009426 \tOP: 0.921053\tOR: 0.985915\tOF1: 0.952381\n",
      "Train Epoch: 9 [320/947 (33%)]\tLoss: 0.012887 \tOP: 0.840580\tOR: 0.983051\tOF1: 0.906250\n",
      "Train Epoch: 9 [352/947 (37%)]\tLoss: 0.007107 \tOP: 0.906667\tOR: 0.971429\tOF1: 0.937931\n",
      "Train Epoch: 9 [384/947 (40%)]\tLoss: 0.010109 \tOP: 0.896104\tOR: 1.000000\tOF1: 0.945205\n",
      "Train Epoch: 9 [416/947 (43%)]\tLoss: 0.010442 \tOP: 0.947368\tOR: 0.972973\tOF1: 0.960000\n",
      "Train Epoch: 9 [448/947 (47%)]\tLoss: 0.008868 \tOP: 0.902778\tOR: 1.000000\tOF1: 0.948905\n",
      "Train Epoch: 9 [480/947 (50%)]\tLoss: 0.006795 \tOP: 0.929577\tOR: 0.970588\tOF1: 0.949640\n",
      "Train Epoch: 9 [512/947 (53%)]\tLoss: 0.021343 \tOP: 0.919355\tOR: 0.904762\tOF1: 0.912000\n",
      "Train Epoch: 9 [544/947 (57%)]\tLoss: 0.013366 \tOP: 0.894118\tOR: 0.950000\tOF1: 0.921212\n",
      "Train Epoch: 9 [576/947 (60%)]\tLoss: 0.012229 \tOP: 0.918919\tOR: 1.000000\tOF1: 0.957746\n",
      "Train Epoch: 9 [608/947 (63%)]\tLoss: 0.019867 \tOP: 0.873016\tOR: 0.932203\tOF1: 0.901639\n",
      "Train Epoch: 9 [640/947 (67%)]\tLoss: 0.009123 \tOP: 0.933333\tOR: 0.985915\tOF1: 0.958904\n",
      "Train Epoch: 9 [672/947 (70%)]\tLoss: 0.005672 \tOP: 0.907692\tOR: 0.951613\tOF1: 0.929134\n",
      "Train Epoch: 9 [704/947 (73%)]\tLoss: 0.007077 \tOP: 0.933333\tOR: 0.972222\tOF1: 0.952381\n",
      "Train Epoch: 9 [736/947 (77%)]\tLoss: 0.005027 \tOP: 0.880597\tOR: 1.000000\tOF1: 0.936508\n",
      "Train Epoch: 9 [768/947 (80%)]\tLoss: 0.014027 \tOP: 0.893333\tOR: 0.957143\tOF1: 0.924138\n",
      "Train Epoch: 9 [800/947 (83%)]\tLoss: 0.005987 \tOP: 0.868421\tOR: 1.000000\tOF1: 0.929577\n",
      "Train Epoch: 9 [832/947 (87%)]\tLoss: 0.010873 \tOP: 0.880597\tOR: 0.967213\tOF1: 0.921875\n",
      "Train Epoch: 9 [864/947 (90%)]\tLoss: 0.016677 \tOP: 0.871795\tOR: 0.957746\tOF1: 0.912752\n",
      "Train Epoch: 9 [896/947 (93%)]\tLoss: 0.009459 \tOP: 0.921053\tOR: 0.958904\tOF1: 0.939597\n",
      "Train Epoch: 9 [551/947 (97%)]\tLoss: 0.007754 \tOP: 0.824561\tOR: 0.979167\tOF1: 0.895238\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.5188 \n",
      "OP: 0.302326\n",
      "OR: 0.245283\n",
      "OF1: 0.270833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model4.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model4(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model4.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model4(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG (Unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_vgg11 = models.vgg11(weights=models.VGG11_Weights.DEFAULT)\n",
    "        \n",
    "        child_counter = 0\n",
    "        for child in model_vgg11.children():\n",
    "            if child_counter < 1:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    child_counter += 1\n",
    "            else:\n",
    "                child_counter += 1\n",
    "        \n",
    "        self.model_pre = nn.Sequential(*list(model_vgg11.children())[:-1])\n",
    "        fc_feat = model_vgg11.classifier[0].in_features\n",
    "        self.model_post = nn.Sequential(\n",
    "            nn.Linear(fc_feat, 4096, bias = True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(.5, inplace = False),\n",
    "            nn.Linear(4096, 4096, bias = True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(.5, inplace = False),\n",
    "        )\n",
    "        self.fc = nn.Linear(4096, len(classes))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.model_pre(X)\n",
    "        X = torch.flatten(X,1)\n",
    "        X = self.model_post(X)\n",
    "        return self.fc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = MyCNN4()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model5.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch):\n",
    "    model5.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model5(data)\n",
    "\n",
    "        preds = torch.round(output)\n",
    "                \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tOP: {:.6f}\\tOR: {:.6f}\\tOF1: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data.item(), OP, OR, OF1))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model5.eval()\n",
    "    test_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        with torch.no_grad():\n",
    "            data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "            output = model5(data)\n",
    "        \n",
    "        preds = torch.round(output)\n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        \n",
    "        target = target.detach().numpy()\n",
    "        preds = preds.detach().numpy()\n",
    "        OP, OR, OF1 = metric(preds, target)\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nOP: {:.6f}\\nOR: {:.6f}\\nOF1: {:.6f}\\n'.format(test_loss.data.item()/i, OP, OR, OF1))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
